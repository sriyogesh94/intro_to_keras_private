{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Batch39_20180421_ANNLab_ClassificationActivity_BackOrders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Statement\n",
    "\n",
    "    Identify products at risk of backorder before the event occurs so the business has time to react. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "Data file contains the historical data for the 8 weeks prior to the week we are trying to predict. The data was taken as weekly snapshots at the start of each week. Columns are defined as follows:\n",
    "\n",
    "    sku - Random ID for the product\n",
    "\n",
    "    national_inv - Current inventory level for the part\n",
    "\n",
    "    lead_time - Transit time for product (if available)\n",
    "\n",
    "    in_transit_qty - Amount of product in transit from source\n",
    "\n",
    "    forecast_3_month - Forecast sales for the next 3 months\n",
    "\n",
    "    forecast_6_month - Forecast sales for the next 6 months\n",
    "\n",
    "    forecast_9_month - Forecast sales for the next 9 months\n",
    "\n",
    "    sales_1_month - Sales quantity for the prior 1 month time period\n",
    "\n",
    "    sales_3_month - Sales quantity for the prior 3 month time period\n",
    "\n",
    "    sales_6_month - Sales quantity for the prior 6 month time period\n",
    "\n",
    "    sales_9_month - Sales quantity for the prior 9 month time period\n",
    "\n",
    "    min_bank - Minimum recommend amount to stock\n",
    "\n",
    "    potential_issue - Source issue for part identified\n",
    "\n",
    "    pieces_past_due - Parts overdue from source\n",
    "\n",
    "    perf_6_month_avg - Source performance for prior 6 month period\n",
    "\n",
    "    perf_12_month_avg - Source performance for prior 12 month period\n",
    "\n",
    "    local_bo_qty - Amount of stock orders overdue\n",
    "\n",
    "    deck_risk - Part risk flag\n",
    "\n",
    "    oe_constraint - Part risk flag\n",
    "\n",
    "    ppap_risk - Part risk flag\n",
    "\n",
    "    stop_auto_buy - Part risk flag\n",
    "\n",
    "    rev_stop - Part risk flag\n",
    "\n",
    "    went_on_backorder - Product actually went on backorder. This is the target value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pre-processing\n",
    "#### Loading the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import to_categorical\n",
    "from keras import optimizers\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'F:\\\\Batch39\\\\CSE7321c\\\\Lab\\\\20180421_Batch39_CSE7321c_Lab01_ANN_Classification'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#For setting working directory, if required\n",
    "#os.chdir('path to file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"BackOrders.csv\",header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understand the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the number row and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(61589, 23)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sku', 'national_inv', 'lead_time', 'in_transit_qty',\n",
       "       'forecast_3_month', 'forecast_6_month', 'forecast_9_month',\n",
       "       'sales_1_month', 'sales_3_month', 'sales_6_month', 'sales_9_month',\n",
       "       'min_bank', 'potential_issue', 'pieces_past_due', 'perf_6_month_avg',\n",
       "       'perf_12_month_avg', 'local_bo_qty', 'deck_risk', 'oe_constraint',\n",
       "       'ppap_risk', 'stop_auto_buy', 'rev_stop', 'went_on_backorder'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RangeIndex(start=0, stop=61589, step=1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the top rows of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sku</th>\n",
       "      <th>national_inv</th>\n",
       "      <th>lead_time</th>\n",
       "      <th>in_transit_qty</th>\n",
       "      <th>forecast_3_month</th>\n",
       "      <th>forecast_6_month</th>\n",
       "      <th>forecast_9_month</th>\n",
       "      <th>sales_1_month</th>\n",
       "      <th>sales_3_month</th>\n",
       "      <th>sales_6_month</th>\n",
       "      <th>...</th>\n",
       "      <th>pieces_past_due</th>\n",
       "      <th>perf_6_month_avg</th>\n",
       "      <th>perf_12_month_avg</th>\n",
       "      <th>local_bo_qty</th>\n",
       "      <th>deck_risk</th>\n",
       "      <th>oe_constraint</th>\n",
       "      <th>ppap_risk</th>\n",
       "      <th>stop_auto_buy</th>\n",
       "      <th>rev_stop</th>\n",
       "      <th>went_on_backorder</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1888279</td>\n",
       "      <td>117</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>-99.00</td>\n",
       "      <td>-99.00</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1870557</td>\n",
       "      <td>7</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1475481</td>\n",
       "      <td>258</td>\n",
       "      <td>15.0</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>77</td>\n",
       "      <td>184</td>\n",
       "      <td>46</td>\n",
       "      <td>132</td>\n",
       "      <td>256</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       sku  national_inv  lead_time  in_transit_qty  forecast_3_month  \\\n",
       "0  1888279           117        NaN               0                 0   \n",
       "1  1870557             7        2.0               0                 0   \n",
       "2  1475481           258       15.0              10                10   \n",
       "\n",
       "   forecast_6_month  forecast_9_month  sales_1_month  sales_3_month  \\\n",
       "0                 0                 0              0              0   \n",
       "1                 0                 0              0              0   \n",
       "2                77               184             46            132   \n",
       "\n",
       "   sales_6_month        ...         pieces_past_due  perf_6_month_avg  \\\n",
       "0             15        ...                       0            -99.00   \n",
       "1              0        ...                       0              0.50   \n",
       "2            256        ...                       0              0.54   \n",
       "\n",
       "  perf_12_month_avg  local_bo_qty  deck_risk  oe_constraint  ppap_risk  \\\n",
       "0            -99.00             0         No             No        Yes   \n",
       "1              0.28             0        Yes             No         No   \n",
       "2              0.70             0         No             No         No   \n",
       "\n",
       "  stop_auto_buy rev_stop went_on_backorder  \n",
       "0           Yes       No                No  \n",
       "1           Yes       No                No  \n",
       "2           Yes       No                No  \n",
       "\n",
       "[3 rows x 23 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shows a quick statistic summary of your data using describe.\n",
    "\n",
    "For object data (e.g. strings or timestamps), the result’s index will include count, unique, top, and freq. \n",
    "\n",
    "The top is the most common value.\n",
    "\n",
    "The freq is the most common value’s frequency.\n",
    "\n",
    "Timestamps also include the first and last items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sku</th>\n",
       "      <th>national_inv</th>\n",
       "      <th>lead_time</th>\n",
       "      <th>in_transit_qty</th>\n",
       "      <th>forecast_3_month</th>\n",
       "      <th>forecast_6_month</th>\n",
       "      <th>forecast_9_month</th>\n",
       "      <th>sales_1_month</th>\n",
       "      <th>sales_3_month</th>\n",
       "      <th>sales_6_month</th>\n",
       "      <th>...</th>\n",
       "      <th>pieces_past_due</th>\n",
       "      <th>perf_6_month_avg</th>\n",
       "      <th>perf_12_month_avg</th>\n",
       "      <th>local_bo_qty</th>\n",
       "      <th>deck_risk</th>\n",
       "      <th>oe_constraint</th>\n",
       "      <th>ppap_risk</th>\n",
       "      <th>stop_auto_buy</th>\n",
       "      <th>rev_stop</th>\n",
       "      <th>went_on_backorder</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6.158900e+04</td>\n",
       "      <td>61589.000000</td>\n",
       "      <td>58186.000000</td>\n",
       "      <td>61589.000000</td>\n",
       "      <td>6.158900e+04</td>\n",
       "      <td>6.158900e+04</td>\n",
       "      <td>6.158900e+04</td>\n",
       "      <td>61589.000000</td>\n",
       "      <td>61589.000000</td>\n",
       "      <td>6.158900e+04</td>\n",
       "      <td>...</td>\n",
       "      <td>61589.000000</td>\n",
       "      <td>61589.000000</td>\n",
       "      <td>61589.000000</td>\n",
       "      <td>61589.000000</td>\n",
       "      <td>61589</td>\n",
       "      <td>61589</td>\n",
       "      <td>61589</td>\n",
       "      <td>61589</td>\n",
       "      <td>61589</td>\n",
       "      <td>61589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>48145</td>\n",
       "      <td>61577</td>\n",
       "      <td>53792</td>\n",
       "      <td>59303</td>\n",
       "      <td>61569</td>\n",
       "      <td>50296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.037188e+06</td>\n",
       "      <td>287.721882</td>\n",
       "      <td>7.559619</td>\n",
       "      <td>30.192843</td>\n",
       "      <td>1.692728e+02</td>\n",
       "      <td>3.150413e+02</td>\n",
       "      <td>4.535760e+02</td>\n",
       "      <td>44.742957</td>\n",
       "      <td>150.732631</td>\n",
       "      <td>2.835465e+02</td>\n",
       "      <td>...</td>\n",
       "      <td>1.605400</td>\n",
       "      <td>-6.264182</td>\n",
       "      <td>-5.863664</td>\n",
       "      <td>1.205361</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>6.564178e+05</td>\n",
       "      <td>4233.906931</td>\n",
       "      <td>6.498952</td>\n",
       "      <td>792.869253</td>\n",
       "      <td>5.286742e+03</td>\n",
       "      <td>9.774362e+03</td>\n",
       "      <td>1.420201e+04</td>\n",
       "      <td>1373.805831</td>\n",
       "      <td>5224.959649</td>\n",
       "      <td>8.872270e+03</td>\n",
       "      <td>...</td>\n",
       "      <td>42.309229</td>\n",
       "      <td>25.537906</td>\n",
       "      <td>24.844514</td>\n",
       "      <td>29.981155</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.068628e+06</td>\n",
       "      <td>-2999.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-99.000000</td>\n",
       "      <td>-99.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.498574e+06</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.620000</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.898033e+06</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.820000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.314826e+06</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.200000e+01</td>\n",
       "      <td>2.500000e+01</td>\n",
       "      <td>3.600000e+01</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>3.400000e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.284895e+06</td>\n",
       "      <td>673445.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>170976.000000</td>\n",
       "      <td>1.126656e+06</td>\n",
       "      <td>2.094336e+06</td>\n",
       "      <td>3.062016e+06</td>\n",
       "      <td>295197.000000</td>\n",
       "      <td>934593.000000</td>\n",
       "      <td>1.799099e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>7392.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2999.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 sku   national_inv     lead_time  in_transit_qty  \\\n",
       "count   6.158900e+04   61589.000000  58186.000000    61589.000000   \n",
       "unique           NaN            NaN           NaN             NaN   \n",
       "top              NaN            NaN           NaN             NaN   \n",
       "freq             NaN            NaN           NaN             NaN   \n",
       "mean    2.037188e+06     287.721882      7.559619       30.192843   \n",
       "std     6.564178e+05    4233.906931      6.498952      792.869253   \n",
       "min     1.068628e+06   -2999.000000      0.000000        0.000000   \n",
       "25%     1.498574e+06       3.000000      4.000000        0.000000   \n",
       "50%     1.898033e+06      10.000000      8.000000        0.000000   \n",
       "75%     2.314826e+06      57.000000      8.000000        0.000000   \n",
       "max     3.284895e+06  673445.000000     52.000000   170976.000000   \n",
       "\n",
       "        forecast_3_month  forecast_6_month  forecast_9_month  sales_1_month  \\\n",
       "count       6.158900e+04      6.158900e+04      6.158900e+04   61589.000000   \n",
       "unique               NaN               NaN               NaN            NaN   \n",
       "top                  NaN               NaN               NaN            NaN   \n",
       "freq                 NaN               NaN               NaN            NaN   \n",
       "mean        1.692728e+02      3.150413e+02      4.535760e+02      44.742957   \n",
       "std         5.286742e+03      9.774362e+03      1.420201e+04    1373.805831   \n",
       "min         0.000000e+00      0.000000e+00      0.000000e+00       0.000000   \n",
       "25%         0.000000e+00      0.000000e+00      0.000000e+00       0.000000   \n",
       "50%         0.000000e+00      0.000000e+00      0.000000e+00       0.000000   \n",
       "75%         1.200000e+01      2.500000e+01      3.600000e+01       6.000000   \n",
       "max         1.126656e+06      2.094336e+06      3.062016e+06  295197.000000   \n",
       "\n",
       "        sales_3_month  sales_6_month        ...         pieces_past_due  \\\n",
       "count    61589.000000   6.158900e+04        ...            61589.000000   \n",
       "unique            NaN            NaN        ...                     NaN   \n",
       "top               NaN            NaN        ...                     NaN   \n",
       "freq              NaN            NaN        ...                     NaN   \n",
       "mean       150.732631   2.835465e+02        ...                1.605400   \n",
       "std       5224.959649   8.872270e+03        ...               42.309229   \n",
       "min          0.000000   0.000000e+00        ...                0.000000   \n",
       "25%          0.000000   0.000000e+00        ...                0.000000   \n",
       "50%          2.000000   4.000000e+00        ...                0.000000   \n",
       "75%         17.000000   3.400000e+01        ...                0.000000   \n",
       "max     934593.000000   1.799099e+06        ...             7392.000000   \n",
       "\n",
       "        perf_6_month_avg perf_12_month_avg  local_bo_qty  deck_risk  \\\n",
       "count       61589.000000      61589.000000  61589.000000      61589   \n",
       "unique               NaN               NaN           NaN          2   \n",
       "top                  NaN               NaN           NaN         No   \n",
       "freq                 NaN               NaN           NaN      48145   \n",
       "mean           -6.264182         -5.863664      1.205361        NaN   \n",
       "std            25.537906         24.844514     29.981155        NaN   \n",
       "min           -99.000000        -99.000000      0.000000        NaN   \n",
       "25%             0.620000          0.640000      0.000000        NaN   \n",
       "50%             0.820000          0.800000      0.000000        NaN   \n",
       "75%             0.960000          0.950000      0.000000        NaN   \n",
       "max             1.000000          1.000000   2999.000000        NaN   \n",
       "\n",
       "        oe_constraint  ppap_risk stop_auto_buy rev_stop went_on_backorder  \n",
       "count           61589      61589         61589    61589             61589  \n",
       "unique              2          2             2        2                 2  \n",
       "top                No         No           Yes       No                No  \n",
       "freq            61577      53792         59303    61569             50296  \n",
       "mean              NaN        NaN           NaN      NaN               NaN  \n",
       "std               NaN        NaN           NaN      NaN               NaN  \n",
       "min               NaN        NaN           NaN      NaN               NaN  \n",
       "25%               NaN        NaN           NaN      NaN               NaN  \n",
       "50%               NaN        NaN           NaN      NaN               NaN  \n",
       "75%               NaN        NaN           NaN      NaN               NaN  \n",
       "max               NaN        NaN           NaN      NaN               NaN  \n",
       "\n",
       "[11 rows x 23 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display data type of each variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sku                    int64\n",
       "national_inv           int64\n",
       "lead_time            float64\n",
       "in_transit_qty         int64\n",
       "forecast_3_month       int64\n",
       "forecast_6_month       int64\n",
       "forecast_9_month       int64\n",
       "sales_1_month          int64\n",
       "sales_3_month          int64\n",
       "sales_6_month          int64\n",
       "sales_9_month          int64\n",
       "min_bank               int64\n",
       "potential_issue       object\n",
       "pieces_past_due        int64\n",
       "perf_6_month_avg     float64\n",
       "perf_12_month_avg    float64\n",
       "local_bo_qty           int64\n",
       "deck_risk             object\n",
       "oe_constraint         object\n",
       "ppap_risk             object\n",
       "stop_auto_buy         object\n",
       "rev_stop              object\n",
       "went_on_backorder     object\n",
       "dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "\n",
    "sku is Categorical but is interpreted as int64 \n",
    "potential_issue, deck_risk, oe_constraint, ppap_risk, stop_auto_buy, rev_stop, and went_on_backorder are also \n",
    "categorical but is interpreted as object. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert all the attributes to appropriate type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data type conversion\n",
    "\n",
    "    Using astype('category') to convert potential_issue, deck_risk, oe_constraint, ppap_risk, stop_auto_buy, rev_stop, and went_on_backorder attributes to categorical attributes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for col in ['sku', 'potential_issue', 'deck_risk', 'oe_constraint', 'ppap_risk', 'stop_auto_buy', 'rev_stop', 'went_on_backorder']:\n",
    "    data[col] = data[col].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display data type of each variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sku                  category\n",
       "national_inv            int64\n",
       "lead_time             float64\n",
       "in_transit_qty          int64\n",
       "forecast_3_month        int64\n",
       "forecast_6_month        int64\n",
       "forecast_9_month        int64\n",
       "sales_1_month           int64\n",
       "sales_3_month           int64\n",
       "sales_6_month           int64\n",
       "sales_9_month           int64\n",
       "min_bank                int64\n",
       "potential_issue      category\n",
       "pieces_past_due         int64\n",
       "perf_6_month_avg      float64\n",
       "perf_12_month_avg     float64\n",
       "local_bo_qty            int64\n",
       "deck_risk            category\n",
       "oe_constraint        category\n",
       "ppap_risk            category\n",
       "stop_auto_buy        category\n",
       "rev_stop             category\n",
       "went_on_backorder    category\n",
       "dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Delete sku attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61589"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.size(np.unique(data.sku))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.drop('sku', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing Data\n",
    "\n",
    "Missing value analysis and dropping the records with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "national_inv            0\n",
       "lead_time            3403\n",
       "in_transit_qty          0\n",
       "forecast_3_month        0\n",
       "forecast_6_month        0\n",
       "forecast_9_month        0\n",
       "sales_1_month           0\n",
       "sales_3_month           0\n",
       "sales_6_month           0\n",
       "sales_9_month           0\n",
       "min_bank                0\n",
       "potential_issue         0\n",
       "pieces_past_due         0\n",
       "perf_6_month_avg        0\n",
       "perf_12_month_avg       0\n",
       "local_bo_qty            0\n",
       "deck_risk               0\n",
       "oe_constraint           0\n",
       "ppap_risk               0\n",
       "stop_auto_buy           0\n",
       "rev_stop                0\n",
       "went_on_backorder       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observing the number of records before and after missing value records removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61589, 22)\n"
     ]
    }
   ],
   "source": [
    "print (data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Since the number of missing values is about 5%. For initial analysis we ignore all these records\n",
    "data = data.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "national_inv         0\n",
      "lead_time            0\n",
      "in_transit_qty       0\n",
      "forecast_3_month     0\n",
      "forecast_6_month     0\n",
      "forecast_9_month     0\n",
      "sales_1_month        0\n",
      "sales_3_month        0\n",
      "sales_6_month        0\n",
      "sales_9_month        0\n",
      "min_bank             0\n",
      "potential_issue      0\n",
      "pieces_past_due      0\n",
      "perf_6_month_avg     0\n",
      "perf_12_month_avg    0\n",
      "local_bo_qty         0\n",
      "deck_risk            0\n",
      "oe_constraint        0\n",
      "ppap_risk            0\n",
      "stop_auto_buy        0\n",
      "rev_stop             0\n",
      "went_on_backorder    0\n",
      "dtype: int64\n",
      "----------------------------------\n",
      "(58186, 22)\n"
     ]
    }
   ],
   "source": [
    "print(data.isnull().sum())\n",
    "print(\"----------------------------------\")\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting Categorical to Numeric\n",
    "\n",
    "For some of the models all the independent attribute should be of type numeric and ANN model is one among them.\n",
    "But this data set has some categorial attributes.\n",
    "\n",
    "'pandas.get_dummies' To convert convert categorical variable into dummy/indicator variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['national_inv', 'lead_time', 'in_transit_qty', 'forecast_3_month',\n",
      "       'forecast_6_month', 'forecast_9_month', 'sales_1_month',\n",
      "       'sales_3_month', 'sales_6_month', 'sales_9_month', 'min_bank',\n",
      "       'potential_issue', 'pieces_past_due', 'perf_6_month_avg',\n",
      "       'perf_12_month_avg', 'local_bo_qty', 'deck_risk', 'oe_constraint',\n",
      "       'ppap_risk', 'stop_auto_buy', 'rev_stop', 'went_on_backorder'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print (data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating dummy variables.\n",
    "\n",
    "If we have k levels in a category, then we create k-1 dummy variables as the last one would be redundant. So we use the parameter drop_first in pd.get_dummies function that drops the first level in each of the category\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categorical_Attributes = data.select_dtypes(include=['category']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.get_dummies(columns=categorical_Attributes, data=data, prefix=categorical_Attributes, prefix_sep=\"_\",drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['national_inv', 'lead_time', 'in_transit_qty', 'forecast_3_month',\n",
      "       'forecast_6_month', 'forecast_9_month', 'sales_1_month',\n",
      "       'sales_3_month', 'sales_6_month', 'sales_9_month', 'min_bank',\n",
      "       'pieces_past_due', 'perf_6_month_avg', 'perf_12_month_avg',\n",
      "       'local_bo_qty', 'potential_issue_Yes', 'deck_risk_Yes',\n",
      "       'oe_constraint_Yes', 'ppap_risk_Yes', 'stop_auto_buy_Yes',\n",
      "       'rev_stop_Yes', 'went_on_backorder_Yes'],\n",
      "      dtype='object') (58186, 22)\n"
     ]
    }
   ],
   "source": [
    "print (data.columns, data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target attribute distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    47217\n",
       "1    10969\n",
       "Name: went_on_backorder_Yes, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.value_counts(data['went_on_backorder_Yes'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train-Test Split\n",
    "\n",
    "Using sklearn.model_selection.train_test_split\n",
    "\n",
    "    Split arrays or matrices into train and test subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Performing train test split on the data\n",
    "X, y = data.loc[:,data.columns!='went_on_backorder_Yes'].values, data.loc[:,'went_on_backorder_Yes'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=123, stratify = data['went_on_backorder_Yes'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    33052\n",
      "1     7678\n",
      "dtype: int64\n",
      "0    14165\n",
      "1     3291\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#To get the distribution in the target in train and test\n",
    "print(pd.value_counts(y_train))\n",
    "print(pd.value_counts(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------\n",
    "-------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Building a logistic regression model using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier = LogisticRegression(random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=123, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predictions on train data\n",
    "train_pred = classifier.predict(X_train)\n",
    "# Predictions on test data\n",
    "test_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix - Train Data: \n",
      " [[32916   136]\n",
      " [ 6912   766]]\n",
      "Confusion Matrix - Test Data: \n",
      " [[14109    56]\n",
      " [ 2962   329]]\n"
     ]
    }
   ],
   "source": [
    "# Train data\n",
    "confusion_matrix_train = confusion_matrix(y_train, train_pred)\n",
    "print(\"Confusion Matrix - Train Data: \\n\", confusion_matrix_train)\n",
    "# Test data\n",
    "confusion_matrix_test= confusion_matrix(y_test, test_pred)\n",
    "print(\"Confusion Matrix - Test Data: \\n\", confusion_matrix_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Error Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Specificity:  0.995885271693\n",
      "Train Recall:  0.0997655639489\n",
      "Train Precision:  0.849223946785\n",
      "Train Accuracy:  0.826958016204\n"
     ]
    }
   ],
   "source": [
    "# Metrics on train data for logistic regression model\n",
    "#Accuracy\n",
    "accuracy_Train_logReg = (confusion_matrix_train[0,0]+confusion_matrix_train[1,1])/(confusion_matrix_train[0,0]+confusion_matrix_train[0,1]+confusion_matrix_train[1,0]+confusion_matrix_train[1,1])\n",
    "#specificity or true negative rate (TNR)\n",
    "specificity_Train_logReg = confusion_matrix_train[0,0]/(confusion_matrix_train[0,0]+confusion_matrix_train[0,1])\n",
    "#sensitivity, recall, hit rate, or true positive rate (TPR)\n",
    "recall_Train_logReg = confusion_matrix_train[1,1]/(confusion_matrix_train[1,0]+confusion_matrix_train[1,1])\n",
    "#precision\n",
    "precision_Train_logReg = confusion_matrix_train[1,1]/(confusion_matrix_train[0,1]+confusion_matrix_train[1,1])\n",
    "\n",
    "print(\"Train Specificity: \",specificity_Train_logReg)\n",
    "print(\"Train Recall: \",recall_Train_logReg)\n",
    "print(\"Train Precision: \",precision_Train_logReg)\n",
    "print(\"Train Accuracy: \",accuracy_Train_logReg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Specificity:  0.996046593717\n",
      "Test Recall:  0.0999696140991\n",
      "Test Precision:  0.854545454545\n",
      "Test Accuracy:  0.827108157654\n"
     ]
    }
   ],
   "source": [
    "# Metrics on test data\n",
    "#Accuracy\n",
    "accuracy_Test_logReg = (confusion_matrix_test[0,0]+confusion_matrix_test[1,1])/(confusion_matrix_test[0,0]+confusion_matrix_test[0,1]+confusion_matrix_test[1,0]+confusion_matrix_test[1,1])\n",
    "#specificity or true negative rate (TNR)\n",
    "specificity_Test_logReg = confusion_matrix_test[0,0]/(confusion_matrix_test[0,0]+confusion_matrix_test[0,1])\n",
    "#sensitivity, recall, hit rate, or true positive rate (TPR)\n",
    "recall_Test_logReg = confusion_matrix_test[1,1]/(confusion_matrix_test[1,0]+confusion_matrix_test[1,1])\n",
    "#precision\n",
    "precision_Test_logReg = confusion_matrix_test[1,1]/(confusion_matrix_test[0,1]+confusion_matrix_test[1,1])\n",
    "\n",
    "print(\"Test Specificity: \",specificity_Test_logReg)\n",
    "print(\"Test Recall: \",recall_Test_logReg)\n",
    "print(\"Test Precision: \",precision_Test_logReg)\n",
    "print(\"Test Accuracy: \",accuracy_Test_logReg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------\n",
    "-------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/fc_dense_layers_keras.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A. The core data structure of Keras is a model, a way to organize layers. The simplest type of model is the Sequential model, a linear stack of layers. \n",
    "\n",
    "* The keras sequential api enables us to build common yet complex neural network architectures flexibly\n",
    "\n",
    "* Objects of the Keras sequential class, can have multiple neural network layers stacked on top of one another\n",
    "\n",
    "![](img/keras_sequential_api.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "\n",
    "model.add(Dense(units=64, input_dim=100, activation='relu', ))\n",
    "model.add(Dense(units=10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 64)                6464      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 7,114\n",
      "Trainable params: 7,114\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### B. Compilation\n",
    "Before training a model, you need to configure the learning process, which is done via the compile method. receives three arguments\n",
    "\n",
    "* optimizer - An optimizer. An optimizer is an algorithm that uses this feedback signal, to actually update the weights so that the output from the network gets closer to the ground truth.\n",
    "* loss - A loss function. This is the objective that the model will try to minimize.\n",
    "* metrics - A list of error metrics. This is for users reference and does not add value to the weights calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss = 'binary_crossentropy', optimizer = 'sgd', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### C. Training\n",
    "Keras models are trained on Numpy arrays of input data and labels. For training a model, you will typically use the  fit function\n",
    "\n",
    "* epoch = one forward pass and one backward pass of all the training examples\n",
    "* batch size = the number of training examples in one forward/backward pass. The higher the batch size, the more memory space you'll need.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NOTE : Don't run the following line of code as we do not yet have X_train and y_train\n",
    "\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANN Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A. Base model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "perceptron_model = Sequential()\n",
    "\n",
    "perceptron_model.add(Dense(1, input_dim=21, activation='sigmoid', kernel_initializer='normal'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "perceptron_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "40730/40730 [==============================] - 2s 55us/step - loss: 0.4925 - acc: 0.8387: 1s - loss: 0.5659 - a\n",
      "Epoch 2/30\n",
      "40730/40730 [==============================] - 1s 21us/step - loss: 0.4047 - acc: 0.8509\n",
      "Epoch 3/30\n",
      "40730/40730 [==============================] - 1s 21us/step - loss: 0.4035 - acc: 0.8543\n",
      "Epoch 4/30\n",
      "40730/40730 [==============================] - 1s 20us/step - loss: 0.4158 - acc: 0.8576\n",
      "Epoch 5/30\n",
      "40730/40730 [==============================] - 1s 22us/step - loss: 0.4564 - acc: 0.8557: 0s - loss: 0.3947 - \n",
      "Epoch 6/30\n",
      "40730/40730 [==============================] - 1s 22us/step - loss: 0.4113 - acc: 0.8601\n",
      "Epoch 7/30\n",
      "40730/40730 [==============================] - 1s 21us/step - loss: 0.4109 - acc: 0.8628\n",
      "Epoch 8/30\n",
      "40730/40730 [==============================] - 1s 21us/step - loss: 0.4036 - acc: 0.8635\n",
      "Epoch 9/30\n",
      "40730/40730 [==============================] - 1s 21us/step - loss: 0.4209 - acc: 0.8634\n",
      "Epoch 10/30\n",
      "40730/40730 [==============================] - 1s 21us/step - loss: 0.4035 - acc: 0.8650\n",
      "Epoch 11/30\n",
      "40730/40730 [==============================] - 1s 22us/step - loss: 0.3967 - acc: 0.8654\n",
      "Epoch 12/30\n",
      "40730/40730 [==============================] - 1s 23us/step - loss: 0.5531 - acc: 0.8541\n",
      "Epoch 13/30\n",
      "40730/40730 [==============================] - 1s 22us/step - loss: 0.3983 - acc: 0.8702\n",
      "Epoch 14/30\n",
      "40730/40730 [==============================] - 1s 22us/step - loss: 0.5476 - acc: 0.8555\n",
      "Epoch 15/30\n",
      "40730/40730 [==============================] - 1s 22us/step - loss: 0.5602 - acc: 0.8577\n",
      "Epoch 16/30\n",
      "40730/40730 [==============================] - 1s 22us/step - loss: 0.4299 - acc: 0.8726: 0s - loss: 0.4701 - a\n",
      "Epoch 17/30\n",
      "40730/40730 [==============================] - 1s 22us/step - loss: 0.4050 - acc: 0.8706\n",
      "Epoch 18/30\n",
      "40730/40730 [==============================] - 1s 20us/step - loss: 0.3980 - acc: 0.8698\n",
      "Epoch 19/30\n",
      "40730/40730 [==============================] - 1s 20us/step - loss: 0.3967 - acc: 0.8704\n",
      "Epoch 20/30\n",
      "40730/40730 [==============================] - 1s 20us/step - loss: 0.4211 - acc: 0.8702\n",
      "Epoch 21/30\n",
      "40730/40730 [==============================] - 1s 20us/step - loss: 0.4058 - acc: 0.8705\n",
      "Epoch 22/30\n",
      "40730/40730 [==============================] - 1s 20us/step - loss: 0.4048 - acc: 0.8689\n",
      "Epoch 23/30\n",
      "40730/40730 [==============================] - 1s 21us/step - loss: 0.4009 - acc: 0.8691\n",
      "Epoch 24/30\n",
      "40730/40730 [==============================] - 1s 21us/step - loss: 0.4570 - acc: 0.8634\n",
      "Epoch 25/30\n",
      "40730/40730 [==============================] - 1s 21us/step - loss: 0.4111 - acc: 0.8675\n",
      "Epoch 26/30\n",
      "40730/40730 [==============================] - 1s 20us/step - loss: 0.3960 - acc: 0.8679: 0s - loss: 0.3971 - \n",
      "Epoch 27/30\n",
      "40730/40730 [==============================] - 1s 20us/step - loss: 0.3946 - acc: 0.8675\n",
      "Epoch 28/30\n",
      "40730/40730 [==============================] - 1s 20us/step - loss: 0.4051 - acc: 0.8641\n",
      "Epoch 29/30\n",
      "40730/40730 [==============================] - 1s 20us/step - loss: 0.4110 - acc: 0.8646: 0s - loss: 0.4034 - acc: \n",
      "Epoch 30/30\n",
      "40730/40730 [==============================] - 1s 20us/step - loss: 0.4273 - acc: 0.8637\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e893910a20>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perceptron_model.fit(X_train, y_train, epochs=30, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[32322   730]\n",
      " [ 5121  2557]]\n",
      "[[13833   332]\n",
      " [ 2178  1113]]\n"
     ]
    }
   ],
   "source": [
    "test_pred=perceptron_model.predict_classes(X_test)\n",
    "train_pred=perceptron_model.predict_classes(X_train)\n",
    "\n",
    "confusion_matrix_test = confusion_matrix(y_test, test_pred)\n",
    "confusion_matrix_train = confusion_matrix(y_train, train_pred)\n",
    "\n",
    "print(confusion_matrix_train)\n",
    "print(confusion_matrix_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train Test Accuracy, True Negative Rate and True Positive Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train TNR:  0.966477066441\n",
      "Train TPR:  0.456759572805\n",
      "Train Accuracy:  0.870390375644\n",
      "-----------------------\n",
      "Test TNR:  0.964701729615\n",
      "Test TPR:  0.449407474932\n",
      "Test Accuracy:  0.867552703941\n"
     ]
    }
   ],
   "source": [
    "Accuracy_Train=(confusion_matrix_train[0,0]+confusion_matrix_train[1,1])/(confusion_matrix_train[0,0]+confusion_matrix_train[0,1]+confusion_matrix_train[1,0]+confusion_matrix_train[1,1])\n",
    "TNR_Train= confusion_matrix_train[0,0]/(confusion_matrix_train[0,0]+confusion_matrix_train[0,1])\n",
    "TPR_Train= confusion_matrix_train[1,1]/(confusion_matrix_train[1,0]+confusion_matrix_train[1,1])\n",
    "\n",
    "print(\"Train TNR: \",TNR_Train)\n",
    "print(\"Train TPR: \",TPR_Train)\n",
    "print(\"Train Accuracy: \",Accuracy_Train)\n",
    "print(\"-----------------------\")\n",
    "\n",
    "Accuracy_Test=(confusion_matrix_test[0,0]+confusion_matrix_test[1,1])/(confusion_matrix_test[0,0]+confusion_matrix_test[0,1]+confusion_matrix_test[1,0]+confusion_matrix_test[1,1])\n",
    "TNR_Test= confusion_matrix_test[0,0]/(confusion_matrix_test[0,0] +confusion_matrix_test[0,1])\n",
    "TPR_Test= confusion_matrix_test[1,1]/(confusion_matrix_test[1,0] +confusion_matrix_test[1,1])\n",
    "\n",
    "print(\"Test TNR: \",TNR_Test)\n",
    "print(\"Test TPR: \",TPR_Test)\n",
    "print(\"Test Accuracy: \",Accuracy_Test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding a hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mlp_model = Sequential()\n",
    "\n",
    "mlp_model.add(Dense(12, input_dim=21, activation='relu', kernel_initializer='normal'))\n",
    "mlp_model.add(Dense(1, activation='sigmoid', kernel_initializer='normal'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mlp_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32584 samples, validate on 8146 samples\n",
      "Epoch 1/30\n",
      "32584/32584 [==============================] - 1s 37us/step - loss: 0.4125 - acc: 0.8215 - val_loss: 0.3303 - val_acc: 0.8526\n",
      "Epoch 2/30\n",
      "32584/32584 [==============================] - 1s 22us/step - loss: 0.3080 - acc: 0.8667 - val_loss: 0.3047 - val_acc: 0.8802\n",
      "Epoch 3/30\n",
      "32584/32584 [==============================] - 1s 22us/step - loss: 0.2924 - acc: 0.8878 - val_loss: 0.2923 - val_acc: 0.8936\n",
      "Epoch 4/30\n",
      "32584/32584 [==============================] - 1s 22us/step - loss: 0.2785 - acc: 0.8962 - val_loss: 0.2871 - val_acc: 0.8920\n",
      "Epoch 5/30\n",
      "32584/32584 [==============================] - 1s 22us/step - loss: 0.2737 - acc: 0.8965 - val_loss: 0.2810 - val_acc: 0.8968\n",
      "Epoch 6/30\n",
      "32584/32584 [==============================] - 1s 22us/step - loss: 0.2679 - acc: 0.8983 - val_loss: 0.3024 - val_acc: 0.8891\n",
      "Epoch 7/30\n",
      "32584/32584 [==============================] - 1s 30us/step - loss: 0.2689 - acc: 0.8980 - val_loss: 0.2706 - val_acc: 0.8950\n",
      "Epoch 8/30\n",
      "32584/32584 [==============================] - 1s 23us/step - loss: 0.2590 - acc: 0.8994 - val_loss: 0.2690 - val_acc: 0.8990\n",
      "Epoch 9/30\n",
      "32584/32584 [==============================] - 1s 24us/step - loss: 0.2592 - acc: 0.8999 - val_loss: 0.2638 - val_acc: 0.9003\n",
      "Epoch 10/30\n",
      "32584/32584 [==============================] - 1s 24us/step - loss: 0.2556 - acc: 0.9019 - val_loss: 0.2672 - val_acc: 0.8952\n",
      "Epoch 11/30\n",
      "32584/32584 [==============================] - 1s 23us/step - loss: 0.2553 - acc: 0.9013 - val_loss: 0.2578 - val_acc: 0.9008\n",
      "Epoch 12/30\n",
      "32584/32584 [==============================] - 1s 24us/step - loss: 0.2532 - acc: 0.9025 - val_loss: 0.2589 - val_acc: 0.8988\n",
      "Epoch 13/30\n",
      "32584/32584 [==============================] - 1s 24us/step - loss: 0.2500 - acc: 0.9014 - val_loss: 0.2553 - val_acc: 0.9009\n",
      "Epoch 14/30\n",
      "32584/32584 [==============================] - 1s 23us/step - loss: 0.2495 - acc: 0.8992 - val_loss: 0.2621 - val_acc: 0.8977\n",
      "Epoch 15/30\n",
      "32584/32584 [==============================] - 1s 24us/step - loss: 0.2500 - acc: 0.8997 - val_loss: 0.2608 - val_acc: 0.9025\n",
      "Epoch 16/30\n",
      "32584/32584 [==============================] - 1s 24us/step - loss: 0.2472 - acc: 0.9011 - val_loss: 0.2623 - val_acc: 0.8985\n",
      "Epoch 17/30\n",
      "32584/32584 [==============================] - 1s 24us/step - loss: 0.2469 - acc: 0.9016 - val_loss: 0.2616 - val_acc: 0.8968\n",
      "Epoch 18/30\n",
      "32584/32584 [==============================] - 1s 23us/step - loss: 0.2472 - acc: 0.9006 - val_loss: 0.2613 - val_acc: 0.8925\n",
      "Epoch 19/30\n",
      "32584/32584 [==============================] - 1s 24us/step - loss: 0.2445 - acc: 0.9017 - val_loss: 0.2584 - val_acc: 0.9013\n",
      "Epoch 20/30\n",
      "32584/32584 [==============================] - 1s 23us/step - loss: 0.2468 - acc: 0.9009 - val_loss: 0.2618 - val_acc: 0.9004\n",
      "Epoch 21/30\n",
      "32584/32584 [==============================] - 1s 24us/step - loss: 0.2472 - acc: 0.9011 - val_loss: 0.2595 - val_acc: 0.9033\n",
      "Epoch 22/30\n",
      "32584/32584 [==============================] - 1s 23us/step - loss: 0.2439 - acc: 0.9013 - val_loss: 0.2569 - val_acc: 0.9019\n",
      "Epoch 23/30\n",
      "32584/32584 [==============================] - 1s 23us/step - loss: 0.2432 - acc: 0.9012 - val_loss: 0.2650 - val_acc: 0.9001\n",
      "Epoch 24/30\n",
      "32584/32584 [==============================] - 1s 23us/step - loss: 0.2447 - acc: 0.9028 - val_loss: 0.2605 - val_acc: 0.9025\n",
      "Epoch 25/30\n",
      "32584/32584 [==============================] - 1s 23us/step - loss: 0.2440 - acc: 0.9020 - val_loss: 0.2528 - val_acc: 0.9049\n",
      "Epoch 26/30\n",
      "32584/32584 [==============================] - 1s 26us/step - loss: 0.2425 - acc: 0.9037 - val_loss: 0.2526 - val_acc: 0.8980\n",
      "Epoch 27/30\n",
      "32584/32584 [==============================] - 1s 25us/step - loss: 0.2408 - acc: 0.9028 - val_loss: 0.2544 - val_acc: 0.9006\n",
      "Epoch 28/30\n",
      "32584/32584 [==============================] - 1s 25us/step - loss: 0.2406 - acc: 0.9031 - val_loss: 0.2548 - val_acc: 0.9028\n",
      "Epoch 29/30\n",
      "32584/32584 [==============================] - 1s 25us/step - loss: 0.2404 - acc: 0.9034 - val_loss: 0.2539 - val_acc: 0.9041\n",
      "Epoch 30/30\n",
      "32584/32584 [==============================] - 1s 25us/step - loss: 0.2391 - acc: 0.9032 - val_loss: 0.2514 - val_acc: 0.9002\n"
     ]
    }
   ],
   "source": [
    "#model_history = ann_model.fit(X_train, y_train, epochs=100, batch_size=64, validation_split=0.2)\n",
    "model_history = mlp_model.fit(X_train, y_train, epochs=30, batch_size=64, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13376/17456 [=====================>........] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "train_pred = mlp_model.predict_classes(X_train)\n",
    "\n",
    "test_pred = mlp_model.predict_classes(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting evaluation metrics and evaluating model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[32322   730]\n",
      " [ 5121  2557]]\n",
      "[[13833   332]\n",
      " [ 2178  1113]]\n"
     ]
    }
   ],
   "source": [
    "confusion_matrix_train = confusion_matrix(y_train, train_pred)\n",
    "confusion_matrix_test = confusion_matrix(y_test, test_pred)\n",
    "\n",
    "print(confusion_matrix_train)\n",
    "print(confusion_matrix_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Accuracy, True Positive Rate and True Negative Rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Specificity:  0.977913590706\n",
      "Train Recall:  0.333029434749\n",
      "Train Precision:  0.777912990569\n",
      "Train Accuracy:  0.856346673214\n"
     ]
    }
   ],
   "source": [
    "# Metrics on train data for ann_model 1\n",
    "#Accuracy\n",
    "accuracy_Train_M1 = (confusion_matrix_train[0,0]+confusion_matrix_train[1,1])/(confusion_matrix_train[0,0]+confusion_matrix_train[0,1]+confusion_matrix_train[1,0]+confusion_matrix_train[1,1])\n",
    "#specificity or true negative rate (TNR)\n",
    "specificity_Train_M1 = confusion_matrix_train[0,0]/(confusion_matrix_train[0,0]+confusion_matrix_train[0,1])\n",
    "#sensitivity, recall, hit rate, or true positive rate (TPR)\n",
    "recall_Train_M1 = confusion_matrix_train[1,1]/(confusion_matrix_train[1,0]+confusion_matrix_train[1,1])\n",
    "#precision\n",
    "precision_Train_M1 = confusion_matrix_train[1,1]/(confusion_matrix_train[0,1]+confusion_matrix_train[1,1])\n",
    "\n",
    "print(\"Train Specificity: \",specificity_Train_M1)\n",
    "print(\"Train Recall: \",recall_Train_M1)\n",
    "print(\"Train Precision: \",precision_Train_M1)\n",
    "print(\"Train Accuracy: \",accuracy_Train_M1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Specificity:  0.976561948465\n",
      "Test Recall:  0.338195077484\n",
      "Test Precision:  0.770242214533\n",
      "Test Accuracy:  0.856209899175\n"
     ]
    }
   ],
   "source": [
    "# Metrics on test data\n",
    "#Accuracy\n",
    "accuracy_Test_M1 = (confusion_matrix_test[0,0]+confusion_matrix_test[1,1])/(confusion_matrix_test[0,0]+confusion_matrix_test[0,1]+confusion_matrix_test[1,0]+confusion_matrix_test[1,1])\n",
    "#specificity or true negative rate (TNR)\n",
    "specificity_Test_M1 = confusion_matrix_test[0,0]/(confusion_matrix_test[0,0]+confusion_matrix_test[0,1])\n",
    "#sensitivity, recall, hit rate, or true positive rate (TPR)\n",
    "recall_Test_M1 = confusion_matrix_test[1,1]/(confusion_matrix_test[1,0]+confusion_matrix_test[1,1])\n",
    "#precision\n",
    "precision_Test_M1 = confusion_matrix_test[1,1]/(confusion_matrix_test[0,1]+confusion_matrix_test[1,1])\n",
    "\n",
    "print(\"Test Specificity: \",specificity_Test_M1)\n",
    "print(\"Test Recall: \",recall_Test_M1)\n",
    "print(\"Test Precision: \",precision_Test_M1)\n",
    "print(\"Test Accuracy: \",accuracy_Test_M1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n"
     ]
    }
   ],
   "source": [
    "print(model_history.history.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEWCAYAAAB1xKBvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nn2jAUEop5RMNGEoppXzy/wEo6UPaZvcQNAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1e897dd4dd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(model_history.history['acc'])\n",
    "plt.plot(model_history.history['val_acc'])\n",
    "plt.title('Accuracy Plot')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(model_history.history['loss'])\n",
    "plt.plot(model_history.history['val_loss'])\n",
    "plt.title('Loss Function Plot')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimenting with Hidden Layers\n",
    "\n",
    "We have to be careful about overfitting!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32584 samples, validate on 8146 samples\n",
      "Epoch 1/10\n",
      "32584/32584 [==============================] - 6s - loss: 0.4803 - acc: 0.8091 - val_loss: 0.4780 - val_acc: 0.8048\n",
      "Epoch 2/10\n",
      "32584/32584 [==============================] - 6s - loss: 0.4544 - acc: 0.8132 - val_loss: 0.4499 - val_acc: 0.8048\n",
      "Epoch 3/10\n",
      "32584/32584 [==============================] - 6s - loss: 0.4191 - acc: 0.8132 - val_loss: 0.4048 - val_acc: 0.8048\n",
      "Epoch 4/10\n",
      "32584/32584 [==============================] - 6s - loss: 0.3736 - acc: 0.8185 - val_loss: 0.3582 - val_acc: 0.8265\n",
      "Epoch 5/10\n",
      "32584/32584 [==============================] - 6s - loss: 0.3329 - acc: 0.8498 - val_loss: 0.3238 - val_acc: 0.8493\n",
      "Epoch 6/10\n",
      "32584/32584 [==============================] - 6s - loss: 0.3037 - acc: 0.8645 - val_loss: 0.2970 - val_acc: 0.8668\n",
      "Epoch 7/10\n",
      "32584/32584 [==============================] - 6s - loss: 0.2833 - acc: 0.8764 - val_loss: 0.2795 - val_acc: 0.8750\n",
      "Epoch 8/10\n",
      "32584/32584 [==============================] - 6s - loss: 0.2704 - acc: 0.8816 - val_loss: 0.2699 - val_acc: 0.8799\n",
      "Epoch 9/10\n",
      "32584/32584 [==============================] - 6s - loss: 0.2627 - acc: 0.8875 - val_loss: 0.2745 - val_acc: 0.8839\n",
      "Epoch 10/10\n",
      "32584/32584 [==============================] - 6s - loss: 0.2578 - acc: 0.8904 - val_loss: 0.2607 - val_acc: 0.8862\n",
      "17152/17456 [============================>.] - ETA: 0sConfusion matrices:\n",
      "---------------------\n",
      "Confusion matrix - Train:\n",
      "[[31609  1443]\n",
      " [ 3027  4651]]\n",
      "Confusion matrix - Test:\n",
      "[[13532   633]\n",
      " [ 1324  1967]]\n",
      "---------------------\n",
      "Evaluation metrics on train data for new model:\n",
      "------------------------------------\n",
      "Train Specificity:  0.956341522449\n",
      "Train Recall:  0.605756707476\n",
      "Train Precision:  0.763209714473\n",
      "Train Accuracy:  0.890252884851\n",
      "------------------------------------\n",
      "Evaluation metrics on train data for model 1:\n",
      "Train Specificity:  0.948142321191\n",
      "Train Recall:  0.723625944256\n",
      "Train Precision:  0.764236588721\n",
      "Train Accuracy:  0.905818806776\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "Evaluation metrics on test data for new model:\n",
      "------------------------------------\n",
      "Test Specificity:  0.955312389693\n",
      "Test Recall:  0.597690671528\n",
      "Test Precision:  0.756538461538\n",
      "Test Accuracy:  0.887889550871\n",
      "------------------------------------\n",
      "Evaluation metrics on test data for model 1:\n",
      "Test Specificity:  0.945570067067\n",
      "Test Recall:  0.708295350957\n",
      "Test Precision:  0.751450676983\n",
      "Test Accuracy:  0.900836388634\n",
      "------------------------------------\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "ann_model_hiddenExp = Sequential()\n",
    "\n",
    "# Adding more hidden layers\n",
    "ann_model_hiddenExp.add(Dense(1000, input_dim=21, activation='sigmoid', kernel_initializer='normal'))\n",
    "ann_model_hiddenExp.add(Dense(500, activation='sigmoid', kernel_initializer='normal'))\n",
    "ann_model_hiddenExp.add(Dense(100, activation='sigmoid', kernel_initializer='normal'))\n",
    "ann_model_hiddenExp.add(Dense(1, activation='sigmoid', kernel_initializer='normal'))\n",
    "\n",
    "ann_model_hiddenExp.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\n",
    "ann_model_hiddenExp.fit(X_train, y_train, epochs=10, batch_size=64, validation_split=0.2)\n",
    "\n",
    "# Predictions\n",
    "train_pred = ann_model_hiddenExp.predict_classes(X_train)\n",
    "test_pred = ann_model_hiddenExp.predict_classes(X_test)\n",
    "\n",
    "#Evaluation metrics\n",
    "confusion_matrix_train = confusion_matrix(y_train, train_pred)\n",
    "confusion_matrix_test = confusion_matrix(y_test, test_pred)\n",
    "\n",
    "print(\"Confusion matrices:\")\n",
    "print(\"---------------------\")\n",
    "print(\"Confusion matrix - Train:\")\n",
    "print(confusion_matrix_train)\n",
    "print(\"Confusion matrix - Test:\")\n",
    "print(confusion_matrix_test)\n",
    "print(\"---------------------\")\n",
    "\n",
    "# Metrics on train data\n",
    "#Accuracy\n",
    "accuracy_Train_hiddenExp = (confusion_matrix_train[0,0]+confusion_matrix_train[1,1])/(confusion_matrix_train[0,0]+confusion_matrix_train[0,1]+confusion_matrix_train[1,0]+confusion_matrix_train[1,1])\n",
    "#specificity or true negative rate (TNR)\n",
    "specificity_Train_hiddenExp = confusion_matrix_train[0,0]/(confusion_matrix_train[0,0]+confusion_matrix_train[0,1])\n",
    "#sensitivity, recall, hit rate, or true positive rate (TPR)\n",
    "recall_Train_hiddenExp = confusion_matrix_train[1,1]/(confusion_matrix_train[1,0]+confusion_matrix_train[1,1])\n",
    "#precision\n",
    "precision_Train_hiddenExp = confusion_matrix_train[1,1]/(confusion_matrix_train[0,1]+confusion_matrix_train[1,1])\n",
    "\n",
    "print(\"Evaluation metrics on train data for new model:\")\n",
    "print(\"------------------------------------\")\n",
    "print(\"Train Specificity: \",specificity_Train_hiddenExp)\n",
    "print(\"Train Recall: \",recall_Train_hiddenExp)\n",
    "print(\"Train Precision: \",precision_Train_hiddenExp)\n",
    "print(\"Train Accuracy: \",accuracy_Train_hiddenExp)\n",
    "print(\"------------------------------------\")\n",
    "\n",
    "print(\"Evaluation metrics on train data for model 1:\")\n",
    "print(\"Train Specificity: \",specificity_Train_M1)\n",
    "print(\"Train Recall: \",recall_Train_M1)\n",
    "print(\"Train Precision: \",precision_Train_M1)\n",
    "print(\"Train Accuracy: \",accuracy_Train_M1)\n",
    "print(\"------------------------------------\")\n",
    "print(\"------------------------------------\")\n",
    "\n",
    "# Metrics on test data\n",
    "#Accuracy\n",
    "accuracy_Test_hiddenExp = (confusion_matrix_test[0,0]+confusion_matrix_test[1,1])/(confusion_matrix_test[0,0]+confusion_matrix_test[0,1]+confusion_matrix_test[1,0]+confusion_matrix_test[1,1])\n",
    "#specificity or true negative rate (TNR)\n",
    "specificity_Test_hiddenExp = confusion_matrix_test[0,0]/(confusion_matrix_test[0,0]+confusion_matrix_test[0,1])\n",
    "#sensitivity, recall, hit rate, or true positive rate (TPR)\n",
    "recall_Test_hiddenExp = confusion_matrix_test[1,1]/(confusion_matrix_test[1,0]+confusion_matrix_test[1,1])\n",
    "#precision\n",
    "precision_Test_hiddenExp = confusion_matrix_test[1,1]/(confusion_matrix_test[0,1]+confusion_matrix_test[1,1])\n",
    "\n",
    "print(\"Evaluation metrics on test data for new model:\")\n",
    "print(\"------------------------------------\")\n",
    "print(\"Test Specificity: \",specificity_Test_hiddenExp)\n",
    "print(\"Test Recall: \",recall_Test_hiddenExp)\n",
    "print(\"Test Precision: \",precision_Test_hiddenExp)\n",
    "print(\"Test Accuracy: \",accuracy_Test_hiddenExp)\n",
    "print(\"------------------------------------\")\n",
    "\n",
    "print(\"Evaluation metrics on test data for model 1:\")\n",
    "print(\"Test Specificity: \",specificity_Test_M1)\n",
    "print(\"Test Recall: \",recall_Test_M1)\n",
    "print(\"Test Precision: \",precision_Test_M1)\n",
    "print(\"Test Accuracy: \",accuracy_Test_M1)\n",
    "print(\"------------------------------------\")\n",
    "print(\"------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimenting with learning rates\n",
    "\n",
    "Learning rate is an important hyper-parameter for nn models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------\n",
      "Modelling for learning rate -  1e-05\n",
      "--------------------------------------------------------\n",
      "Train on 32584 samples, validate on 8146 samples\n",
      "Epoch 1/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.6353 - acc: 0.8132 - val_loss: 0.6351 - val_acc: 0.8048\n",
      "Epoch 2/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.6312 - acc: 0.8132 - val_loss: 0.6312 - val_acc: 0.8048\n",
      "Epoch 3/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.6272 - acc: 0.8132 - val_loss: 0.6275 - val_acc: 0.8048\n",
      "Epoch 4/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.6233 - acc: 0.8132 - val_loss: 0.6238 - val_acc: 0.8048\n",
      "Epoch 5/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.6195 - acc: 0.8132 - val_loss: 0.6203 - val_acc: 0.8048\n",
      "Epoch 6/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.6159 - acc: 0.8132 - val_loss: 0.6168 - val_acc: 0.8048\n",
      "Epoch 7/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.6123 - acc: 0.8132 - val_loss: 0.6134 - val_acc: 0.8048\n",
      "Epoch 8/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.6089 - acc: 0.8132 - val_loss: 0.6100 - val_acc: 0.8048\n",
      "Epoch 9/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.6055 - acc: 0.8132 - val_loss: 0.6067 - val_acc: 0.8048\n",
      "Epoch 10/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.6021 - acc: 0.8132 - val_loss: 0.6035 - val_acc: 0.8048\n",
      "Epoch 11/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.5988 - acc: 0.8132 - val_loss: 0.6004 - val_acc: 0.8048\n",
      "Epoch 12/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.5956 - acc: 0.8132 - val_loss: 0.5974 - val_acc: 0.8048\n",
      "Epoch 13/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.5926 - acc: 0.8132 - val_loss: 0.5946 - val_acc: 0.8048\n",
      "Epoch 14/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.5898 - acc: 0.8132 - val_loss: 0.5919 - val_acc: 0.8048\n",
      "Epoch 15/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.5871 - acc: 0.8132 - val_loss: 0.5894 - val_acc: 0.8048\n",
      "Epoch 16/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.5844 - acc: 0.8132 - val_loss: 0.5869 - val_acc: 0.8048\n",
      "Epoch 17/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.5819 - acc: 0.8132 - val_loss: 0.5845 - val_acc: 0.8048\n",
      "Epoch 18/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.5795 - acc: 0.8132 - val_loss: 0.5822 - val_acc: 0.8048\n",
      "Epoch 19/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.5771 - acc: 0.8132 - val_loss: 0.5799 - val_acc: 0.8048\n",
      "Epoch 20/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.5748 - acc: 0.8132 - val_loss: 0.5778 - val_acc: 0.8048\n",
      "Epoch 21/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.5725 - acc: 0.8132 - val_loss: 0.5757 - val_acc: 0.8048\n",
      "Epoch 22/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.5704 - acc: 0.8132 - val_loss: 0.5736 - val_acc: 0.8048\n",
      "Epoch 23/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.5683 - acc: 0.8132 - val_loss: 0.5717 - val_acc: 0.8048\n",
      "Epoch 24/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.5662 - acc: 0.8132 - val_loss: 0.5697 - val_acc: 0.8048\n",
      "Epoch 25/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.5642 - acc: 0.8132 - val_loss: 0.5679 - val_acc: 0.8048\n",
      "Epoch 26/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.5623 - acc: 0.8132 - val_loss: 0.5661 - val_acc: 0.8048\n",
      "Epoch 27/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.5604 - acc: 0.8132 - val_loss: 0.5643 - val_acc: 0.8048\n",
      "Epoch 28/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.5586 - acc: 0.8132 - val_loss: 0.5626 - val_acc: 0.8048\n",
      "Epoch 29/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.5568 - acc: 0.8132 - val_loss: 0.5609 - val_acc: 0.8048\n",
      "Epoch 30/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.5550 - acc: 0.8132 - val_loss: 0.5592 - val_acc: 0.8048\n",
      "Epoch 31/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.5533 - acc: 0.8132 - val_loss: 0.5576 - val_acc: 0.8048\n",
      "Epoch 32/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.5516 - acc: 0.8132 - val_loss: 0.5561 - val_acc: 0.8048\n",
      "Epoch 33/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.5500 - acc: 0.8132 - val_loss: 0.5545 - val_acc: 0.8048\n",
      "Epoch 34/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.5484 - acc: 0.8132 - val_loss: 0.5530 - val_acc: 0.8048\n",
      "Epoch 35/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.5469 - acc: 0.8132 - val_loss: 0.5516 - val_acc: 0.8048\n",
      "Epoch 36/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.5454 - acc: 0.8132 - val_loss: 0.5502 - val_acc: 0.8048\n",
      "Epoch 37/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.5439 - acc: 0.8132 - val_loss: 0.5488 - val_acc: 0.8048\n",
      "Epoch 38/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.5425 - acc: 0.8132 - val_loss: 0.5475 - val_acc: 0.8048\n",
      "Epoch 39/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.5411 - acc: 0.8132 - val_loss: 0.5461 - val_acc: 0.8048\n",
      "Epoch 40/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.5397 - acc: 0.8132 - val_loss: 0.5448 - val_acc: 0.8048\n",
      "Epoch 41/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.5383 - acc: 0.8132 - val_loss: 0.5436 - val_acc: 0.8048\n",
      "Epoch 42/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.5370 - acc: 0.8132 - val_loss: 0.5423 - val_acc: 0.8048\n",
      "Epoch 43/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.5357 - acc: 0.8132 - val_loss: 0.5411 - val_acc: 0.8048\n",
      "Epoch 44/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.5345 - acc: 0.8132 - val_loss: 0.5399 - val_acc: 0.8048\n",
      "Epoch 45/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.5332 - acc: 0.8132 - val_loss: 0.5388 - val_acc: 0.8048\n",
      "Epoch 46/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.5320 - acc: 0.8132 - val_loss: 0.5376 - val_acc: 0.8048\n",
      "Epoch 47/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.5308 - acc: 0.8132 - val_loss: 0.5365 - val_acc: 0.8048\n",
      "Epoch 48/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.5297 - acc: 0.8132 - val_loss: 0.5354 - val_acc: 0.8048\n",
      "Epoch 49/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.5285 - acc: 0.8132 - val_loss: 0.5344 - val_acc: 0.8048\n",
      "Epoch 50/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.5274 - acc: 0.8132 - val_loss: 0.5333 - val_acc: 0.8048\n",
      "Epoch 51/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.5263 - acc: 0.8132 - val_loss: 0.5323 - val_acc: 0.8048\n",
      "Epoch 52/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.5252 - acc: 0.8132 - val_loss: 0.5313 - val_acc: 0.8048\n",
      "Epoch 53/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.5242 - acc: 0.8132 - val_loss: 0.5303 - val_acc: 0.8048\n",
      "Epoch 54/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.5231 - acc: 0.8132 - val_loss: 0.5293 - val_acc: 0.8048\n",
      "Epoch 55/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.5221 - acc: 0.8132 - val_loss: 0.5283 - val_acc: 0.8048\n",
      "Epoch 56/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.5211 - acc: 0.8132 - val_loss: 0.5274 - val_acc: 0.8048\n",
      "Epoch 57/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.5201 - acc: 0.8132 - val_loss: 0.5265 - val_acc: 0.8048\n",
      "Epoch 58/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.5191 - acc: 0.8132 - val_loss: 0.5256 - val_acc: 0.8048\n",
      "Epoch 59/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.5182 - acc: 0.8132 - val_loss: 0.5247 - val_acc: 0.8048\n",
      "Epoch 60/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.5172 - acc: 0.8132 - val_loss: 0.5238 - val_acc: 0.8048\n",
      "Epoch 61/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.5163 - acc: 0.8132 - val_loss: 0.5229 - val_acc: 0.8048\n",
      "Epoch 62/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.5154 - acc: 0.8132 - val_loss: 0.5220 - val_acc: 0.8048\n",
      "Epoch 63/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32584/32584 [==============================] - 0s - loss: 0.5145 - acc: 0.8132 - val_loss: 0.5212 - val_acc: 0.8048\n",
      "Epoch 64/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.5136 - acc: 0.8132 - val_loss: 0.5204 - val_acc: 0.8048\n",
      "Epoch 65/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.5127 - acc: 0.8132 - val_loss: 0.5195 - val_acc: 0.8048\n",
      "Epoch 66/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.5118 - acc: 0.8132 - val_loss: 0.5187 - val_acc: 0.8048\n",
      "Epoch 67/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.5110 - acc: 0.8132 - val_loss: 0.5179 - val_acc: 0.8048\n",
      "Epoch 68/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.5101 - acc: 0.8132 - val_loss: 0.5171 - val_acc: 0.8048\n",
      "Epoch 69/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.5093 - acc: 0.8132 - val_loss: 0.5163 - val_acc: 0.8048\n",
      "Epoch 70/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.5084 - acc: 0.8132 - val_loss: 0.5155 - val_acc: 0.8048\n",
      "Epoch 71/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.5076 - acc: 0.8132 - val_loss: 0.5148 - val_acc: 0.8048\n",
      "Epoch 72/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.5068 - acc: 0.8132 - val_loss: 0.5140 - val_acc: 0.8048\n",
      "Epoch 73/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.5060 - acc: 0.8132 - val_loss: 0.5132 - val_acc: 0.8048\n",
      "Epoch 74/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.5051 - acc: 0.8132 - val_loss: 0.5124 - val_acc: 0.8048\n",
      "Epoch 75/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.5042 - acc: 0.8132 - val_loss: 0.5115 - val_acc: 0.8048\n",
      "Epoch 76/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.5033 - acc: 0.8132 - val_loss: 0.5107 - val_acc: 0.8048\n",
      "Epoch 77/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.5025 - acc: 0.8132 - val_loss: 0.5100 - val_acc: 0.8048\n",
      "Epoch 78/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.5017 - acc: 0.8132 - val_loss: 0.5092 - val_acc: 0.8048\n",
      "Epoch 79/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.5010 - acc: 0.8132 - val_loss: 0.5085 - val_acc: 0.8048\n",
      "Epoch 80/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.5002 - acc: 0.8132 - val_loss: 0.5079 - val_acc: 0.8048\n",
      "Epoch 81/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.4995 - acc: 0.8132 - val_loss: 0.5072 - val_acc: 0.8048\n",
      "Epoch 82/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.4989 - acc: 0.8132 - val_loss: 0.5066 - val_acc: 0.8048\n",
      "Epoch 83/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.4982 - acc: 0.8132 - val_loss: 0.5059 - val_acc: 0.8048\n",
      "Epoch 84/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.4975 - acc: 0.8132 - val_loss: 0.5053 - val_acc: 0.8048\n",
      "Epoch 85/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.4969 - acc: 0.8132 - val_loss: 0.5047 - val_acc: 0.8048\n",
      "Epoch 86/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.4962 - acc: 0.8132 - val_loss: 0.5041 - val_acc: 0.8048\n",
      "Epoch 87/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.4956 - acc: 0.8132 - val_loss: 0.5035 - val_acc: 0.8048\n",
      "Epoch 88/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.4949 - acc: 0.8132 - val_loss: 0.5029 - val_acc: 0.8048\n",
      "Epoch 89/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.4943 - acc: 0.8132 - val_loss: 0.5023 - val_acc: 0.8048\n",
      "Epoch 90/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.4937 - acc: 0.8132 - val_loss: 0.5017 - val_acc: 0.8048\n",
      "Epoch 91/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.4931 - acc: 0.8132 - val_loss: 0.5011 - val_acc: 0.8048\n",
      "Epoch 92/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.4924 - acc: 0.8132 - val_loss: 0.5005 - val_acc: 0.8048\n",
      "Epoch 93/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.4918 - acc: 0.8132 - val_loss: 0.4999 - val_acc: 0.8048\n",
      "Epoch 94/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.4912 - acc: 0.8132 - val_loss: 0.4993 - val_acc: 0.8048\n",
      "Epoch 95/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.4906 - acc: 0.8132 - val_loss: 0.4987 - val_acc: 0.8048\n",
      "Epoch 96/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.4900 - acc: 0.8132 - val_loss: 0.4981 - val_acc: 0.8048\n",
      "Epoch 97/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.4894 - acc: 0.8132 - val_loss: 0.4975 - val_acc: 0.8048\n",
      "Epoch 98/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.4888 - acc: 0.8132 - val_loss: 0.4970 - val_acc: 0.8048\n",
      "Epoch 99/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.4882 - acc: 0.8132 - val_loss: 0.4964 - val_acc: 0.8048\n",
      "Epoch 100/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.4875 - acc: 0.8132 - val_loss: 0.4958 - val_acc: 0.8048\n",
      "17440/17456 [============================>.] - ETA: 0sConfusion matrices:\n",
      "---------------------\n",
      "[[33052     0]\n",
      " [ 7678     0]]\n",
      "[[14165     0]\n",
      " [ 3291     0]]\n",
      "---------------------\n",
      "Evaluation metrics on train data for ann_lrExp model:\n",
      "------------------------------------\n",
      "Train Specificity:  1.0\n",
      "Train Recall:  0.0\n",
      "Train Precision:  nan\n",
      "Train Accuracy:  0.811490301989\n",
      "------------------------------------\n",
      "Evaluation metrics on test data for ann_lrExp model:\n",
      "------------------------------------\n",
      "Test Specificity:  1.0\n",
      "Test Recall:  0.0\n",
      "Test Precision:  nan\n",
      "Test Accuracy:  0.81146883593\n",
      "------------------------------------\n",
      "--------------------------------------------------------\n",
      "Modelling for learning rate -  0.0001\n",
      "--------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jaywardhansawale/anaconda2/envs/python3/lib/python3.6/site-packages/ipykernel_launcher.py:49: RuntimeWarning: invalid value encountered in long_scalars\n",
      "/home/jaywardhansawale/anaconda2/envs/python3/lib/python3.6/site-packages/ipykernel_launcher.py:68: RuntimeWarning: invalid value encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32584 samples, validate on 8146 samples\n",
      "Epoch 1/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.6869 - acc: 0.5193 - val_loss: 0.6652 - val_acc: 0.8051\n",
      "Epoch 2/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.6439 - acc: 0.8132 - val_loss: 0.6285 - val_acc: 0.8048\n",
      "Epoch 3/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.6102 - acc: 0.8132 - val_loss: 0.5995 - val_acc: 0.8048\n",
      "Epoch 4/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.5833 - acc: 0.8132 - val_loss: 0.5762 - val_acc: 0.8048\n",
      "Epoch 5/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.5615 - acc: 0.8132 - val_loss: 0.5571 - val_acc: 0.8048\n",
      "Epoch 6/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.5434 - acc: 0.8132 - val_loss: 0.5412 - val_acc: 0.8048\n",
      "Epoch 7/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.5283 - acc: 0.8132 - val_loss: 0.5280 - val_acc: 0.8048\n",
      "Epoch 8/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.5158 - acc: 0.8132 - val_loss: 0.5170 - val_acc: 0.8048\n",
      "Epoch 9/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.5052 - acc: 0.8132 - val_loss: 0.5076 - val_acc: 0.8048\n",
      "Epoch 10/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.4960 - acc: 0.8132 - val_loss: 0.4995 - val_acc: 0.8048\n",
      "Epoch 11/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.4881 - acc: 0.8132 - val_loss: 0.4924 - val_acc: 0.8048\n",
      "Epoch 12/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.4809 - acc: 0.8132 - val_loss: 0.4859 - val_acc: 0.8048\n",
      "Epoch 13/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.4743 - acc: 0.8132 - val_loss: 0.4795 - val_acc: 0.8048\n",
      "Epoch 14/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.4679 - acc: 0.8132 - val_loss: 0.4738 - val_acc: 0.8048\n",
      "Epoch 15/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.4625 - acc: 0.8132 - val_loss: 0.4687 - val_acc: 0.8048\n",
      "Epoch 16/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.4575 - acc: 0.8132 - val_loss: 0.4639 - val_acc: 0.8048\n",
      "Epoch 17/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.4524 - acc: 0.8132 - val_loss: 0.4586 - val_acc: 0.8048\n",
      "Epoch 18/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.4475 - acc: 0.8132 - val_loss: 0.4542 - val_acc: 0.8048\n",
      "Epoch 19/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.4433 - acc: 0.8132 - val_loss: 0.4502 - val_acc: 0.8048\n",
      "Epoch 20/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.4392 - acc: 0.8132 - val_loss: 0.4461 - val_acc: 0.8048\n",
      "Epoch 21/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.4349 - acc: 0.8132 - val_loss: 0.4420 - val_acc: 0.8048\n",
      "Epoch 22/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.4311 - acc: 0.8132 - val_loss: 0.4387 - val_acc: 0.8048\n",
      "Epoch 23/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.4279 - acc: 0.8132 - val_loss: 0.4357 - val_acc: 0.8048\n",
      "Epoch 24/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.4249 - acc: 0.8132 - val_loss: 0.4328 - val_acc: 0.8048\n",
      "Epoch 25/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.4220 - acc: 0.8132 - val_loss: 0.4301 - val_acc: 0.8048\n",
      "Epoch 26/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.4194 - acc: 0.8132 - val_loss: 0.4276 - val_acc: 0.8048\n",
      "Epoch 27/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.4170 - acc: 0.8132 - val_loss: 0.4253 - val_acc: 0.8048\n",
      "Epoch 28/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.4144 - acc: 0.8132 - val_loss: 0.4227 - val_acc: 0.8048\n",
      "Epoch 29/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.4121 - acc: 0.8132 - val_loss: 0.4207 - val_acc: 0.8048\n",
      "Epoch 30/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.4100 - acc: 0.8132 - val_loss: 0.4188 - val_acc: 0.8048\n",
      "Epoch 31/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.4082 - acc: 0.8132 - val_loss: 0.4170 - val_acc: 0.8048\n",
      "Epoch 32/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.4064 - acc: 0.8132 - val_loss: 0.4153 - val_acc: 0.8048\n",
      "Epoch 33/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.4046 - acc: 0.8132 - val_loss: 0.4136 - val_acc: 0.8048\n",
      "Epoch 34/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.4030 - acc: 0.8132 - val_loss: 0.4120 - val_acc: 0.8048\n",
      "Epoch 35/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.4014 - acc: 0.8132 - val_loss: 0.4104 - val_acc: 0.8048\n",
      "Epoch 36/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3999 - acc: 0.8132 - val_loss: 0.4089 - val_acc: 0.8048\n",
      "Epoch 37/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3984 - acc: 0.8132 - val_loss: 0.4074 - val_acc: 0.8048\n",
      "Epoch 38/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3969 - acc: 0.8132 - val_loss: 0.4058 - val_acc: 0.8048\n",
      "Epoch 39/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3954 - acc: 0.8132 - val_loss: 0.4044 - val_acc: 0.8048\n",
      "Epoch 40/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3940 - acc: 0.8132 - val_loss: 0.4029 - val_acc: 0.8048\n",
      "Epoch 41/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3925 - acc: 0.8132 - val_loss: 0.4016 - val_acc: 0.8048\n",
      "Epoch 42/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3911 - acc: 0.8132 - val_loss: 0.4002 - val_acc: 0.8048\n",
      "Epoch 43/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3898 - acc: 0.8132 - val_loss: 0.3988 - val_acc: 0.8048\n",
      "Epoch 44/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3884 - acc: 0.8132 - val_loss: 0.3975 - val_acc: 0.8048\n",
      "Epoch 45/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3870 - acc: 0.8132 - val_loss: 0.3960 - val_acc: 0.8048\n",
      "Epoch 46/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3857 - acc: 0.8132 - val_loss: 0.3947 - val_acc: 0.8048\n",
      "Epoch 47/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3844 - acc: 0.8132 - val_loss: 0.3933 - val_acc: 0.8048\n",
      "Epoch 48/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3830 - acc: 0.8132 - val_loss: 0.3919 - val_acc: 0.8048\n",
      "Epoch 49/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3817 - acc: 0.8132 - val_loss: 0.3905 - val_acc: 0.8048\n",
      "Epoch 50/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3803 - acc: 0.8132 - val_loss: 0.3890 - val_acc: 0.8048\n",
      "Epoch 51/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3788 - acc: 0.8132 - val_loss: 0.3875 - val_acc: 0.8048\n",
      "Epoch 52/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3773 - acc: 0.8132 - val_loss: 0.3858 - val_acc: 0.8048\n",
      "Epoch 53/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3757 - acc: 0.8132 - val_loss: 0.3840 - val_acc: 0.8048\n",
      "Epoch 54/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3741 - acc: 0.8132 - val_loss: 0.3824 - val_acc: 0.8048\n",
      "Epoch 55/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3726 - acc: 0.8132 - val_loss: 0.3810 - val_acc: 0.8048\n",
      "Epoch 56/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3713 - acc: 0.8132 - val_loss: 0.3797 - val_acc: 0.8048\n",
      "Epoch 57/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3699 - acc: 0.8132 - val_loss: 0.3783 - val_acc: 0.8048\n",
      "Epoch 58/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3686 - acc: 0.8132 - val_loss: 0.3770 - val_acc: 0.8048\n",
      "Epoch 59/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3673 - acc: 0.8132 - val_loss: 0.3758 - val_acc: 0.8048\n",
      "Epoch 60/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3662 - acc: 0.8132 - val_loss: 0.3746 - val_acc: 0.8048\n",
      "Epoch 61/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3650 - acc: 0.8132 - val_loss: 0.3734 - val_acc: 0.8048\n",
      "Epoch 62/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3639 - acc: 0.8132 - val_loss: 0.3724 - val_acc: 0.8048\n",
      "Epoch 63/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3634 - acc: 0.8132 - val_loss: 0.3723 - val_acc: 0.8048\n",
      "Epoch 64/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3625 - acc: 0.8132 - val_loss: 0.3706 - val_acc: 0.8048\n",
      "Epoch 65/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3609 - acc: 0.8160 - val_loss: 0.3695 - val_acc: 0.8106\n",
      "Epoch 66/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3599 - acc: 0.8188 - val_loss: 0.3685 - val_acc: 0.8123\n",
      "Epoch 67/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3590 - acc: 0.8199 - val_loss: 0.3675 - val_acc: 0.8133\n",
      "Epoch 68/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3581 - acc: 0.8212 - val_loss: 0.3665 - val_acc: 0.8143\n",
      "Epoch 69/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3571 - acc: 0.8216 - val_loss: 0.3655 - val_acc: 0.8143\n",
      "Epoch 70/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3562 - acc: 0.8229 - val_loss: 0.3646 - val_acc: 0.8159\n",
      "Epoch 71/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3552 - acc: 0.8247 - val_loss: 0.3636 - val_acc: 0.8178\n",
      "Epoch 72/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3543 - acc: 0.8267 - val_loss: 0.3626 - val_acc: 0.8195\n",
      "Epoch 73/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3536 - acc: 0.8279 - val_loss: 0.3620 - val_acc: 0.8203\n",
      "Epoch 74/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3527 - acc: 0.8287 - val_loss: 0.3610 - val_acc: 0.8221\n",
      "Epoch 75/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3518 - acc: 0.8306 - val_loss: 0.3601 - val_acc: 0.8243\n",
      "Epoch 76/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3510 - acc: 0.8315 - val_loss: 0.3592 - val_acc: 0.8256\n",
      "Epoch 77/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3502 - acc: 0.8317 - val_loss: 0.3584 - val_acc: 0.8258\n",
      "Epoch 78/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3493 - acc: 0.8319 - val_loss: 0.3576 - val_acc: 0.8268\n",
      "Epoch 79/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3485 - acc: 0.8322 - val_loss: 0.3567 - val_acc: 0.8269\n",
      "Epoch 80/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3477 - acc: 0.8329 - val_loss: 0.3559 - val_acc: 0.8273\n",
      "Epoch 81/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3469 - acc: 0.8332 - val_loss: 0.3551 - val_acc: 0.8280\n",
      "Epoch 82/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3462 - acc: 0.8337 - val_loss: 0.3543 - val_acc: 0.8285\n",
      "Epoch 83/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3454 - acc: 0.8343 - val_loss: 0.3537 - val_acc: 0.8285\n",
      "Epoch 84/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3446 - acc: 0.8345 - val_loss: 0.3527 - val_acc: 0.8301\n",
      "Epoch 85/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3439 - acc: 0.8349 - val_loss: 0.3519 - val_acc: 0.8307\n",
      "Epoch 86/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3431 - acc: 0.8352 - val_loss: 0.3512 - val_acc: 0.8303\n",
      "Epoch 87/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3424 - acc: 0.8357 - val_loss: 0.3504 - val_acc: 0.8307\n",
      "Epoch 88/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3417 - acc: 0.8362 - val_loss: 0.3496 - val_acc: 0.8318\n",
      "Epoch 89/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3409 - acc: 0.8365 - val_loss: 0.3488 - val_acc: 0.8323\n",
      "Epoch 90/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3402 - acc: 0.8371 - val_loss: 0.3480 - val_acc: 0.8321\n",
      "Epoch 91/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3400 - acc: 0.8374 - val_loss: 0.3479 - val_acc: 0.8316\n",
      "Epoch 92/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3389 - acc: 0.8380 - val_loss: 0.3466 - val_acc: 0.8328\n",
      "Epoch 93/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3380 - acc: 0.8390 - val_loss: 0.3458 - val_acc: 0.8330\n",
      "Epoch 94/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3373 - acc: 0.8394 - val_loss: 0.3451 - val_acc: 0.8333\n",
      "Epoch 95/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3366 - acc: 0.8397 - val_loss: 0.3443 - val_acc: 0.8339\n",
      "Epoch 96/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3359 - acc: 0.8399 - val_loss: 0.3436 - val_acc: 0.8349\n",
      "Epoch 97/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3353 - acc: 0.8406 - val_loss: 0.3429 - val_acc: 0.8350\n",
      "Epoch 98/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3346 - acc: 0.8408 - val_loss: 0.3423 - val_acc: 0.8357\n",
      "Epoch 99/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3339 - acc: 0.8411 - val_loss: 0.3416 - val_acc: 0.8359\n",
      "Epoch 100/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3333 - acc: 0.8414 - val_loss: 0.3410 - val_acc: 0.8362\n",
      "17280/17456 [============================>.] - ETA: 0sConfusion matrices:\n",
      "---------------------\n",
      "[[32401   651]\n",
      " [ 5847  1831]]\n",
      "[[13898   267]\n",
      " [ 2534   757]]\n",
      "---------------------\n",
      "Evaluation metrics on train data for ann_lrExp model:\n",
      "------------------------------------\n",
      "Train Specificity:  0.980303763766\n",
      "Train Recall:  0.238473560823\n",
      "Train Precision:  0.737711522965\n",
      "Train Accuracy:  0.840461576234\n",
      "------------------------------------\n",
      "Evaluation metrics on test data for ann_lrExp model:\n",
      "------------------------------------\n",
      "Test Specificity:  0.981150723615\n",
      "Test Recall:  0.230021270131\n",
      "Test Precision:  0.7392578125\n",
      "Test Accuracy:  0.839539413382\n",
      "------------------------------------\n",
      "--------------------------------------------------------\n",
      "Modelling for learning rate -  0.001\n",
      "--------------------------------------------------------\n",
      "Train on 32584 samples, validate on 8146 samples\n",
      "Epoch 1/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.5801 - acc: 0.7506 - val_loss: 0.4965 - val_acc: 0.8048\n",
      "Epoch 2/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.4586 - acc: 0.8132 - val_loss: 0.4456 - val_acc: 0.8048\n",
      "Epoch 3/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.4240 - acc: 0.8132 - val_loss: 0.4210 - val_acc: 0.8048\n",
      "Epoch 4/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.4030 - acc: 0.8132 - val_loss: 0.4042 - val_acc: 0.8048\n",
      "Epoch 5/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3876 - acc: 0.8132 - val_loss: 0.3891 - val_acc: 0.8048\n",
      "Epoch 6/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3740 - acc: 0.8132 - val_loss: 0.3768 - val_acc: 0.8049\n",
      "Epoch 7/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3642 - acc: 0.8204 - val_loss: 0.3677 - val_acc: 0.8211\n",
      "Epoch 8/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3556 - acc: 0.8309 - val_loss: 0.3595 - val_acc: 0.8252\n",
      "Epoch 9/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3476 - acc: 0.8346 - val_loss: 0.3514 - val_acc: 0.8313\n",
      "Epoch 10/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3438 - acc: 0.8340 - val_loss: 0.3469 - val_acc: 0.8330\n",
      "Epoch 11/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3348 - acc: 0.8407 - val_loss: 0.3390 - val_acc: 0.8391\n",
      "Epoch 12/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3285 - acc: 0.8453 - val_loss: 0.3330 - val_acc: 0.8418\n",
      "Epoch 13/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3234 - acc: 0.8486 - val_loss: 0.3278 - val_acc: 0.8456\n",
      "Epoch 14/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3184 - acc: 0.8524 - val_loss: 0.3225 - val_acc: 0.8491\n",
      "Epoch 15/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3137 - acc: 0.8559 - val_loss: 0.3185 - val_acc: 0.8524\n",
      "Epoch 16/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3111 - acc: 0.8565 - val_loss: 0.3140 - val_acc: 0.8562\n",
      "Epoch 17/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3060 - acc: 0.8602 - val_loss: 0.3103 - val_acc: 0.8592\n",
      "Epoch 18/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3020 - acc: 0.8635 - val_loss: 0.3067 - val_acc: 0.8605\n",
      "Epoch 19/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2988 - acc: 0.8640 - val_loss: 0.3031 - val_acc: 0.8621\n",
      "Epoch 20/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32584/32584 [==============================] - 0s - loss: 0.2962 - acc: 0.8665 - val_loss: 0.3004 - val_acc: 0.8631\n",
      "Epoch 21/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2933 - acc: 0.8683 - val_loss: 0.2978 - val_acc: 0.8659\n",
      "Epoch 22/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2907 - acc: 0.8705 - val_loss: 0.2949 - val_acc: 0.8668\n",
      "Epoch 23/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2882 - acc: 0.8723 - val_loss: 0.2923 - val_acc: 0.8673\n",
      "Epoch 24/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2862 - acc: 0.8742 - val_loss: 0.2910 - val_acc: 0.8694\n",
      "Epoch 25/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2841 - acc: 0.8755 - val_loss: 0.2883 - val_acc: 0.8705\n",
      "Epoch 26/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2831 - acc: 0.8761 - val_loss: 0.2880 - val_acc: 0.8700\n",
      "Epoch 27/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2802 - acc: 0.8776 - val_loss: 0.2845 - val_acc: 0.8722\n",
      "Epoch 28/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2807 - acc: 0.8767 - val_loss: 0.2921 - val_acc: 0.8657\n",
      "Epoch 29/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2861 - acc: 0.8717 - val_loss: 0.2923 - val_acc: 0.8662\n",
      "Epoch 30/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2840 - acc: 0.8744 - val_loss: 0.2875 - val_acc: 0.8720\n",
      "Epoch 31/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2812 - acc: 0.8773 - val_loss: 0.2886 - val_acc: 0.8701\n",
      "Epoch 32/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2803 - acc: 0.8775 - val_loss: 0.2854 - val_acc: 0.8727\n",
      "Epoch 33/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2772 - acc: 0.8801 - val_loss: 0.2808 - val_acc: 0.8739\n",
      "Epoch 34/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2758 - acc: 0.8814 - val_loss: 0.2797 - val_acc: 0.8753\n",
      "Epoch 35/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2747 - acc: 0.8814 - val_loss: 0.2794 - val_acc: 0.8752\n",
      "Epoch 36/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2738 - acc: 0.8821 - val_loss: 0.2807 - val_acc: 0.8756\n",
      "Epoch 37/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2734 - acc: 0.8823 - val_loss: 0.2766 - val_acc: 0.8774\n",
      "Epoch 38/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2707 - acc: 0.8843 - val_loss: 0.2752 - val_acc: 0.8774\n",
      "Epoch 39/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2694 - acc: 0.8855 - val_loss: 0.2739 - val_acc: 0.8799\n",
      "Epoch 40/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2684 - acc: 0.8869 - val_loss: 0.2720 - val_acc: 0.8809\n",
      "Epoch 41/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2676 - acc: 0.8874 - val_loss: 0.2718 - val_acc: 0.8820\n",
      "Epoch 42/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2664 - acc: 0.8878 - val_loss: 0.2698 - val_acc: 0.8833\n",
      "Epoch 43/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2656 - acc: 0.8884 - val_loss: 0.2686 - val_acc: 0.8836\n",
      "Epoch 44/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2646 - acc: 0.8889 - val_loss: 0.2684 - val_acc: 0.8839\n",
      "Epoch 45/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2634 - acc: 0.8901 - val_loss: 0.2675 - val_acc: 0.8846\n",
      "Epoch 46/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2633 - acc: 0.8903 - val_loss: 0.2679 - val_acc: 0.8852\n",
      "Epoch 47/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2623 - acc: 0.8912 - val_loss: 0.2670 - val_acc: 0.8853\n",
      "Epoch 48/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2615 - acc: 0.8913 - val_loss: 0.2651 - val_acc: 0.8866\n",
      "Epoch 49/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2608 - acc: 0.8910 - val_loss: 0.2642 - val_acc: 0.8873\n",
      "Epoch 50/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2599 - acc: 0.8922 - val_loss: 0.2682 - val_acc: 0.8851\n",
      "Epoch 51/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2630 - acc: 0.8907 - val_loss: 0.2670 - val_acc: 0.8877\n",
      "Epoch 52/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2613 - acc: 0.8918 - val_loss: 0.2665 - val_acc: 0.8839\n",
      "Epoch 53/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2611 - acc: 0.8921 - val_loss: 0.2661 - val_acc: 0.8868\n",
      "Epoch 54/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2602 - acc: 0.8917 - val_loss: 0.2638 - val_acc: 0.8884\n",
      "Epoch 55/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2602 - acc: 0.8929 - val_loss: 0.2652 - val_acc: 0.8866\n",
      "Epoch 56/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2608 - acc: 0.8928 - val_loss: 0.2640 - val_acc: 0.8889\n",
      "Epoch 57/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2591 - acc: 0.8927 - val_loss: 0.2637 - val_acc: 0.8882\n",
      "Epoch 58/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2587 - acc: 0.8935 - val_loss: 0.2623 - val_acc: 0.8885\n",
      "Epoch 59/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2585 - acc: 0.8937 - val_loss: 0.2634 - val_acc: 0.8880\n",
      "Epoch 60/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2574 - acc: 0.8939 - val_loss: 0.2596 - val_acc: 0.8910\n",
      "Epoch 61/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2585 - acc: 0.8933 - val_loss: 0.2612 - val_acc: 0.8883\n",
      "Epoch 62/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2560 - acc: 0.8954 - val_loss: 0.2590 - val_acc: 0.8912\n",
      "Epoch 63/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2552 - acc: 0.8954 - val_loss: 0.2595 - val_acc: 0.8914\n",
      "Epoch 64/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2546 - acc: 0.8946 - val_loss: 0.2574 - val_acc: 0.8923\n",
      "Epoch 65/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2568 - acc: 0.8942 - val_loss: 0.2614 - val_acc: 0.8895\n",
      "Epoch 66/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2559 - acc: 0.8944 - val_loss: 0.2587 - val_acc: 0.8901\n",
      "Epoch 67/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2542 - acc: 0.8950 - val_loss: 0.2587 - val_acc: 0.8909\n",
      "Epoch 68/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2533 - acc: 0.8948 - val_loss: 0.2565 - val_acc: 0.8918\n",
      "Epoch 69/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2529 - acc: 0.8958 - val_loss: 0.2567 - val_acc: 0.8921\n",
      "Epoch 70/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2530 - acc: 0.8960 - val_loss: 0.2554 - val_acc: 0.8920\n",
      "Epoch 71/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2519 - acc: 0.8955 - val_loss: 0.2550 - val_acc: 0.8936\n",
      "Epoch 72/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2518 - acc: 0.8965 - val_loss: 0.2554 - val_acc: 0.8927\n",
      "Epoch 73/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2515 - acc: 0.8956 - val_loss: 0.2557 - val_acc: 0.8907\n",
      "Epoch 74/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2518 - acc: 0.8958 - val_loss: 0.2573 - val_acc: 0.8921\n",
      "Epoch 75/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2516 - acc: 0.8958 - val_loss: 0.2553 - val_acc: 0.8947\n",
      "Epoch 76/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2531 - acc: 0.8961 - val_loss: 0.2592 - val_acc: 0.8912\n",
      "Epoch 77/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2548 - acc: 0.8949 - val_loss: 0.2571 - val_acc: 0.8914\n",
      "Epoch 78/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2529 - acc: 0.8960 - val_loss: 0.2553 - val_acc: 0.8939\n",
      "Epoch 79/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2532 - acc: 0.8962 - val_loss: 0.2567 - val_acc: 0.8932\n",
      "Epoch 80/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2527 - acc: 0.8963 - val_loss: 0.2545 - val_acc: 0.8939\n",
      "Epoch 81/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2512 - acc: 0.8963 - val_loss: 0.2545 - val_acc: 0.8937\n",
      "Epoch 82/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2507 - acc: 0.8972 - val_loss: 0.2530 - val_acc: 0.8943\n",
      "Epoch 83/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2515 - acc: 0.8959 - val_loss: 0.2584 - val_acc: 0.8914\n",
      "Epoch 84/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2521 - acc: 0.8960 - val_loss: 0.2539 - val_acc: 0.8939\n",
      "Epoch 85/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2495 - acc: 0.8970 - val_loss: 0.2525 - val_acc: 0.8938\n",
      "Epoch 86/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2487 - acc: 0.8978 - val_loss: 0.2517 - val_acc: 0.8958\n",
      "Epoch 87/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2483 - acc: 0.8979 - val_loss: 0.2508 - val_acc: 0.8963\n",
      "Epoch 88/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2476 - acc: 0.8977 - val_loss: 0.2511 - val_acc: 0.8948\n",
      "Epoch 89/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2478 - acc: 0.8974 - val_loss: 0.2511 - val_acc: 0.8955\n",
      "Epoch 90/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2478 - acc: 0.8974 - val_loss: 0.2516 - val_acc: 0.8950\n",
      "Epoch 91/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2479 - acc: 0.8972 - val_loss: 0.2524 - val_acc: 0.8942\n",
      "Epoch 92/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2480 - acc: 0.8972 - val_loss: 0.2514 - val_acc: 0.8939\n",
      "Epoch 93/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2471 - acc: 0.8983 - val_loss: 0.2505 - val_acc: 0.8964\n",
      "Epoch 94/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2472 - acc: 0.8980 - val_loss: 0.2501 - val_acc: 0.8955\n",
      "Epoch 95/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2465 - acc: 0.8980 - val_loss: 0.2504 - val_acc: 0.8952\n",
      "Epoch 96/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2470 - acc: 0.8982 - val_loss: 0.2506 - val_acc: 0.8966\n",
      "Epoch 97/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2467 - acc: 0.8987 - val_loss: 0.2503 - val_acc: 0.8955\n",
      "Epoch 98/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2464 - acc: 0.8987 - val_loss: 0.2490 - val_acc: 0.8970\n",
      "Epoch 99/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2457 - acc: 0.8986 - val_loss: 0.2499 - val_acc: 0.8968\n",
      "Epoch 100/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2459 - acc: 0.8991 - val_loss: 0.2489 - val_acc: 0.8968\n",
      "17056/17456 [============================>.] - ETA: 0sConfusion matrices:\n",
      "---------------------\n",
      "[[31523  1529]\n",
      " [ 2589  5089]]\n",
      "[[13486   679]\n",
      " [ 1150  2141]]\n",
      "---------------------\n",
      "Evaluation metrics on train data for ann_lrExp model:\n",
      "------------------------------------\n",
      "Train Specificity:  0.953739561902\n",
      "Train Recall:  0.662802813233\n",
      "Train Precision:  0.768963433061\n",
      "Train Accuracy:  0.89889516327\n",
      "------------------------------------\n",
      "Evaluation metrics on test data for ann_lrExp model:\n",
      "------------------------------------\n",
      "Test Specificity:  0.952064948818\n",
      "Test Recall:  0.650562139167\n",
      "Test Precision:  0.759219858156\n",
      "Test Accuracy:  0.895222273144\n",
      "------------------------------------\n",
      "--------------------------------------------------------\n",
      "Modelling for learning rate -  0.01\n",
      "--------------------------------------------------------\n",
      "Train on 32584 samples, validate on 8146 samples\n",
      "Epoch 1/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3873 - acc: 0.8227 - val_loss: 0.3409 - val_acc: 0.8342\n",
      "Epoch 2/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3177 - acc: 0.8534 - val_loss: 0.3213 - val_acc: 0.8497\n",
      "Epoch 3/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3031 - acc: 0.8610 - val_loss: 0.3197 - val_acc: 0.8241\n",
      "Epoch 4/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2848 - acc: 0.8775 - val_loss: 0.2801 - val_acc: 0.8796\n",
      "Epoch 5/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2701 - acc: 0.8865 - val_loss: 0.2738 - val_acc: 0.8809\n",
      "Epoch 6/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2665 - acc: 0.8888 - val_loss: 0.2678 - val_acc: 0.8887\n",
      "Epoch 7/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2648 - acc: 0.8893 - val_loss: 0.2699 - val_acc: 0.8864\n",
      "Epoch 8/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2683 - acc: 0.8898 - val_loss: 0.2737 - val_acc: 0.8828\n",
      "Epoch 9/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2626 - acc: 0.8922 - val_loss: 0.2702 - val_acc: 0.8879\n",
      "Epoch 10/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2673 - acc: 0.8891 - val_loss: 0.2716 - val_acc: 0.8878\n",
      "Epoch 11/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2626 - acc: 0.8920 - val_loss: 0.2613 - val_acc: 0.8893\n",
      "Epoch 12/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2700 - acc: 0.8882 - val_loss: 0.2741 - val_acc: 0.8836\n",
      "Epoch 13/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2646 - acc: 0.8924 - val_loss: 0.2595 - val_acc: 0.8931\n",
      "Epoch 14/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2568 - acc: 0.8930 - val_loss: 0.2580 - val_acc: 0.8952\n",
      "Epoch 15/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2566 - acc: 0.8941 - val_loss: 0.2606 - val_acc: 0.8923\n",
      "Epoch 16/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2538 - acc: 0.8959 - val_loss: 0.2593 - val_acc: 0.8939\n",
      "Epoch 17/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2565 - acc: 0.8941 - val_loss: 0.2529 - val_acc: 0.8957\n",
      "Epoch 18/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2532 - acc: 0.8937 - val_loss: 0.2589 - val_acc: 0.8941\n",
      "Epoch 19/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2521 - acc: 0.8955 - val_loss: 0.2560 - val_acc: 0.8943\n",
      "Epoch 20/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2539 - acc: 0.8953 - val_loss: 0.2547 - val_acc: 0.8943\n",
      "Epoch 21/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2548 - acc: 0.8945 - val_loss: 0.2580 - val_acc: 0.8926\n",
      "Epoch 22/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2542 - acc: 0.8957 - val_loss: 0.2620 - val_acc: 0.8900\n",
      "Epoch 23/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2541 - acc: 0.8942 - val_loss: 0.2546 - val_acc: 0.8932\n",
      "Epoch 24/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2602 - acc: 0.8910 - val_loss: 0.2607 - val_acc: 0.8931\n",
      "Epoch 25/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2532 - acc: 0.8969 - val_loss: 0.2553 - val_acc: 0.8960\n",
      "Epoch 26/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2624 - acc: 0.8922 - val_loss: 0.2855 - val_acc: 0.8779\n",
      "Epoch 27/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2651 - acc: 0.8906 - val_loss: 0.2657 - val_acc: 0.8890\n",
      "Epoch 28/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2715 - acc: 0.8895 - val_loss: 0.2903 - val_acc: 0.8750\n",
      "Epoch 29/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2678 - acc: 0.8916 - val_loss: 0.2783 - val_acc: 0.8683\n",
      "Epoch 30/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2582 - acc: 0.8941 - val_loss: 0.2623 - val_acc: 0.8923\n",
      "Epoch 31/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2565 - acc: 0.8952 - val_loss: 0.2589 - val_acc: 0.8916\n",
      "Epoch 32/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2583 - acc: 0.8944 - val_loss: 0.2582 - val_acc: 0.8901\n",
      "Epoch 33/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2523 - acc: 0.8940 - val_loss: 0.2515 - val_acc: 0.8934\n",
      "Epoch 34/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2502 - acc: 0.8958 - val_loss: 0.2534 - val_acc: 0.8917\n",
      "Epoch 35/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2638 - acc: 0.8895 - val_loss: 0.2601 - val_acc: 0.8952\n",
      "Epoch 36/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2591 - acc: 0.8938 - val_loss: 0.2596 - val_acc: 0.8910\n",
      "Epoch 37/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2581 - acc: 0.8908 - val_loss: 0.2576 - val_acc: 0.8930\n",
      "Epoch 38/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2667 - acc: 0.8859 - val_loss: 0.2602 - val_acc: 0.8933\n",
      "Epoch 39/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32584/32584 [==============================] - 0s - loss: 0.2567 - acc: 0.8940 - val_loss: 0.2686 - val_acc: 0.8857\n",
      "Epoch 40/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2535 - acc: 0.8948 - val_loss: 0.2734 - val_acc: 0.8818\n",
      "Epoch 41/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2563 - acc: 0.8955 - val_loss: 0.2658 - val_acc: 0.8790\n",
      "Epoch 42/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2604 - acc: 0.8926 - val_loss: 0.2579 - val_acc: 0.8891\n",
      "Epoch 43/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2595 - acc: 0.8914 - val_loss: 0.2599 - val_acc: 0.8920\n",
      "Epoch 44/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2547 - acc: 0.8950 - val_loss: 0.2644 - val_acc: 0.8923\n",
      "Epoch 45/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2524 - acc: 0.8969 - val_loss: 0.2590 - val_acc: 0.8878\n",
      "Epoch 46/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2538 - acc: 0.8944 - val_loss: 0.2608 - val_acc: 0.8888\n",
      "Epoch 47/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2580 - acc: 0.8935 - val_loss: 0.2709 - val_acc: 0.8927\n",
      "Epoch 48/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2765 - acc: 0.8809 - val_loss: 0.2768 - val_acc: 0.8928\n",
      "Epoch 49/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2580 - acc: 0.8955 - val_loss: 0.2630 - val_acc: 0.8829\n",
      "Epoch 50/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2518 - acc: 0.8951 - val_loss: 0.2508 - val_acc: 0.8964\n",
      "Epoch 51/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2471 - acc: 0.8978 - val_loss: 0.2528 - val_acc: 0.8915\n",
      "Epoch 52/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2493 - acc: 0.8966 - val_loss: 0.2490 - val_acc: 0.8985\n",
      "Epoch 53/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2547 - acc: 0.8940 - val_loss: 0.2565 - val_acc: 0.8952\n",
      "Epoch 54/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2543 - acc: 0.8953 - val_loss: 0.2551 - val_acc: 0.8903\n",
      "Epoch 55/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2589 - acc: 0.8959 - val_loss: 0.2712 - val_acc: 0.8916\n",
      "Epoch 56/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2632 - acc: 0.8935 - val_loss: 0.2646 - val_acc: 0.8894\n",
      "Epoch 57/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2664 - acc: 0.8883 - val_loss: 0.2818 - val_acc: 0.8610\n",
      "Epoch 58/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2637 - acc: 0.8881 - val_loss: 0.2631 - val_acc: 0.8926\n",
      "Epoch 59/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2698 - acc: 0.8877 - val_loss: 0.2920 - val_acc: 0.8709\n",
      "Epoch 60/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2598 - acc: 0.8945 - val_loss: 0.2651 - val_acc: 0.8863\n",
      "Epoch 61/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2760 - acc: 0.8777 - val_loss: 0.2777 - val_acc: 0.8885\n",
      "Epoch 62/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2677 - acc: 0.8928 - val_loss: 0.2678 - val_acc: 0.8927\n",
      "Epoch 63/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2685 - acc: 0.8876 - val_loss: 0.2645 - val_acc: 0.8954\n",
      "Epoch 64/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2591 - acc: 0.8934 - val_loss: 0.2680 - val_acc: 0.8890\n",
      "Epoch 65/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2613 - acc: 0.8946 - val_loss: 0.2833 - val_acc: 0.8793\n",
      "Epoch 66/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2619 - acc: 0.8930 - val_loss: 0.2538 - val_acc: 0.8930\n",
      "Epoch 67/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2507 - acc: 0.8966 - val_loss: 0.2691 - val_acc: 0.8907\n",
      "Epoch 68/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2611 - acc: 0.8916 - val_loss: 0.2621 - val_acc: 0.8852\n",
      "Epoch 69/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2635 - acc: 0.8890 - val_loss: 0.2723 - val_acc: 0.8860\n",
      "Epoch 70/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2574 - acc: 0.8934 - val_loss: 0.2559 - val_acc: 0.8952\n",
      "Epoch 71/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2556 - acc: 0.8959 - val_loss: 0.2586 - val_acc: 0.8909\n",
      "Epoch 72/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2667 - acc: 0.8931 - val_loss: 0.2888 - val_acc: 0.8874\n",
      "Epoch 73/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2891 - acc: 0.8716 - val_loss: 0.2908 - val_acc: 0.8863\n",
      "Epoch 74/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2728 - acc: 0.8916 - val_loss: 0.2595 - val_acc: 0.8891\n",
      "Epoch 75/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2538 - acc: 0.8943 - val_loss: 0.2579 - val_acc: 0.8847\n",
      "Epoch 76/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2535 - acc: 0.8938 - val_loss: 0.2560 - val_acc: 0.8882\n",
      "Epoch 77/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2511 - acc: 0.8966 - val_loss: 0.2516 - val_acc: 0.8917\n",
      "Epoch 78/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2574 - acc: 0.8911 - val_loss: 0.2515 - val_acc: 0.8914\n",
      "Epoch 79/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2526 - acc: 0.8917 - val_loss: 0.2524 - val_acc: 0.8891\n",
      "Epoch 80/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2563 - acc: 0.8917 - val_loss: 0.2674 - val_acc: 0.8836\n",
      "Epoch 81/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2638 - acc: 0.8913 - val_loss: 0.3053 - val_acc: 0.8397\n",
      "Epoch 82/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2533 - acc: 0.8949 - val_loss: 0.2957 - val_acc: 0.8247\n",
      "Epoch 83/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2560 - acc: 0.8959 - val_loss: 0.2527 - val_acc: 0.8923\n",
      "Epoch 84/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2500 - acc: 0.8969 - val_loss: 0.2513 - val_acc: 0.8941\n",
      "Epoch 85/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2538 - acc: 0.8963 - val_loss: 0.2548 - val_acc: 0.8920\n",
      "Epoch 86/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2505 - acc: 0.8976 - val_loss: 0.2643 - val_acc: 0.8905\n",
      "Epoch 87/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2547 - acc: 0.8946 - val_loss: 0.2514 - val_acc: 0.8945\n",
      "Epoch 88/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2691 - acc: 0.8787 - val_loss: 0.3130 - val_acc: 0.8075\n",
      "Epoch 89/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2963 - acc: 0.8810 - val_loss: 0.2939 - val_acc: 0.8885\n",
      "Epoch 90/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2829 - acc: 0.8881 - val_loss: 0.3154 - val_acc: 0.8399\n",
      "Epoch 91/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2687 - acc: 0.8932 - val_loss: 0.2674 - val_acc: 0.8918\n",
      "Epoch 92/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2601 - acc: 0.8959 - val_loss: 0.2606 - val_acc: 0.8950\n",
      "Epoch 93/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2591 - acc: 0.8951 - val_loss: 0.2579 - val_acc: 0.8969\n",
      "Epoch 94/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2613 - acc: 0.8917 - val_loss: 0.2589 - val_acc: 0.8953\n",
      "Epoch 95/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2575 - acc: 0.8930 - val_loss: 0.2620 - val_acc: 0.8887\n",
      "Epoch 96/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2535 - acc: 0.8968 - val_loss: 0.2537 - val_acc: 0.8954\n",
      "Epoch 97/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2526 - acc: 0.8961 - val_loss: 0.2674 - val_acc: 0.8885\n",
      "Epoch 98/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2544 - acc: 0.8954 - val_loss: 0.2834 - val_acc: 0.8807\n",
      "Epoch 99/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2636 - acc: 0.8849 - val_loss: 0.2618 - val_acc: 0.8966\n",
      "Epoch 100/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2910 - acc: 0.8730 - val_loss: 0.2890 - val_acc: 0.8868\n",
      "17312/17456 [============================>.] - ETA: 0sConfusion matrices:\n",
      "---------------------\n",
      "[[31059  1993]\n",
      " [ 2451  5227]]\n",
      "[[13284   881]\n",
      " [ 1070  2221]]\n",
      "---------------------\n",
      "Evaluation metrics on train data for ann_lrExp model:\n",
      "------------------------------------\n",
      "Train Specificity:  0.939701077091\n",
      "Train Recall:  0.680776243813\n",
      "Train Precision:  0.723961218837\n",
      "Train Accuracy:  0.890891234962\n",
      "------------------------------------\n",
      "Evaluation metrics on test data for ann_lrExp model:\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Specificity:  0.937804447582\n",
      "Test Recall:  0.674870859921\n",
      "Test Precision:  0.715989684075\n",
      "Test Accuracy:  0.888233272227\n",
      "------------------------------------\n",
      "--------------------------------------------------------\n",
      "Modelling for learning rate -  0.1\n",
      "--------------------------------------------------------\n",
      "Train on 32584 samples, validate on 8146 samples\n",
      "Epoch 1/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3206 - acc: 0.8469 - val_loss: 0.2992 - val_acc: 0.8515\n",
      "Epoch 2/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2917 - acc: 0.8694 - val_loss: 0.2838 - val_acc: 0.8820\n",
      "Epoch 3/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2885 - acc: 0.8702 - val_loss: 0.2943 - val_acc: 0.8512\n",
      "Epoch 4/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2820 - acc: 0.8751 - val_loss: 0.2868 - val_acc: 0.8716\n",
      "Epoch 5/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2879 - acc: 0.8752 - val_loss: 0.3677 - val_acc: 0.8108\n",
      "Epoch 6/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3024 - acc: 0.8661 - val_loss: 0.3044 - val_acc: 0.8686\n",
      "Epoch 7/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3194 - acc: 0.8440 - val_loss: 0.3010 - val_acc: 0.8550\n",
      "Epoch 8/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2978 - acc: 0.8602 - val_loss: 0.3271 - val_acc: 0.8253\n",
      "Epoch 9/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3018 - acc: 0.8583 - val_loss: 0.3411 - val_acc: 0.8265\n",
      "Epoch 10/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3045 - acc: 0.8552 - val_loss: 0.3385 - val_acc: 0.8543\n",
      "Epoch 11/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3319 - acc: 0.8325 - val_loss: 0.3555 - val_acc: 0.8070\n",
      "Epoch 12/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3001 - acc: 0.8588 - val_loss: 0.3057 - val_acc: 0.8626\n",
      "Epoch 13/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2899 - acc: 0.8643 - val_loss: 0.3724 - val_acc: 0.7574\n",
      "Epoch 14/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3177 - acc: 0.8407 - val_loss: 0.3956 - val_acc: 0.8053\n",
      "Epoch 15/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3384 - acc: 0.8271 - val_loss: 0.2945 - val_acc: 0.8712\n",
      "Epoch 16/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2888 - acc: 0.8715 - val_loss: 0.2922 - val_acc: 0.8616\n",
      "Epoch 17/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2929 - acc: 0.8656 - val_loss: 0.2972 - val_acc: 0.8734\n",
      "Epoch 18/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2924 - acc: 0.8728 - val_loss: 0.2982 - val_acc: 0.8664\n",
      "Epoch 19/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2893 - acc: 0.8694 - val_loss: 0.2865 - val_acc: 0.8782\n",
      "Epoch 20/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2833 - acc: 0.8787 - val_loss: 0.3030 - val_acc: 0.8148\n",
      "Epoch 21/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3109 - acc: 0.8600 - val_loss: 0.2834 - val_acc: 0.8866\n",
      "Epoch 22/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2865 - acc: 0.8782 - val_loss: 0.2852 - val_acc: 0.8745\n",
      "Epoch 23/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2774 - acc: 0.8804 - val_loss: 0.2761 - val_acc: 0.8755\n",
      "Epoch 24/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2912 - acc: 0.8641 - val_loss: 0.3614 - val_acc: 0.8364\n",
      "Epoch 25/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2927 - acc: 0.8689 - val_loss: 0.3179 - val_acc: 0.8423\n",
      "Epoch 26/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3445 - acc: 0.8307 - val_loss: 0.3661 - val_acc: 0.8114\n",
      "Epoch 27/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3158 - acc: 0.8419 - val_loss: 0.3167 - val_acc: 0.8220\n",
      "Epoch 28/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3027 - acc: 0.8602 - val_loss: 0.3177 - val_acc: 0.8559\n",
      "Epoch 29/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2915 - acc: 0.8703 - val_loss: 0.2967 - val_acc: 0.8176\n",
      "Epoch 30/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3051 - acc: 0.8646 - val_loss: 0.2921 - val_acc: 0.8711\n",
      "Epoch 31/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3018 - acc: 0.8643 - val_loss: 0.2996 - val_acc: 0.8747\n",
      "Epoch 32/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2903 - acc: 0.8745 - val_loss: 0.2766 - val_acc: 0.8853\n",
      "Epoch 33/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2708 - acc: 0.8872 - val_loss: 0.2734 - val_acc: 0.8812\n",
      "Epoch 34/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2787 - acc: 0.8846 - val_loss: 0.2901 - val_acc: 0.8820\n",
      "Epoch 35/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2818 - acc: 0.8786 - val_loss: 0.3058 - val_acc: 0.8675\n",
      "Epoch 36/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2793 - acc: 0.8744 - val_loss: 0.2693 - val_acc: 0.8896\n",
      "Epoch 37/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2739 - acc: 0.8845 - val_loss: 0.2723 - val_acc: 0.8876\n",
      "Epoch 38/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2925 - acc: 0.8734 - val_loss: 0.3143 - val_acc: 0.8685\n",
      "Epoch 39/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3097 - acc: 0.8657 - val_loss: 0.3277 - val_acc: 0.8337\n",
      "Epoch 40/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3246 - acc: 0.8437 - val_loss: 0.3504 - val_acc: 0.8138\n",
      "Epoch 41/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3367 - acc: 0.8343 - val_loss: 0.3087 - val_acc: 0.8416\n",
      "Epoch 42/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2933 - acc: 0.8756 - val_loss: 0.2876 - val_acc: 0.8850\n",
      "Epoch 43/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3459 - acc: 0.8277 - val_loss: 0.3575 - val_acc: 0.8085\n",
      "Epoch 44/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3326 - acc: 0.8384 - val_loss: 0.3006 - val_acc: 0.8682\n",
      "Epoch 45/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3159 - acc: 0.8535 - val_loss: 0.3489 - val_acc: 0.8105\n",
      "Epoch 46/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3307 - acc: 0.8250 - val_loss: 0.3198 - val_acc: 0.8122\n",
      "Epoch 47/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3083 - acc: 0.8671 - val_loss: 0.3025 - val_acc: 0.8675\n",
      "Epoch 48/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2975 - acc: 0.8717 - val_loss: 0.2960 - val_acc: 0.8699\n",
      "Epoch 49/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3045 - acc: 0.8677 - val_loss: 0.3930 - val_acc: 0.8106\n",
      "Epoch 50/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3555 - acc: 0.8257 - val_loss: 0.3358 - val_acc: 0.8165\n",
      "Epoch 51/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3085 - acc: 0.8624 - val_loss: 0.2927 - val_acc: 0.8829\n",
      "Epoch 52/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3276 - acc: 0.8431 - val_loss: 0.3459 - val_acc: 0.8230\n",
      "Epoch 53/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3069 - acc: 0.8568 - val_loss: 0.2823 - val_acc: 0.8869\n",
      "Epoch 54/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2756 - acc: 0.8836 - val_loss: 0.2683 - val_acc: 0.8896\n",
      "Epoch 55/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2738 - acc: 0.8782 - val_loss: 0.2678 - val_acc: 0.8853\n",
      "Epoch 56/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2678 - acc: 0.8905 - val_loss: 0.2673 - val_acc: 0.8813\n",
      "Epoch 57/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2717 - acc: 0.8881 - val_loss: 0.2923 - val_acc: 0.8862\n",
      "Epoch 58/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2852 - acc: 0.8748 - val_loss: 0.2756 - val_acc: 0.8809\n",
      "Epoch 59/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2661 - acc: 0.8852 - val_loss: 0.3124 - val_acc: 0.8652\n",
      "Epoch 60/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2815 - acc: 0.8824 - val_loss: 0.2752 - val_acc: 0.8829\n",
      "Epoch 61/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2744 - acc: 0.8841 - val_loss: 0.3436 - val_acc: 0.8479\n",
      "Epoch 62/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2773 - acc: 0.8851 - val_loss: 0.2967 - val_acc: 0.8648\n",
      "Epoch 63/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2975 - acc: 0.8694 - val_loss: 0.2852 - val_acc: 0.8855\n",
      "Epoch 64/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3329 - acc: 0.8417 - val_loss: 0.4264 - val_acc: 0.8044\n",
      "Epoch 65/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3525 - acc: 0.8323 - val_loss: 0.2916 - val_acc: 0.8818\n",
      "Epoch 66/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3153 - acc: 0.8582 - val_loss: 0.3050 - val_acc: 0.8829\n",
      "Epoch 67/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3125 - acc: 0.8563 - val_loss: 0.2944 - val_acc: 0.8810\n",
      "Epoch 68/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3275 - acc: 0.8411 - val_loss: 0.3723 - val_acc: 0.8053\n",
      "Epoch 69/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3318 - acc: 0.8253 - val_loss: 0.3060 - val_acc: 0.8752\n",
      "Epoch 70/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3390 - acc: 0.8378 - val_loss: 0.3229 - val_acc: 0.8063\n",
      "Epoch 71/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3403 - acc: 0.8353 - val_loss: 0.3825 - val_acc: 0.8042\n",
      "Epoch 72/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3213 - acc: 0.8523 - val_loss: 0.2913 - val_acc: 0.8722\n",
      "Epoch 73/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3108 - acc: 0.8603 - val_loss: 0.3795 - val_acc: 0.8044\n",
      "Epoch 74/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3681 - acc: 0.8142 - val_loss: 0.3715 - val_acc: 0.8044\n",
      "Epoch 75/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3611 - acc: 0.8145 - val_loss: 0.3789 - val_acc: 0.8046\n",
      "Epoch 76/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3432 - acc: 0.8218 - val_loss: 0.3166 - val_acc: 0.8561\n",
      "Epoch 77/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3265 - acc: 0.8419 - val_loss: 0.3481 - val_acc: 0.8049\n",
      "Epoch 78/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3116 - acc: 0.8490 - val_loss: 0.3596 - val_acc: 0.8380\n",
      "Epoch 79/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3104 - acc: 0.8642 - val_loss: 0.2800 - val_acc: 0.8893\n",
      "Epoch 80/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3213 - acc: 0.8414 - val_loss: 0.3010 - val_acc: 0.8846\n",
      "Epoch 81/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3195 - acc: 0.8538 - val_loss: 0.3782 - val_acc: 0.8130\n",
      "Epoch 82/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3394 - acc: 0.8424 - val_loss: 0.3088 - val_acc: 0.8423\n",
      "Epoch 83/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3015 - acc: 0.8680 - val_loss: 0.2833 - val_acc: 0.8904\n",
      "Epoch 84/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3578 - acc: 0.8204 - val_loss: 0.3677 - val_acc: 0.8051\n",
      "Epoch 85/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3620 - acc: 0.8153 - val_loss: 0.3649 - val_acc: 0.8067\n",
      "Epoch 86/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3531 - acc: 0.8209 - val_loss: 0.3588 - val_acc: 0.8055\n",
      "Epoch 87/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3412 - acc: 0.8266 - val_loss: 0.3592 - val_acc: 0.8067\n",
      "Epoch 88/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3282 - acc: 0.8428 - val_loss: 0.3139 - val_acc: 0.8469\n",
      "Epoch 89/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2851 - acc: 0.8719 - val_loss: 0.3007 - val_acc: 0.8605\n",
      "Epoch 90/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3062 - acc: 0.8679 - val_loss: 0.3107 - val_acc: 0.8577\n",
      "Epoch 91/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2912 - acc: 0.8784 - val_loss: 0.2938 - val_acc: 0.8760\n",
      "Epoch 92/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2879 - acc: 0.8776 - val_loss: 0.2942 - val_acc: 0.8745\n",
      "Epoch 93/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3513 - acc: 0.8326 - val_loss: 0.3737 - val_acc: 0.8044\n",
      "Epoch 94/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3607 - acc: 0.8146 - val_loss: 0.3635 - val_acc: 0.8065\n",
      "Epoch 95/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3391 - acc: 0.8194 - val_loss: 0.3067 - val_acc: 0.8621\n",
      "Epoch 96/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3307 - acc: 0.8520 - val_loss: 0.3042 - val_acc: 0.8644\n",
      "Epoch 97/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3626 - acc: 0.8265 - val_loss: 0.3864 - val_acc: 0.8289\n",
      "Epoch 98/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3022 - acc: 0.8659 - val_loss: 0.2932 - val_acc: 0.8722\n",
      "Epoch 99/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3085 - acc: 0.8612 - val_loss: 0.2963 - val_acc: 0.8828\n",
      "Epoch 100/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3093 - acc: 0.8571 - val_loss: 0.3530 - val_acc: 0.8198\n",
      "17216/17456 [============================>.] - ETA: 0sConfusion matrices:\n",
      "---------------------\n",
      "[[32309   743]\n",
      " [ 6241  1437]]\n",
      "[[13826   339]\n",
      " [ 2692   599]]\n",
      "---------------------\n",
      "Evaluation metrics on train data for ann_lrExp model:\n",
      "------------------------------------\n",
      "Train Specificity:  0.977520271088\n",
      "Train Recall:  0.187158114092\n",
      "Train Precision:  0.659174311927\n",
      "Train Accuracy:  0.828529339553\n",
      "------------------------------------\n",
      "Evaluation metrics on test data for ann_lrExp model:\n",
      "------------------------------------\n",
      "Test Specificity:  0.976067772679\n",
      "Test Recall:  0.182011546642\n",
      "Test Precision:  0.638592750533\n",
      "Test Accuracy:  0.826363428048\n",
      "------------------------------------\n",
      "--------------------------------------------------------\n",
      "Modelling for learning rate -  1\n",
      "--------------------------------------------------------\n",
      "Train on 32584 samples, validate on 8146 samples\n",
      "Epoch 1/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3789 - acc: 0.8086 - val_loss: 0.6057 - val_acc: 0.8049\n",
      "Epoch 2/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3826 - acc: 0.8150 - val_loss: 0.4741 - val_acc: 0.8123\n",
      "Epoch 3/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3714 - acc: 0.8100 - val_loss: 0.6338 - val_acc: 0.8040\n",
      "Epoch 4/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3680 - acc: 0.8178 - val_loss: 0.3093 - val_acc: 0.8499\n",
      "Epoch 5/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3486 - acc: 0.8246 - val_loss: 0.3544 - val_acc: 0.8559\n",
      "Epoch 6/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3544 - acc: 0.8249 - val_loss: 0.4701 - val_acc: 0.8048\n",
      "Epoch 7/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3887 - acc: 0.8102 - val_loss: 0.4726 - val_acc: 0.6716\n",
      "Epoch 8/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3840 - acc: 0.8053 - val_loss: 0.4939 - val_acc: 0.8051\n",
      "Epoch 9/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3443 - acc: 0.8387 - val_loss: 0.8924 - val_acc: 0.8048\n",
      "Epoch 10/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3843 - acc: 0.8149 - val_loss: 2.0692 - val_acc: 0.2571\n",
      "Epoch 11/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.4289 - acc: 0.7967 - val_loss: 0.7551 - val_acc: 0.8052\n",
      "Epoch 12/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3723 - acc: 0.8107 - val_loss: 0.3874 - val_acc: 0.8062\n",
      "Epoch 13/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3975 - acc: 0.8156 - val_loss: 0.3881 - val_acc: 0.8257\n",
      "Epoch 14/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3521 - acc: 0.8301 - val_loss: 0.5774 - val_acc: 0.8057\n",
      "Epoch 15/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3774 - acc: 0.8102 - val_loss: 0.3826 - val_acc: 0.8294\n",
      "Epoch 16/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3733 - acc: 0.8036 - val_loss: 0.5789 - val_acc: 0.6798\n",
      "Epoch 17/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32584/32584 [==============================] - 0s - loss: 0.3824 - acc: 0.7990 - val_loss: 0.3735 - val_acc: 0.8052\n",
      "Epoch 18/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3823 - acc: 0.7985 - val_loss: 0.8631 - val_acc: 0.6406\n",
      "Epoch 19/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3917 - acc: 0.7928 - val_loss: 0.4676 - val_acc: 0.8051\n",
      "Epoch 20/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3856 - acc: 0.7990 - val_loss: 0.3993 - val_acc: 0.8051\n",
      "Epoch 21/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3906 - acc: 0.7901 - val_loss: 0.7058 - val_acc: 0.8049\n",
      "Epoch 22/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3922 - acc: 0.7889 - val_loss: 0.4190 - val_acc: 0.8051\n",
      "Epoch 23/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3943 - acc: 0.7896 - val_loss: 0.5041 - val_acc: 0.8048\n",
      "Epoch 24/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3824 - acc: 0.7935 - val_loss: 0.3267 - val_acc: 0.8553\n",
      "Epoch 25/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3874 - acc: 0.7927 - val_loss: 0.4189 - val_acc: 0.8067\n",
      "Epoch 26/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3536 - acc: 0.8197 - val_loss: 0.6150 - val_acc: 0.8049\n",
      "Epoch 27/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3275 - acc: 0.8438 - val_loss: 0.2971 - val_acc: 0.8706\n",
      "Epoch 28/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3235 - acc: 0.8414 - val_loss: 0.3656 - val_acc: 0.8232\n",
      "Epoch 29/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3287 - acc: 0.8331 - val_loss: 0.4130 - val_acc: 0.8233\n",
      "Epoch 30/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3107 - acc: 0.8535 - val_loss: 0.4373 - val_acc: 0.7582\n",
      "Epoch 31/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3228 - acc: 0.8502 - val_loss: 0.3930 - val_acc: 0.8051\n",
      "Epoch 32/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3875 - acc: 0.7909 - val_loss: 0.3626 - val_acc: 0.8054\n",
      "Epoch 33/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3708 - acc: 0.8009 - val_loss: 1.5958 - val_acc: 0.5539\n",
      "Epoch 34/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3669 - acc: 0.8188 - val_loss: 0.3739 - val_acc: 0.8143\n",
      "Epoch 35/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3999 - acc: 0.7960 - val_loss: 0.3860 - val_acc: 0.8176\n",
      "Epoch 36/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3275 - acc: 0.8436 - val_loss: 0.3192 - val_acc: 0.8048\n",
      "Epoch 37/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3185 - acc: 0.8446 - val_loss: 0.3716 - val_acc: 0.8052\n",
      "Epoch 38/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3372 - acc: 0.8287 - val_loss: 0.3685 - val_acc: 0.8193\n",
      "Epoch 39/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3358 - acc: 0.8400 - val_loss: 0.3158 - val_acc: 0.8760\n",
      "Epoch 40/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3330 - acc: 0.8442 - val_loss: 0.5236 - val_acc: 0.8051\n",
      "Epoch 41/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3377 - acc: 0.8469 - val_loss: 0.3256 - val_acc: 0.8683\n",
      "Epoch 42/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3222 - acc: 0.8574 - val_loss: 0.7039 - val_acc: 0.6617\n",
      "Epoch 43/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3393 - acc: 0.8382 - val_loss: 0.4837 - val_acc: 0.8616\n",
      "Epoch 44/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3356 - acc: 0.8339 - val_loss: 0.3799 - val_acc: 0.8053\n",
      "Epoch 45/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3629 - acc: 0.8197 - val_loss: 0.3135 - val_acc: 0.8706\n",
      "Epoch 46/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3364 - acc: 0.8275 - val_loss: 0.4032 - val_acc: 0.8053\n",
      "Epoch 47/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3293 - acc: 0.8287 - val_loss: 0.3164 - val_acc: 0.8281\n",
      "Epoch 48/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3160 - acc: 0.8526 - val_loss: 0.3630 - val_acc: 0.8771\n",
      "Epoch 49/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3061 - acc: 0.8681 - val_loss: 0.6026 - val_acc: 0.6714\n",
      "Epoch 50/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3122 - acc: 0.8462 - val_loss: 0.3261 - val_acc: 0.8051\n",
      "Epoch 51/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3343 - acc: 0.8473 - val_loss: 0.4620 - val_acc: 0.8137\n",
      "Epoch 52/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3177 - acc: 0.8582 - val_loss: 0.2902 - val_acc: 0.8686\n",
      "Epoch 53/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2914 - acc: 0.8778 - val_loss: 0.2864 - val_acc: 0.8705\n",
      "Epoch 54/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2941 - acc: 0.8648 - val_loss: 0.2984 - val_acc: 0.8545\n",
      "Epoch 55/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3082 - acc: 0.8524 - val_loss: 0.3327 - val_acc: 0.8216\n",
      "Epoch 56/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3301 - acc: 0.8430 - val_loss: 0.2983 - val_acc: 0.8609\n",
      "Epoch 57/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2998 - acc: 0.8619 - val_loss: 0.2997 - val_acc: 0.8640\n",
      "Epoch 58/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3125 - acc: 0.8468 - val_loss: 0.3138 - val_acc: 0.8052\n",
      "Epoch 59/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3082 - acc: 0.8489 - val_loss: 0.5568 - val_acc: 0.8051\n",
      "Epoch 60/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3181 - acc: 0.8528 - val_loss: 0.7246 - val_acc: 0.8051\n",
      "Epoch 61/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3661 - acc: 0.8294 - val_loss: 0.5110 - val_acc: 0.8052\n",
      "Epoch 62/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3304 - acc: 0.8495 - val_loss: 0.5684 - val_acc: 0.8051\n",
      "Epoch 63/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3117 - acc: 0.8534 - val_loss: 0.3054 - val_acc: 0.8723\n",
      "Epoch 64/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3045 - acc: 0.8721 - val_loss: 0.2964 - val_acc: 0.8739\n",
      "Epoch 65/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3009 - acc: 0.8706 - val_loss: 0.4049 - val_acc: 0.8545\n",
      "Epoch 66/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3231 - acc: 0.8626 - val_loss: 0.4047 - val_acc: 0.8053\n",
      "Epoch 67/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2967 - acc: 0.8776 - val_loss: 0.2994 - val_acc: 0.8594\n",
      "Epoch 68/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3339 - acc: 0.8354 - val_loss: 0.8092 - val_acc: 0.6767\n",
      "Epoch 69/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3601 - acc: 0.8254 - val_loss: 0.5880 - val_acc: 0.6960\n",
      "Epoch 70/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3054 - acc: 0.8693 - val_loss: 0.3873 - val_acc: 0.8054\n",
      "Epoch 71/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3015 - acc: 0.8669 - val_loss: 0.3123 - val_acc: 0.8767\n",
      "Epoch 72/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2920 - acc: 0.8774 - val_loss: 0.3257 - val_acc: 0.8801\n",
      "Epoch 73/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2948 - acc: 0.8690 - val_loss: 0.3248 - val_acc: 0.8053\n",
      "Epoch 74/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2916 - acc: 0.8695 - val_loss: 0.3275 - val_acc: 0.8073\n",
      "Epoch 75/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2967 - acc: 0.8717 - val_loss: 0.2936 - val_acc: 0.8841\n",
      "Epoch 76/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2958 - acc: 0.8718 - val_loss: 0.3326 - val_acc: 0.8636\n",
      "Epoch 77/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2942 - acc: 0.8813 - val_loss: 0.3388 - val_acc: 0.8052\n",
      "Epoch 78/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3738 - acc: 0.8081 - val_loss: 0.7196 - val_acc: 0.8051\n",
      "Epoch 79/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3873 - acc: 0.7951 - val_loss: 0.3799 - val_acc: 0.8064\n",
      "Epoch 80/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3708 - acc: 0.7985 - val_loss: 0.6907 - val_acc: 0.8051\n",
      "Epoch 81/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3543 - acc: 0.8150 - val_loss: 0.3731 - val_acc: 0.8378\n",
      "Epoch 82/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3413 - acc: 0.8177 - val_loss: 0.3577 - val_acc: 0.8078\n",
      "Epoch 83/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3414 - acc: 0.8497 - val_loss: 0.7933 - val_acc: 0.6300\n",
      "Epoch 84/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3709 - acc: 0.7991 - val_loss: 0.3545 - val_acc: 0.8082\n",
      "Epoch 85/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3461 - acc: 0.8318 - val_loss: 0.6854 - val_acc: 0.8055\n",
      "Epoch 86/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.4033 - acc: 0.8019 - val_loss: 0.7569 - val_acc: 0.6116\n",
      "Epoch 87/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3939 - acc: 0.8077 - val_loss: 0.4208 - val_acc: 0.8076\n",
      "Epoch 88/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3732 - acc: 0.8172 - val_loss: 0.3026 - val_acc: 0.8782\n",
      "Epoch 89/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3702 - acc: 0.8172 - val_loss: 0.4260 - val_acc: 0.6824\n",
      "Epoch 90/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3738 - acc: 0.8107 - val_loss: 0.3623 - val_acc: 0.8200\n",
      "Epoch 91/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3742 - acc: 0.8013 - val_loss: 0.3961 - val_acc: 0.7189\n",
      "Epoch 92/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3755 - acc: 0.7979 - val_loss: 0.4328 - val_acc: 0.8052\n",
      "Epoch 93/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3481 - acc: 0.8183 - val_loss: 0.3761 - val_acc: 0.8124\n",
      "Epoch 94/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3868 - acc: 0.8110 - val_loss: 0.4386 - val_acc: 0.8516\n",
      "Epoch 95/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3514 - acc: 0.8148 - val_loss: 0.3656 - val_acc: 0.7760\n",
      "Epoch 96/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3383 - acc: 0.8234 - val_loss: 0.6332 - val_acc: 0.8051\n",
      "Epoch 97/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3761 - acc: 0.8173 - val_loss: 0.4943 - val_acc: 0.8051\n",
      "Epoch 98/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3791 - acc: 0.8188 - val_loss: 0.3581 - val_acc: 0.8405\n",
      "Epoch 99/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.3133 - acc: 0.8766 - val_loss: 0.2987 - val_acc: 0.8860\n",
      "Epoch 100/100\n",
      "32584/32584 [==============================] - 0s - loss: 0.2930 - acc: 0.8685 - val_loss: 0.3192 - val_acc: 0.8048\n",
      "16960/17456 [============================>.] - ETA: 0sConfusion matrices:\n",
      "---------------------\n",
      "[[33020    32]\n",
      " [ 7590    88]]\n",
      "[[14143    22]\n",
      " [ 3251    40]]\n",
      "---------------------\n",
      "Evaluation metrics on train data for ann_lrExp model:\n",
      "------------------------------------\n",
      "Train Specificity:  0.999031828634\n",
      "Train Recall:  0.0114613180516\n",
      "Train Precision:  0.733333333333\n",
      "Train Accuracy:  0.812865209919\n",
      "------------------------------------\n",
      "Evaluation metrics on test data for ann_lrExp model:\n",
      "------------------------------------\n",
      "Test Specificity:  0.998446876103\n",
      "Test Recall:  0.0121543603768\n",
      "Test Precision:  0.645161290323\n",
      "Test Accuracy:  0.8125\n",
      "------------------------------------\n",
      "--------------------------------------------------------\n",
      "Modelling for learning rate -  10\n",
      "--------------------------------------------------------\n",
      "Train on 32584 samples, validate on 8146 samples\n",
      "Epoch 1/100\n",
      "32584/32584 [==============================] - 0s - loss: 3.0070 - acc: 0.8119 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 2/100\n",
      "32584/32584 [==============================] - 0s - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 3/100\n",
      "32584/32584 [==============================] - 0s - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 4/100\n",
      "32584/32584 [==============================] - 0s - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 5/100\n",
      "32584/32584 [==============================] - 0s - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 6/100\n",
      "32584/32584 [==============================] - 0s - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 7/100\n",
      "32584/32584 [==============================] - 0s - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 8/100\n",
      "32584/32584 [==============================] - 0s - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 9/100\n",
      "32584/32584 [==============================] - 0s - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 10/100\n",
      "32584/32584 [==============================] - 0s - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 11/100\n",
      "32584/32584 [==============================] - 0s - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 12/100\n",
      "32584/32584 [==============================] - 0s - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 13/100\n",
      "32584/32584 [==============================] - 0s - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 14/100\n",
      "32584/32584 [==============================] - 0s - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 15/100\n",
      "32584/32584 [==============================] - 0s - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 16/100\n",
      "32584/32584 [==============================] - 0s - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 17/100\n",
      "32584/32584 [==============================] - 0s - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 18/100\n",
      "32584/32584 [==============================] - 0s - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 19/100\n",
      "32584/32584 [==============================] - 0s - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 20/100\n",
      "32584/32584 [==============================] - 0s - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 21/100\n",
      "32584/32584 [==============================] - 0s - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 22/100\n",
      "32584/32584 [==============================] - 0s - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 23/100\n",
      "32584/32584 [==============================] - 0s - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 24/100\n",
      "32584/32584 [==============================] - 0s - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 25/100\n",
      "32584/32584 [==============================] - 0s - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 26/100\n",
      "32584/32584 [==============================] - 0s - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 27/100\n",
      "32584/32584 [==============================] - 0s - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 28/100\n",
      "32584/32584 [==============================] - 0s - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 29/100\n",
      "32584/32584 [==============================] - 0s - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 30/100\n",
      "32584/32584 [==============================] - 0s - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 31/100\n",
      "32584/32584 [==============================] - 0s - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 32/100\n",
      "32584/32584 [==============================] - 0s - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 33/100\n",
      "32584/32584 [==============================] - 0s - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 34/100\n",
      "32584/32584 [==============================] - 0s - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 35/100\n",
      "32584/32584 [==============================] - 0s - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 36/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32584/32584 [==============================] - 0s - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 37/100\n",
      "32584/32584 [==============================] - 0s - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 38/100\n",
      "32584/32584 [==============================] - 0s - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 39/100\n",
      "32584/32584 [==============================] - 0s - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 40/100\n",
      "32584/32584 [==============================] - 0s - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 41/100\n",
      "32584/32584 [==============================] - 0s - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 42/100\n",
      "32584/32584 [==============================] - 0s - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 43/100\n",
      "32584/32584 [==============================] - 0s - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 44/100\n",
      "32584/32584 [==============================] - 0s - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 45/100\n",
      "32584/32584 [==============================] - 0s - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 46/100\n",
      "32584/32584 [==============================] - 0s - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 47/100\n",
      "32584/32584 [==============================] - 0s - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 48/100\n",
      "32584/32584 [==============================] - 0s - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 49/100\n",
      "32584/32584 [==============================] - 0s - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 50/100\n",
      "32584/32584 [==============================] - 0s - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 51/100\n",
      "32584/32584 [==============================] - 0s - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 52/100\n",
      "32584/32584 [==============================] - 0s - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 53/100\n",
      "32584/32584 [==============================] - 0s - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 54/100\n",
      "32584/32584 [==============================] - 0s - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 55/100\n",
      "32584/32584 [==============================] - 0s - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 56/100\n",
      "32584/32584 [==============================] - 0s - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 57/100\n",
      "32584/32584 [==============================] - 0s - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 58/100\n",
      "32584/32584 [==============================] - 0s - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 59/100\n",
      "32584/32584 [==============================] - 0s - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 60/100\n",
      "32584/32584 [==============================] - 0s - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 61/100\n",
      "32584/32584 [==============================] - 0s - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 62/100\n",
      "32584/32584 [==============================] - 0s - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 63/100\n",
      "32584/32584 [==============================] - 0s - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 64/100\n",
      "32584/32584 [==============================] - 0s - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 65/100\n",
      "32584/32584 [==============================] - 0s - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 66/100\n",
      "32584/32584 [==============================] - 0s - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 67/100\n",
      "32584/32584 [==============================] - 0s - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 68/100\n",
      "32584/32584 [==============================] - 0s - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 69/100\n",
      "32584/32584 [==============================] - 0s - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 70/100\n",
      "32584/32584 [==============================] - 0s - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 71/100\n",
      "32584/32584 [==============================] - 0s - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 72/100\n",
      "32584/32584 [==============================] - 0s - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 73/100\n",
      "32584/32584 [==============================] - 0s - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 74/100\n",
      "32584/32584 [==============================] - 0s - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 75/100\n",
      "32584/32584 [==============================] - 0s - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 76/100\n",
      "32584/32584 [==============================] - 0s - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 77/100\n",
      "32584/32584 [==============================] - 0s - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 78/100\n",
      "32584/32584 [==============================] - 0s - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 79/100\n",
      "32584/32584 [==============================] - 0s - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 80/100\n",
      "32584/32584 [==============================] - 0s - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 81/100\n",
      "32584/32584 [==============================] - 0s - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 82/100\n",
      "32584/32584 [==============================] - 0s - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 83/100\n",
      "32584/32584 [==============================] - 0s - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 84/100\n",
      "32584/32584 [==============================] - 0s - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 85/100\n",
      "32584/32584 [==============================] - 0s - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 86/100\n",
      "32584/32584 [==============================] - 0s - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 87/100\n",
      "32584/32584 [==============================] - 0s - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 88/100\n",
      "32584/32584 [==============================] - 0s - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 89/100\n",
      "32584/32584 [==============================] - 0s - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 90/100\n",
      "32584/32584 [==============================] - 0s - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 91/100\n",
      "32584/32584 [==============================] - 0s - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 92/100\n",
      "32584/32584 [==============================] - 0s - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 93/100\n",
      "32584/32584 [==============================] - 0s - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 94/100\n",
      "32584/32584 [==============================] - 0s - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 95/100\n",
      "32584/32584 [==============================] - 0s - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 96/100\n",
      "32584/32584 [==============================] - 0s - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 97/100\n",
      "32584/32584 [==============================] - 0s - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 98/100\n",
      "32584/32584 [==============================] - 0s - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 99/100\n",
      "32584/32584 [==============================] - 0s - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "Epoch 100/100\n",
      "32584/32584 [==============================] - 0s - loss: 3.0115 - acc: 0.8132 - val_loss: 3.1461 - val_acc: 0.8048\n",
      "16800/17456 [===========================>..] - ETA: 0sConfusion matrices:\n",
      "---------------------\n",
      "[[33052     0]\n",
      " [ 7678     0]]\n",
      "[[14165     0]\n",
      " [ 3291     0]]\n",
      "---------------------\n",
      "Evaluation metrics on train data for ann_lrExp model:\n",
      "------------------------------------\n",
      "Train Specificity:  1.0\n",
      "Train Recall:  0.0\n",
      "Train Precision:  nan\n",
      "Train Accuracy:  0.811490301989\n",
      "------------------------------------\n",
      "Evaluation metrics on test data for ann_lrExp model:\n",
      "------------------------------------\n",
      "Test Specificity:  1.0\n",
      "Test Recall:  0.0\n",
      "Test Precision:  nan\n",
      "Test Accuracy:  0.81146883593\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Initialiations for plotting later on\n",
    "train_recall = []\n",
    "test_recall = []\n",
    "train_accuracy = []\n",
    "test_accuracy = []\n",
    "\n",
    "# Model for all learning rates\n",
    "for i in range(-5,2,1):\n",
    "    eta = 10**i\n",
    "    print(\"--------------------------------------------------------\")\n",
    "    print(\"Modelling for learning rate - \", eta)\n",
    "    print(\"--------------------------------------------------------\")\n",
    "    \n",
    "    # Custom optimizer based on SGD\n",
    "    SGD = optimizers.SGD(lr = eta)\n",
    "    \n",
    "    # Modelling\n",
    "    ann_model_lrExp = Sequential()\n",
    "\n",
    "    ann_model_lrExp.add(Dense(29, input_dim=21, activation='sigmoid', kernel_initializer='normal'))\n",
    "    ann_model_lrExp.add(Dense(1, activation='sigmoid', kernel_initializer='normal'))\n",
    "\n",
    "    ann_model_lrExp.compile(loss='binary_crossentropy', optimizer=SGD, metrics=['accuracy'])\n",
    "\n",
    "    ann_model_lrExp.fit(X_train, y_train, epochs=100, batch_size=64, validation_split=0.2)\n",
    "\n",
    "    # Predictions\n",
    "    train_pred = ann_model_lrExp.predict_classes(X_train)\n",
    "    test_pred = ann_model_lrExp.predict_classes(X_test)\n",
    "\n",
    "    # Evaluation metrics\n",
    "    confusion_matrix_train = confusion_matrix(y_train, train_pred)\n",
    "    confusion_matrix_test = confusion_matrix(y_test, test_pred)\n",
    "\n",
    "    print(\"Confusion matrices:\")\n",
    "    print(\"---------------------\")\n",
    "    print(confusion_matrix_train)\n",
    "    print(confusion_matrix_test)\n",
    "    print(\"---------------------\")\n",
    "\n",
    "    # Metrics on train data\n",
    "    # Accuracy\n",
    "    accuracy_Train_lrExp = (confusion_matrix_train[0,0]+confusion_matrix_train[1,1])/(confusion_matrix_train[0,0]+confusion_matrix_train[0,1]+confusion_matrix_train[1,0]+confusion_matrix_train[1,1])\n",
    "    # Specificity or true negative rate (TNR)\n",
    "    specificity_Train_lrExp = confusion_matrix_train[0,0]/(confusion_matrix_train[0,0]+confusion_matrix_train[0,1])\n",
    "    # Sensitivity, recall, hit rate, or true positive rate (TPR)\n",
    "    recall_Train_lrExp = confusion_matrix_train[1,1]/(confusion_matrix_train[1,0]+confusion_matrix_train[1,1])\n",
    "    # Precision\n",
    "    precision_Train_lrExp = confusion_matrix_train[1,1]/(confusion_matrix_train[0,1]+confusion_matrix_train[1,1])\n",
    "\n",
    "    print(\"Evaluation metrics on train data for ann_lrExp model:\")\n",
    "    print(\"------------------------------------\")\n",
    "    print(\"Train Specificity: \",specificity_Train_lrExp)\n",
    "    print(\"Train Recall: \",recall_Train_lrExp)\n",
    "    print(\"Train Precision: \",precision_Train_lrExp)\n",
    "    print(\"Train Accuracy: \",accuracy_Train_lrExp)\n",
    "    print(\"------------------------------------\")\n",
    "\n",
    "\n",
    "    # Metrics on test data\n",
    "    #Accuracy\n",
    "    accuracy_Test_lrExp = (confusion_matrix_test[0,0]+confusion_matrix_test[1,1])/(confusion_matrix_test[0,0]+confusion_matrix_test[0,1]+confusion_matrix_test[1,0]+confusion_matrix_test[1,1])\n",
    "    # Specificity or true negative rate (TNR)\n",
    "    specificity_Test_lrExp = confusion_matrix_test[0,0]/(confusion_matrix_test[0,0]+confusion_matrix_test[0,1])\n",
    "    # Sensitivity, recall, hit rate, or true positive rate (TPR)\n",
    "    recall_Test_lrExp = confusion_matrix_test[1,1]/(confusion_matrix_test[1,0]+confusion_matrix_test[1,1])\n",
    "    # Precision\n",
    "    precision_Test_lrExp = confusion_matrix_test[1,1]/(confusion_matrix_test[0,1]+confusion_matrix_test[1,1])\n",
    "\n",
    "    print(\"Evaluation metrics on test data for ann_lrExp model:\")\n",
    "    print(\"------------------------------------\")\n",
    "    print(\"Test Specificity: \",specificity_Test_lrExp)\n",
    "    print(\"Test Recall: \",recall_Test_lrExp)\n",
    "    print(\"Test Precision: \",precision_Test_lrExp)\n",
    "    print(\"Test Accuracy: \",accuracy_Test_lrExp)\n",
    "    print(\"------------------------------------\")\n",
    "    \n",
    "    # Plotting realted variables\n",
    "    train_recall.append(recall_Train_lrExp)\n",
    "    test_recall.append(recall_Test_lrExp)\n",
    "    train_accuracy.append(accuracy_Train_lrExp)\n",
    "    test_accuracy.append(accuracy_Test_lrExp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEaCAYAAADg2nttAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYWZmuVwszMws1/8AsUBeWXoM7J4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting accuracy for various learning rates for 100 epochs\n",
    "plt.plot([0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10], train_accuracy)\n",
    "plt.plot([0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10], test_accuracy)\n",
    "plt.title('Accuracy for various learning rates')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Learning Rates')\n",
    "plt.xscale('log')\n",
    "plt.legend(['Train', 'Test'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEaCAYAAAD+E0veAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3XecVPX1//HXmdne6UhHQBBBF1gx\ndomoYBDsgCV2NIkl+tWoSX7GFls0iS1BosaCikSjolKCdIWVXYp0pMjCAgtL2V22t/P74w444FbY\n2bsze56PxzyYe+feO++7M9wzt30+oqoYY4wxAB63AxhjjGk6rCgYY4w5xIqCMcaYQ6woGGOMOcSK\ngjHGmEOsKBhjjDnEioI5ZiJynohk+g1vEZGh1UwbLSKfi0iuiPyn8VLWjYiMF5H/14jv101EVETC\nGus9/d77bBFZ39jva5q2Rv8imsASkS1AO6ACyAemA3eqar6bufxciZOvlaqWux3mSKp6h9sZGouq\nLgB6u50DnB8WwERV7eR2lubO9hRC0yWqGgckAwOAh13O468r8P3RFIRA/5oWEW8gl9+Y3NjzqI44\nbFsTJOyDCmGqmgXMwCkOAIhIpIg8LyJbRWSX73BJtN/ro0RkuYjkicgmERnmG3+TiKwVkQMisllE\nbq9vHhF5DHgEGC0i+SJyi4h4ROSPIpIhIrtF5B0RSfRNf/DQyi0ishWYXcUy14rICL/hMBHJFpGB\nvuH/iEiW73DVfBE5yW/at0TknyIyVUQKgCG+cU/6TXObiGwUkX0iMkVEOhyRLcxv2rkicqvveU8R\nmed73z0i8mEd/0aJIvKGiOwUke0i8uTBYiUiPURktojs9S3zPRFJ8pt3i4g8KCIrgALf32KLiNwv\nIit8WT4UkSjf9FUd9qtyWt/rv/Pl2iEit/rWv2c16zFXRP4sIt8AhcDx1X2HRCQWmAZ08H0v8kWk\ng++78ZDve7hXRCaLSEvfPFEiMtE3PkdE0kSkXV3+xqZmVhRCmIh0AoYDG/1GPwOcgFMoegIdcTbU\niMhg4B3gASAJOAfY4ptvNzACSABuAv52cMNbV6r6J+Ap4ENVjVPVN4AbfY8hwPFAHPDKEbOeC5wI\nXFTFYj8AxvoNXwTsUdWlvuFpQC+gLbAUeO+I+a8B/gzEA1/7vyAiPweeBq4GjgMygEl1Wll4Avgf\n0ALoBLxcx/neAspxPpsBwIXArQcj+fJ0wPl7dAYePWL+scAvgCS/vbGrgWFAd+BknL93daqc1vfj\n4D5gqC/beXVYl+uBcTh/2wyq+Q6pagHO93SH73sRp6o7gLuAS3E+/w7AfuBV37JvABJ9f4NWwB1A\nUR0ymdqoqj1C6IGzEc8HDgAKzMLZQICzUSkAevhNfzrwg+/5a8Df6vg+nwL3+J6fB2QekWFoNfM9\ninPs+ODwLODXfsO9gTKc813dfOtwfA05evrWNcY3/B7wSDXTJvmWl+gbfgt454hp3gKe9D1/A3jO\n77U4X7ZuftnC/F6fC9zqe/4OMAHoVMvf8dBycM61lADRfq+PBeZUM++lwLIj/u43V/F9uM5v+Dlg\nfA2fW3XTvgk8fcTfXYGe1WSbCzx+tN8h37i1wPl+w8f5fTduBhYCJ7v9fy7UHranEJouVdV4nP9o\nfYDWvvFtgBhgiW+XOwfnRHQb3+udgU1VLVBEhotIqu8wSg5wsd9yj0UHnF+RB2Xw4wbyoG3Vzayq\nG3E2HpeISAwwEnjfl9krIs/4Dj/k8eNej3/uapd9ZDZ1Ttbvxdm7qs3vcIrwYhFZLSI312GerkA4\nsNPv83kNZy8HEWknIpN8h5XygIn89DOoan2y/J4X4hS36lQ3bYcjll3T363KaY7iO9QV+MTvb7EW\n5wKKdsC7OIdGJ/kOZz0nIuF1yGRqYUUhhKnqPJxfvs/7Ru3B2cU+SVWTfI9EdU5Kg/OfuMeRyxGR\nSOBj33LaqWoSMBVno3esduD85z+oC87hk13+q1LLMg4eQhoFrPEVCnAODY3COeSRiPOrHA7PXdOy\nD8vmO/bdCtiOs8cFTpE9qP2hhapmqeptqtoBuB34R3XH3/1sw9lTaO33+SSo6sHzIE/58vZX1QTg\nOn76GQSq2eOdOIfBDupch3kOZanDd6iq3NuA4X5/iyRVjVLV7apapqqPqWpf4Aycw1K/rP9qmSNZ\nUQh9fwcuEJFTVLUS+BfOsdyDvz47isjBY/VvADeJyPm+k3wdRaQPEAFEAtlAuYgMxznW3RA+AO4V\nke4iEseP5xzqc3XSJF+eX+HbS/CJx9nI7sXZeD91FNluEpFk30btKeBbVd2iqtk4xeE63x7JzfgV\nVBG5yndOB5xj4QpU1vRmqroT5zzECyKS4PsMeojIuX7rkw/kikhHnHM/jWUyzt/iRN8eWX3v5ajt\nO7QLaCW+iwx8xgN/FpGuACLSRkRG+Z4PEZH+4pyEz8M5rFTj39fUjRWFEOfbeL2D72Qy8CDOiedU\n3yGIr/Bdq66qi/GdAARygXlAV1U9ANyNs2HYj/MLfEoDRXwT51DAfOAHoBjnBGOd+Tami3B+Mfpf\n5fMOzuGf7cAaILWey/0KZ+P3Mc4v5R7AGL9JbsPZMO8FTsI5xn3QqcC3IpKP87e6R1U31+Ftf4mz\nAV2D87f+COdYOsBjwECcz+ZL4L/1WZ9joarTgJeAOfi+P76XSuo4f43fIVVdh1OEN/sOF3UAXvRN\n8z8ROeB7z9N8s7TH+dvk4RxWmofzPTLHSHwncIwxps5E5ERgFRBZz70608TZnoIxpk5E5DJx7nNp\nATwLfG4FIfRYUTDG1NXtOPcabMK5CuhX7sYxgWCHj4wxxhxiewrGGGMOsaJgjDHmkCbTkmJdtW7d\nWrt16+Z2DGOMCSpLlizZo6ptapsu0E0RD8O51tgLvK6qzxzx+t9wGkID5+aitr47HavVrVs30tPT\nAxHXGGNClohk1D5VAIuC707DV4ELgEwgTUSmqOqag9Oo6r1+09+F0yqkMcYYlwTynMJgYKOqblbV\nUpymCEbVMP1YnDsajTHGuCSQRaEjh7eSmEk1rUv62jbpThWdqPheHyci6SKSnp2d3eBBjTHGOJrK\nieYxwEeqWlHVi6o6AadtelJSUuzGCmNMnZSVlZGZmUlxcbHbURpNVFQUnTp1Ijz86FoSD2RR2M7h\nzet28o2ryhjgNwHMYoxphjIzM4mPj6dbt26INERL702bqrJ3714yMzPp3r37US0jkIeP0oBeviaR\nI3A2/D9pWdPXNHMLnFYujTGmwRQXF9OqVatmURAARIRWrVod055RwPYUVLVcRO7E6R3JC7ypqqtF\n5HEgXVUPFogxwCS19jZMECqvqCQ7v4RdeSUUl1VwWveWzWYDFCya2+dxrOsb0HMKqjoVp3cl/3GP\nHDH8aCAzGHM0VJX9hWVk5Raz60Axu/OKycot+fF5XjG78krYk1+C/8+Zf990KkN6t3UvuGlS9u7d\ny/nnnw9AVlYWXq+XNm2c+8cWL15MRERErcu46aabeOihh+jdu3dAsx7UVE40G9No8kvK2ZVXzC7f\nBn9XXglZucXs9nuefaCE0oqfduTVKjaCtglRtE+IZGC7cHqFF9FNdtG+YgfzV25i6pIWVhTMIa1a\ntWL58uUAPProo8TFxXH//fcfNo2qoqp4PFUfzf/3v/8d8Jz+rCiYkFFSXsHuvBJ2H/D9qs/zbfRz\nnY39wecFpT+9yC0+Moy2CZG0S4jitO4taZsQRbv4CDpHFdORLNqWbSexaDthOT/Avs2w5wfIOPzy\n6F7AI+vaUlAymNhI+69lqrdx40ZGjhzJgAEDWLZsGTNnzuSxxx5j6dKlFBUVMXr0aB55xDmoctZZ\nZ/HKK6/Qr18/WrduzR133MG0adOIiYnhs88+o23bhv0RYt9c0+RVVCp7fcftd/kO3ez2Hb5xDuMU\ns/tACfsKSn8yb0SYh3YJkbSLj+LE9gmcd0JbZzghynnER9Dek0NM/lbY79vg79sMW3+AfT9ASe7h\nC0zoBC27Q+/h0PJ4aNHd9283iv5xDsP3f81Xa3cxKrnKW3KMix77fDVrduQ16DL7dkjgT5ecdFTz\nrlu3jnfeeYeUlBQAnnnmGVq2bEl5eTlDhgzhyiuvpG/fvofNk5uby7nnnsszzzzDfffdx5tvvslD\nDz10zOvhz4qCcVVRaQXb9hc6x+7zDj5KDnuenV9CReXh1yF4BFrHRdI+MYpOLWIY1LUF7X0b+rYJ\nzvh28VEkxYQjlRWQl+ls5Pctczb6O7f4CsAPUF7044LFC0ldnA19p1OdAnBw49+iK4RHV7sukQPG\ncNq8Z7g/bbkVBVOrHj16HCoIAB988AFvvPEG5eXl7NixgzVr1vykKERHRzN8+HAABg0axIIFCxo8\nlxUF45r8knIu/Os8duQefvlcUkw47eKjaJcYxQnt4p1f9IlRtIt3fuG3T4yiVWwEYV6/Y7DlJZCz\nFfatcTb2m/x+9edshcqyH6f1Rjob+xbd4fghvg2/b+Of2Bm8R3fTj+fkq2De07TN+IK9+UNpFRd5\nVMsxgXG0v+gDJTY29tDzDRs28OKLL7J48WKSkpK47rrrqrys1P/EtNfrpby84XtDtaJgXPPJsu3s\nyC3msZEn0bdDAu3inV/5UeHeqmcoLXB+2WduPvxQz74tkLsN8NubiIiHlt2gfT/oO/LHwzwtj4f4\n46Cak3rHpFUPitomMzLrG6au3Mn1p3dr+PcwISkvL4/4+HgSEhLYuXMnM2bMYNiwYa5ksaJgXKGq\nTFyUQb+OCfzy9K4/XltdtB92+w7r7PNt+A8WgPxdhy8kuqWzke9yGrQce/gx/tjW4ML16VEDx9B3\n+kNMSF9kRcHU2cCBA+nbty99+vSha9eunHnmma5lCbo+mlNSUtT6Uwh+aVv2cdX4RXw4eBOnVX73\n48a/aP/hE8Yf5/crv/vhx/ija+x6wx35u6l8vjf/KL+EUfeNp3PLGLcTNWtr167lxBNPdDtGo6tq\nvUVkiaqmVDPLIbanYFzx7qIMUqIyOW3F/3M2/G16w0mXHV4AWnSDiNhal9WkxLWlpMvZjNqykCnL\nt/Obn/dyO5Ex9WJFwTS6PfklTFu1k0ntv4bcaPj1Iohu4XasBhM9cCydt97BhvSv0CE9m10zCya4\nBbJBPGOqNDl9G9EV+QzInQn9rwipggDAiSMo90QxMG8Wa3cecDuNMfViRcE0qopK5b3UrdzXdime\n8iI49Ta3IzW8yHgqThjGCG8qny+tU7e4xjQZVhRMo5r3/W625xRyZeV06JgCHZLdjhQQkcmjaSkH\n2L18KpWVwXUxh2nerCiYRvXuogwujv2euPwf4NRb3Y4TOD2HUhqeyDklc1m8ZZ/baYypMysKptFs\n21fI3O+zuTdxvnOPwUmXuR0pcMIikH6XcYFnCdOWbHQ7jXHJ3r17SU5OJjk5mfbt29OxY8dDw6Wl\nP22rqzpvvvkmWVlZAUz6IysKptG8v3grHWQfPffPh4HXQ3iU25ECKjx5NDFSQunqLygt/2kz3Cb0\nHWw6e/ny5dxxxx3ce++9h4br0pfCQVYUTMgpKa/gw7RtPNw2FdFKSLnZ7UiB1/lnFMccx4UV85i7\nfrfbaUwT8/bbbzN48GCSk5P59a9/TWVlJeXl5Vx//fX079+ffv368dJLL/Hhhx+yfPlyRo8eXe89\njKNh9ymYRjF9VRYHCgq5IHwG9LrQuTEt1Hk8hCdfzdkLX+YPS1Zz4Unt3U7UvE17CLJWNuwy2/eH\n4c/Ue7ZVq1bxySefsHDhQsLCwhg3bhyTJk2iR48e7Nmzh5UrnZw5OTkkJSXx8ssv88orr5CcHPgL\nM2xPwTSKiakZXJe4gsji7NA+wXwE7ymjCaOSmA2fc6C4rPYZTLPw1VdfkZaWRkpKCsnJycybN49N\nmzbRs2dP1q9fz913382MGTNITExs9Gy2p2ACbl1WHmlb9vNS+9kQ3RV6nu92pMbT7iQKW/RmxN6v\n+d/qXVwxqJPbiZqvo/hFHyiqys0338wTTzzxk9dWrFjBtGnTePXVV/n444+ZMGFCo2YL6J6CiAwT\nkfUislFEquweSESuFpE1IrJaRN4PZB7jjompGfQPy+S4nKVw6i3gqaZp7BAVPXAMgzwb+MYacjQ+\nQ4cOZfLkyezZswdwrlLaunUr2dnZqCpXXXUVjz/+OEuXLgUgPj6eAwca5+74gO0piIgXeBW4AMgE\n0kRkiqqu8ZumF/AwcKaq7hcR6/E8xOSXlPPJ0u1MaLMQ8iIh+Tq3IzU66X8lzHqMjtu+YPeBYbSN\nD+2rrkzt+vfvz5/+9CeGDh1KZWUl4eHhjB8/Hq/Xyy233IKqIiI8++yzANx0003ceuutREdHs3jx\n4npduVRfAWs6W0ROBx5V1Yt8ww8DqOrTftM8B3yvqq/XdbnWdHZweTc1g2c/Xcx3cffgPWkkXDbe\n7UiuKHrtQrZv38aCC7/kprOOdztOs2FNZ/+ork1nB/LwUUdgm99wpm+cvxOAE0TkGxFJFZEquxoS\nkXEiki4i6dnZ2QGKaxqaqvJeaga/aZWOt7wgNNs5qqPoQWPo6dnBd+lfux3FmBq5ffVRGNALOA8Y\nC/xLRH7Sc4qqTlDVFFVNadOmTSNHNEcrPWM/67LyuEZmwnHJ0HGg25Hc0/dSKiSMvnums2VPgdtp\njKlWIIvCdqCz33An3zh/mcAUVS1T1R+A73GKhAkBE1MzGBK1gcT8Tc5lqM25X4GYlpR1/zkjvQuZ\nsmxb7dMb45JAFoU0oJeIdBeRCGAMMOWIaT7F2UtARFrjHE7aHMBMppHsyS9h6sqd3J80H6KSoN8V\nbkdyXdTAMbSX/WxZ8j+CrRvcYNbc/tbHur4BKwqqWg7cCcwA1gKTVXW1iDwuIiN9k80A9orIGmAO\n8ICq7g1UJtN4Jqdvo0XFPvrmzYcB10GE9VXMCcMp88YwOH8Wq7bnuZ2mWYiKimLv3r3NpjCoKnv3\n7iUq6uivcAvozWuqOhWYesS4R/yeK3Cf72FCREWl8v63W3mgTSpyoLx5tHNUFxExaJ8RXLzqC15Z\nupn+nQa4nSjkderUiczMTJrTBSpRUVF06nT0N0naHc2mwc37fjdZ+w9wSdIM6HE+tOrhdqQmI2LA\naCJWTybnu6lUjEjG62nG51kaQXh4ON27d3c7RlBx++ojE4Impm7l8tgVRBXvblbtHNVJ9/MoiWzF\neaVzSd1sR0pN02NFwTSobfsKmbN+N3fGzYXEznDCRW5Halq8YXhPvoLzPcuYnr7e7TTG/IQVBdOg\n3l+8lZ6ynS656ZByU7Nr56guwk4ZTaSUwdopFJdVuB3HmMNYUTANpqS8gslp23io9ULwRsCAX7od\nqWnqOIiiuC5cVLmAOeus8x3TtFhRMA1m+qosigryOLdoJvS9FOLs7vMqiRA5YAxneNcwN32F22mM\nOYwVBdNgJqZmcEtCGmFl+XaCuRaeU67Gg5K0+XNyi6zzHdN0WFEwDcLpSGcfN0Z8Be36Q+fBbkdq\n2lr3orB1f0bIAqav2ul2GmMOsaJgGsTE1AxOD9tAq/wNMLiZt3NUR9EDx9Dfs4W0tFS3oxhziBUF\nc8wOdqTzQMuvITIB+l/ldqSgIP2vRBG67JhKVm6x23GMAawomAbwybLtRJfuIzl/HiRfAxGxbkcK\nDvHtKep0JqM83/DFd0c2IGyMO6womGNysCOdu1ssxFNZZieY6ylm0Fi6enazNn2221GMAawomGOU\nnrGfDVk5XKEzofu50Nq6w6iXEy+h3BNBv30z2bg73+00xlhRMMdmYmoGv4haQWxxlu0lHI2oRMp7\nXMQl3kV8vizD7TTGWFEwR29PfgnTVmZxT/xciO8AvS92O1JQiho4mtaSx/al05tNu/+m6bKiYI7a\n5PRtdKzcTo8DaU47R15rif2o9LqQ0vAEziiczfJtOW6nMc2cFQVzVA52pHN/y6/BEwYDrZ2joxYW\nCX1HcpEnjalLrTda4y4rCuaozPt+N3v253Bh6Sw4cSTEt3c7UlCLSB5DrJRQsGIK5RWVbscxzZgV\nBXNUJqZu5drYxYSX5dkJ5obQ9UyKo9vx87J5fLPJOt8x7rGiYOrN6UhnF+OiZkPbvtD1DLcjBT+P\nh7BTruJc7wq+SlvtdhrTjAW0KIjIMBFZLyIbReShKl6/UUSyRWS572E/OYPA+4u3MkA20q5gPZx6\ni7Vz1EDCThlNOBWErf+colLrfMe4I2BFQUS8wKvAcKAvMFZE+lYx6Yeqmux7vB6oPKZhHOxI54GW\nX0NEPJw82u1IoaN9fwoTezGcBXy1dpfbaUwzFcg9hcHARlXdrKqlwCRgVADfzzSC6auyqCzYw2mF\nc+GUMRAZ73ak0CFC1MDRDPasZ0HaUrfTmGYqkEWhI7DNbzjTN+5IV4jIChH5SEQ6V7UgERknIuki\nkp6dnR2IrKaOJqZmcHv8wXaObnE7TsjxnOy0MNt2yxT2F5S6nMY0R26faP4c6KaqJwMzgbermkhV\nJ6hqiqqmtGljXTy6ZV1WHku27OVa71fQ9Sxoe6LbkUJPi24UtBvECM9CplrnO8YFgSwK2wH/X/6d\nfOMOUdW9qlriG3wdGBTAPOYYTUzNYGj4d8QX73A60jEBETNoLH0821iW9rXbUUwzFMiikAb0EpHu\nIhIBjAGm+E8gIsf5DY4E1gYwjzkGBzvSuTdxPsS1gz4j3I4UsuSky6jES8+saWzPKXI7jmlmAlYU\nVLUcuBOYgbOxn6yqq0XkcREZ6ZvsbhFZLSLfAXcDNwYqjzk2ny7bTquyHfTJXwyDbgRvuNuRQlds\na0q6nscl3oVMWZbpdhrTzAS0BTNVnQpMPWLcI37PHwYeDmQGc+xUlYmpGdyTuABKPE5RMAEVPWgs\nHTNmsWnJTBhifVSYxuP2iWYTBJZk7OeHrL2MqJiF9PkFJHRwO1Lo63MxZZ4oBubMZH3WAbfTmGbE\nioKp1bupGVwZtZjIslxr56ixRMRS0ftiLvZ+y+dLt7idxjQjVhRMjfb6OtL5dcwcaH0CdD/H7UjN\nRtSAsSRJAdnLvqSy0jrfMY3DioKp0eT0TPpUbqBj4VpnL8HaOWo8PYZQEtGCs4vnsGTrfrfTmGbC\nioKpVkWl8t63GdybtADCY51mLUzj8YYj/S5nqGcJ05d873Ya00xYUTDVmv99Nvn7d3NOyTw4+WqI\nSnQ7UrMTkTyaKCmjbNUUyqzzHdMIrCiYar2bmsGNMQvxVpbYCWa3dB5MUWwnhpbPZ8EGa/fLBJ4V\nBVOlbfsKmbs+ixsiZkGX06F9P7cjNU8iRCSP5kzvKmalrXQ7jWkGrCiYKn2weCvnyEpaFGfaXoLL\nvMmj8aLEbphCQUm523FMiLOiYH6ipLyCD9O28duk+RDbBk68xO1IzVub3hS07MvFLGDmGut8xwSW\nFQXzE9NXZRFduJ1TClNh4A0QFul2pGYveuBYkj2bWZT2rdtRTIizomB+YmJqBnfEzXfuSbB2jpoE\nz8lXogjHbf2Cvfkltc9gzFGyomAOsy4rj++27OYKZiEnDIekKjvDM40toQOFHU5npOcbvlyxw+00\nJoRZUTCHeS91K5eEpxFdlmMd6TQxsYPGcLwni1Vpc92OYkKYFQVzSH5JOf9dmsmdcXOgZQ/ofp7b\nkYy/vqOokHB6Z89g695Ct9OYEGVFwRzy6bLtdCvbRPei1XDqLeCxr0eTEp1E6fFDucS7iM+Xb3U7\njQlR9r/eAD92pHNXwlw0LBqSr3E7kqlC9KCxtJUcMpZMR9VaTjUNz4qCAZyOdHZkZTG0bD7S/0qI\nbuF2JFOVXhdRGhbH4AOzWLMzz+00JgRZUTCAcxnqNZHfEFZZbHcwN2XhUWifSxjmSePLpZvdTmNC\nkBUF4+tIZwe3Rs2GTqdCh2S3I5kaRA4cQ5wUkbP8C+t8xzS4gBYFERkmIutFZKOIPFTDdFeIiIpI\nSiDzmKpNTs8kRVfSumSr7SUEg25nUxzVhvNK5vDtD/vcTmNCTMCKgoh4gVeB4UBfYKyI9K1iunjg\nHsDu33dBRaXy/uIM7kmYBzGtoO+lbkcytfF4CTv5SoZ4ljNzyVq305gQE8g9hcHARlXdrKqlwCRg\nVBXTPQE8CxQHMIupxvzvsynfl8mpxakw4HoIj3I7kqmDsOTRhEsFuuYzSsor3I5jQkggi0JHYJvf\ncKZv3CEiMhDorKpf1rQgERknIukikp6dbR2NNKSJqRncGjMPUEi5ye04pq6OS6Yw/nguqlzA3PX2\nf8I0HNdONIuIB/gr8H+1TauqE1Q1RVVT2rRpE/hwzcS2fYUsWL+DMd7ZSK8LoUU3tyOZuhIhcuAY\nfuZZy/y05W6nMSEkkEVhO+Dfmlon37iD4oF+wFwR2QL8DJhiJ5sbzweLtzLMs5jYsn0w+Da345h6\n8p58JQBJmz7jQHGZy2lMqAhkUUgDeolIdxGJAMYAUw6+qKq5qtpaVbupajcgFRipqukBzGR8Ssor\nmJy+jTvj5zl7CD3OdzuSqa9WPShok8wv5BtmrLbOd0zDCFhRUNVy4E5gBrAWmKyqq0XkcREZGaj3\nNXUzfVUWrQo2ckLxSkixdo6CVcygsfT1ZJCe9o3bUUyICKvpRRG5r6bXVfWvtbw+FZh6xLhHqpn2\nvJqWZRrWe6lb+XXsXFQjkQHXuR3HHCXpdzmV039Pl8wv2H1gFG3j7eoxc2xq+3kYX8vDBKF1WXms\n2ZLJxToP6XcFxLR0O5I5WnFtKep8FiO9C/liuXW+Y45djXsKqvpYYwUxjee91K1cFf4N4RVF1pFO\nCIhNuYbYbbezPv0rOHuc23FMkKvt8NFLNb2uqnc3bBwTaPkl5XyyLJOvomdD6wHQcZDbkcyx6vML\nyj1R9Ns7gx/2XEv31rFuJzJBrMaiACxplBSm0Xy6bDv9ylbSXjLg1N+5Hcc0hMh4ynpexC/Wz2Li\n0gzuvvAnrckYU2e1HT56u7GCmMA72JHOH2PnoN4k53yCCQnRg8YS/f1n7Fw6Fb3gRETE7UgmSNXp\nOkQRaSMiz4vIVBGZffAR6HCmYS3J2M++rK2cUZ7qXHEUHu12JNNQepxPSXgiPyuYxcrtuW6nMUGs\nrhenv4dzr0F34DFgC87NaSb38GPyAAAfwklEQVSITEzN4IbIuXi0AlJudjuOaUhhEXDSZVzgWcKX\n6RvdTmOCWF2LQitVfQMoU9V5qnoz8PMA5jINbG9+Cf9bmckvI+ZAz6HQqofbkUwDixwwmhgpoWjl\nZ1RY5zvmKNW1KBxsWGWniPxCRAYAdnF7EJmcnsm5mkZ82R7rSCdUdf4ZRTEdGFI6j0Wb9rqdxgSp\nuhaFJ0UkEadF0/uB14F7A5bKNKiDHen8Jm4uJHaBXhe6HckEgsdDePLVnO1ZyVfpq9xOY4JUnYqC\nqn7ha8BulaoOUdVBqjql9jlNUzD/+2wi92+gX+l3Tp8JHq/bkUyAhJ0ymjCpJHzdZxSXWec7pv7q\nevXR2yKS5DfcQkTeDFws05AmpmZwW/Qc1Bvh9K5mQle7vhQk9Wa4zmf2ut1upzFBqK6Hj05W1ZyD\nA6q6HxgQmEimIW3bV0jq+q1cKvORvpdCnHVSFOqiB41loGcj36TZBYKm/upaFDwi0uLggIi0pPa7\noU0T8MHirVzm+ZrIigLrSKeZ8PR3Ot9p/cPn5BZa5zumfupaFF4AFonIEyLyBLAQeC5wsUxDKCmv\nYHLaVu6InQPt+0OnU92OZBpDUmfy25/GJfI101Zay6mmfup6ovkd4HJgl+9xuaq+G8hg5thNX5VF\nt8KVdCr9wbkM1Zo+aDZiB42hp2cH36XNdzuKCTL16W6rJVCgqq8A2SLSPUCZTAN5L3Urv4qZg0Ym\nQP+r3I5jGpGcdCkVEsbxWdPIyi12O44JInW9+uhPwIPAw75R4cDEQIUyx2591gF+2LKZ8yoXIcnX\nQoQ1p9ysxLSkuNvPucS7kC+Wb3M7jQkidd1TuAwYCRQAqOoOrOe1Jm1iagZjw+fh1XI49Ra34xgX\nxA4aS3vZz6b0GW5HMUGkrkWhVFUVUAARsZ+dTVh+STlTlm3lpsjZcPx50LqX25GMG04YRpk3huT9\nM9m4+4DbaUyQqGtRmCwirwFJInIb8BVOUxc1EpFhIrJeRDaKyENVvH6HiKwUkeUi8rWIWO8gDeDT\nZds5rWwxLcqzrZ2j5iwihvLeIxjuXcwXS39wO40JEnW9+uh54CPgY6A38Iiq1thVp4h4gVeB4UBf\nYGwVG/33VbW/qibjXOL613rmN0c42JHOr2LmoAkd4YThbkcyLooeOIYEKWTP0i9wdvaNqVmdrz5S\n1Zmq+oCq3g/MEpFra5llMLBRVTeraikwCRh1xDLz/AZj8R2eMkdv6db9lO5az4Dy5cigm8Br9xg2\na93PpTiyFWcWzWbZtpzapzfNXo1FQUQSRORhEXlFRC4Ux53AZuDqWpbdEfC/7CHTN+7I9/iNiGzC\n2VO4u5oc40QkXUTSs7Oza3nb5u3dRRncFDEb9YTDwF+6Hce4zRuGp98VnO9Zxoy0dW6nMUGgtj2F\nd3EOF60EbgXmAFcBl6rqqJpmrCtVfVVVe+Bc8vrHaqaZoKopqprSpo213VOdvfklzFm5havC5iN9\nR0J8O7cjmSYgYsAYIqSc0lWfUVZR6XYc08TVdmzheFXtDyAirwM7gS6qWpe7YbYDnf2GO/nGVWcS\n8M86LNdU4z9LMhnGN0RV5NsJZvOjjgMpiOvK0Nz5fLNxD+f1but2ItOE1bancKg1LVWtADLrWBDA\n6cO5l4h0F5EIYAxwWB8MIuJ/reQvgA11XLY5QmWl8l7qFu6Ing1t+0KX092OZJoKESIHjOZ07xrm\npn3ndhrTxNVWFE4RkTzf4wBw8sHnIpJX04yqWg7cCcwA1gKTVXW1iDwuIiN9k90pIqtFZDlwH3DD\nMa5PszVvQzZtclbQvXyTtXNkfiLslNF4UKK//4yiUut8x1SvxsNHqnpMXXSp6lRg6hHjHvF7fs+x\nLN/8aOKiDG6Nmo2GxyMn13YNgGl2Wvckv1V/Ls5ewMy1uxh5Sge3E5kmqj4N4pkmKnN/IcvXb+RC\nFiGnjIFIa4HE/FTMoLH092zh228XuR3FNGFWFELAB4u3Mto7lzAtsxPMplqe/ldQiYfjtn3O/oJS\nt+OYJsqKQpArLa/kP4u3cHPUHOh2NrTt43Yk01TFt6ew4xlcIt/w5QrrfMdUzYpCkJu+Oot+RWm0\nLt9lraGaWsWmjKWrZzdr02a7HcU0UVYUgtzERRncHj0bjWsPfUa4Hcc0cXLiSMo9kfTaPY3M/YVu\nxzFNkBWFILY+6wBZGWs5rWIpMuhG8Ia7Hck0dVEJlB5/ISO8qXyxLMPtNKYJsqIQxN77NoNfhs9C\nxQuD7BYPUzcxg8bQWvLYtmS621FME2RFIUgVlJTz5dIfGBs+HzlxBCTYdeemjnpdQElYPIPyvmJd\nVo33oJpmyIpCkPp0+XaGlC8gtiLPLkM19RMWifa9lIs8aUxdssntNKaJsaIQhFSVdxdlcFvUbLR1\nb+dSVGPqIWrgGGKlhNzlU6istG5MzI+sKAShpVv3E7FrOb0rNiDWzpE5Gl3OoDCqPWcXzyE9Y7/b\naUwTYkUhyKgqE+Zv5uaIWWh4LJwy2u1IJhh5PISdchXnelYwM32122lME2JFIch8vHQ7367eyAjv\nIuSU0RCV6HYkE6QiBowmXCpgzaeUllvnO8ZhRSGIbMrO55HPVnF/2zTCKksgxe5gNsegXT/yE3tx\nYcV8Fmywbm6Nw4pCkCgpr+Cu95fR3ZvNNSX/cU4ut+/ndiwTzESIHjiaUz3fMz9tidtpTBNhRSFI\nPD11HRt37uWDpPHOhzbyZbcjmRDg9fW9kbjxM/JLyl1OY5oCKwpBYOaaXby1cAsTO08hYf8quPRV\naNnd7VgmFLToyoG2KfyCr5m5eqfbaUwTYEWhiduZW8QDH33HHa2WMzj7Yzj9TjjxErdjmRASmzKW\n3p5Mliz+2u0opgmwotCEVVQq90xaTofyTH5X+ip0GgxDH3U7lgkxnpMuo0K8dN7+JXvyS9yOY1xm\nRaEJe3n2Blb8sJMPkv6JJywSrvq3tYRqGl5sK4q6nMcIz0K+/G6722mMywJaFERkmIisF5GNIvJQ\nFa/fJyJrRGSFiMwSka6BzBNMUjfv5aVZG3i7/X9IzNsAl/8LEju5HcuEqLiUa+goe9mQ9j+3oxiX\nBawoiIgXeBUYDvQFxopI3yMmWwakqOrJwEfAc4HKE0z2F5Ty20nLGZeQymk5U+Gc+6HXULdjmVDW\nezhl3mj67pnO1r3W+U5zFsg9hcHARlXdrKqlwCRglP8EqjpHVQ9+A1OBZv9TWFV54KPvaF2wkd+V\nT3DuRzjvYbdjmVAXEUtZr4u52LuYz5f+4HYa46JAFoWOwDa/4UzfuOrcAkyr6gURGSci6SKSnp0d\n2ndevrVwC4vWZjAx8R94ohPhijfA43U7lmkGYgaNJUkKyFryBarWcmpz1SRONIvIdUAK8JeqXlfV\nCaqaoqopbdq0adxwjWjV9lyenrqWN1tNJLFoq1MQ4tu5Hcs0F8efR3FES04rmMXqHdb5TnMVyKKw\nHejsN9zJN+4wIjIU+AMwUlWb7fVwBSXl3PXBMm6NmsNpBXOQIX+A7tZPgmlE3nDkpEsZ6lnKtCXf\nu53GuCSQRSEN6CUi3UUkAhgDTPGfQEQGAK/hFITdAczS5D3y2WoS9q3kfn0Lel4AZ93ndiTTDEUO\nHEuUlJG/7FMy99sJ5+YoYEVBVcuBO4EZwFpgsqquFpHHRWSkb7K/AHHAf0RkuYhMqWZxIe2TZZnM\nXLqOd+L/gSeuLVw+ATxN4sieaW46nUppfBeG6zzGTljE9pwitxOZRhYWyIWr6lRg6hHjHvF73uyv\ns/xhTwF//GQlbyX+m4Sy3XDddIhp6XYs01yJEDH4Rn4263GuL3qXsa8Jk24/nQ5J0W4nM40koEXB\n1KykvIK7PljKzd6pnFqyCC56Gjqf6nYs09ydeS/sz2Dc0reJKCpjzGvw4R2nc1yiFYbmwIqCi56b\nvp7IHWncG/We08jdz37ldiRjnEOXI/4OYZHcuHgC4YXljHkNJt1uhaE5sKLgkllrd/HJ198xN/4f\neOI6w6hXQcTtWMY4PB4Y/hx4I7h20StEFJRzzWvKB7efSfvEKLfTmQCyouCCrNxiHpi8jH/FvUZ8\nZR5c/bH1tWyaHhG48EkIi+SqBS8QXlDGtRPgvXFWGEKZFYVGVlGp/PbDZdxY8TGDdJmzm37cKW7H\nMqZqIvDz/wfeSC6d+xRh+RVcOwHev/1M2iVYYQhFdt1jI3t1zkY8W+Zzl+c/0P9qGHSj25GMqZkI\nnPcgnP8nRsjXPJj/LNe/toDdecVuJzMBYHsKjWjxD/t4/6tvmRnzT2hxAoz4m51HMMHj7PsgLJIL\nZ/ye8PynuX4CvDvubNraHkNIsT2FRpJTWMr/fZDO+Oh/EOcpQa5+ByLj3I5lTP2c/hu4+HmGyBL+\neOBJbpgwj90HbI8hlFhRaASqyu8+WsE1RRNJrlyNjPgbtO3jdixjjs7g2+CSlzhLvuORA49z84R5\nZB9ots2WhRwrCo3g3dQMytZN51fez2DgDXDKGLcjGXNsBt2AXDaen3nW8Ke8P3HLhNlWGEKEFYUA\nW7Mjjze/nM/LUePR9v1h+LNuRzKmYZwyBrnidVI83/NY3iPcOmEWe/KtMAQ7KwoBVFhazm/f/5aX\nw18iJkyRq96GcLsj1ISQflcgV7/NKd4feDLvD9z+2kz2WmEIalYUAujRKasZk/M6/XUDnkv/Aa16\nuB3JmIZ34iV4Rk/kJG8mT+T9gV+99j8rDEHMikKAfLZ8OweWfszNYdPhtF9B31G1z2RMsOo9DM81\nk+gdtpMn8x7izgnTrTAEKSsKAZCxt4Dxn3zFC5ET0I4pcMHjbkcyJvB6no/32v/QI2wvf859iLsn\nTGVfQanbqUw9WVFoYKXllfzf+6n8Tf5KVGQUctVbEBbhdixjGsfx5+L95Sd0Cc/jqdwH+e2Ez9lv\nhSGoWFFoYH+ZsY7Ld71MH7bgufw1SOpc+0zGhJKupxN242d0jCjkzzkPct9rn1lhCCJWFBrQnPW7\n2fPNO1wTNgfOuhdOuMjtSMa4o1MKYTd9TvvIMp7K/R2/m/BfcgqtMAQDKwoNZHdeMf/48AuejniT\nyi5nwJA/uh3JGHd1GED4zV/QKgr+nPMgD732kRWGIGBFoQFUVCoPfrCIpyueJyw6Hs9V/wavtTVo\nDO37E3HLNBKjw/lzzoP88bVJ5BaWuZ3K1CCgRUFEhonIehHZKCIPVfH6OSKyVETKReTKQGYJpPFz\nNzIy8y/0kB2EXfUmxLd3O5IxTUfbPkTeOp3YmGieyHmYP732HrlFVhiaqoAVBRHxAq8Cw4G+wFgR\n6XvEZFuBG4H3A5Uj0JZk7GPH7PFc5v0GznsYjj/X7UjGND2texJ12wyiYhN5LOf3PD7+HSsMTVQg\n9xQGAxtVdbOqlgKTgMPu4FLVLaq6AqgMYI6AyS0s4+X3/ssjYW9T3v3nyDkPuB3JmKarZXeix00n\nPK4lj+X8gafG/9sKQxMUyKLQEdjmN5zpG1dvIjJORNJFJD07O7tBwh0rVeVPkxfyWMlzSEwrwq78\nl9PZuTGmekldiLn9f0hCex7J+SPPjX+dvGIrDE1JUGzFVHWCqqaoakqbNm3cjgPAe6kZXLTpCTp7\n9hAx5m2Ibe12JGOCQ0IHYsfNQBM788ecR3jhn+OtMDQhgSwK2wH/O7c6+cYFvXVZeWRMfYHh3jQY\n+ih0+ZnbkYwJLvHtiLt9BmVJx/P7nMd48Z+vcsAKQ5MQyKKQBvQSke4iEgGMAaYE8P0aRWFpOa++\n8wEPet+jpOcwPGfc5XYkY4JTbGsSbp9GccsTeDDncV7959+tMDQBASsKqloO3AnMANYCk1V1tYg8\nLiIjAUTkVBHJBK4CXhOR1YHK01Ce/2QRDxU8S3lcByKvGA8ibkcyJnjFtCRx3FQKW/Xn/pynmPDP\nF8gvKXc7VbMmqup2hnpJSUnR9PR0V9778+WZxH18DWeHrSHstpnQYYArOYwJOSUH2PevS0nMXsIr\nifdzy28eIi7SbgBtSCKyRFVTapsuKE40NwXb9hWy6ZMnGeL9Dhn2tBUEYxpSZDwtx01hf9vTuCv3\ned559QnbY3CJFYU6KKuoZPzbb3OXTKLghEvxDr7V7UjGhJ6IWFqP+5Q97c7k13l/54NXH6XACkOj\ns6JQB//8YiH35DxDUVw3Yq94xc4jGBMo4dG0ve1jstoP4ba8l5n86h+sMDQyKwq1mL8ui5T0B0jy\nFBF3/XsQGe92JGNCW3gU7W+dzI4OF3JT3ng+feV3FJZaYWgsVhRqsPtAMd9/+HvO8K5BfvECtDvJ\n7UjGNA9hEXS45QMyO13MtQfe4IuXf2uFoZFYUahGZaXy1tuvc6t+TG6f0YSnXO92JGOaF28YnW6e\nyNbOo7j6wLvMeOlOiuxQUsBZUajGxP8t5Nbsp8mJ70Xi5X93O44xzZPHS5eb3mJL1yu5LP8DZr18\nuxWGALOiUIWlW3bTb+E9xHgqSLzhA4iIcTuSMc2Xx0O3G/7Fpu5jGZH/EfNevoViO5QUMFYUjpBb\nVMb3E/+PgZ4NVI58CWndy+1IxhiPhx6//Ccbjr+BYfmf8s2Lv6S41JrECAQrCn5UlQ/f+Qdjyqew\n+8RfEjPgarcjGWMOEqHX9S+yrudtnF/wJYv/fg3FJdbnc0OzouDn83kLGbPjGXbH96XtFc+7HccY\ncyQR+lz7F1ad8BvOKfwfS1+8muKSErdThRQrCj4btmdz/Jzf4PV4aH3j+xAW6XYkY0xVROh3zVOs\n6PNbziicw4q/X0FxcbHbqUKGFQWguKyCdW/fRT/5gfKR/8DTqrvbkYwxtTh5zGMsP/F3DC5awJoX\nL6W4qNDtSCHBigLw2bt/55LSaWw78TYSB4yqfQZjTJOQPPoPLDnp9wwsWsT3L46kpCjf7UhBr9kX\nhbnffMOIjGfZFn8Kna982u04xph6GnTVg3zb/1H6FaWz8cVLKCk64HakoNasi0Lmrj10nHk75Z5I\n2t/8PnjD3Y5kjDkKp11xL9+e8iR9ipax5e8XU1KQ43akoNVsi0JZeQUb/j2OHppJyagJhLfo5HYk\nY8wxOP3yO1mU/Aw9ilex7aXhlObvdztSUGq2ReGr919gSPEsNp74a9omD3c7jjGmAZx12R18PeB5\nuhSvZ/tLF1F6YJ/bkYJOsywKyxYvYMim59gQl8IJVz/hdhxjTAM679Jb+Hrg3+hQsomsly+gNGsd\nFOdBkHU97JZm1wnqnj17aDn1Ngo88XS65T3weN2OZIxpYD8fdQNfecI4K/0eIsafBkAFHgo88RR5\n4ykJT6Q0IpGKyCQ0KgmJboE3tiXhcS2Jim9NdEIrYhJbExbXCqKSwNt8NpUBXVMRGQa8CHiB11X1\nmSNejwTeAQYBe4HRqrolUHkqKyrZ9OZNDNJd7Bj1H1q1aB+otzLGuGzoJdeyoFVX9qz7BinOwVuc\nQ3hZLpFluUQXHSC2IIsE3UiS5JMoNd/jUEAMhd44irwJlIQnUuYrKEQ7BSUsthXh8S2JjG9NTGJr\nYpNaERnXGsKjg66nxoAVBRHxAq8CFwCZQJqITFHVNX6T3QLsV9WeIjIGeBYYHahMiz58mjML57Os\nz28ZMGBooN7GGNNEnH3GWXDGWdW+XlxWQW5RGbsKiinI3UtR3h5KDuylLH8fFQX7oGi/U1BKcogo\nzSOyPJeYwjxi83eSQD5JFBAuFdUuv4Rw8iWeAm88xd54Sn0FpTIqCaJbIDEtCIttSURcK6ISWhOb\n2JrYFm2IjktCXDqKEcg9hcHARlXdDCAik4BRgH9RGAU86nv+EfCKiIhqwx/8+37pXE5d/wIrYn9G\n8uhHGnrxxpggFBXuJSrcS7uEKDguCehR53lLyyvJLSolLy+Xwtxsin0FpTx/HxWF+6FoP57iHMJK\nnD2UqPIDxJRup1X+OhI0n1ipvs2mChUOSCz5Ek+hN57isERKwxOIGnwD/c4O7A22gSwKHYFtfsOZ\nwGnVTaOq5SKSC7QC9vhPJCLjgHEAXbp0OaowOT8sI9vThm63vOtaBTbGhI6IMA+t46NoHR8FHdvV\na96KSiWnoIADOXsoytlDUd4eSvOdglJZuA+Kc5yCUppDZFkeUeW5tCzJJCt3V4DW5kdBcfZEVScA\nEwBSUlKOai9i8BX3UnzxOKKiYxs0mzHG1JfXIyTFx5EUHwedu9V5vsa4myqQl6RuBzr7DXfyjaty\nGhEJAxJxTjgHhBUEY4ypWSCLQhrQS0S6i0gEMAaYcsQ0U4AbfM+vBGYH4nyCMcaYugnY4SPfOYI7\ngRk4l6S+qaqrReRxIF1VpwBvAO+KyEZgH07hMMYY45KAnlNQ1anA1CPGPeL3vBi4KpAZjDHG1F2z\nbObCGGNM1awoGGOMOcSKgjHGmEOsKBhjjDlEgu0KUN9dzxv8RiUCuXV83poj7pauJ/9l1vf1ql47\nclxjrUtt61HbNDXlrm344HP/cW6tS30/kyOHj1yXQH+/apomlL9fVY0LhnVp6O8XHNu69FLVxFqn\nUtWgegATqhuu7TnOpbAN9t71eb2q19xal9rWo77rUp9hv/z+41xZl/p+JrWtS6C/Xw25LsH0/QrW\ndWno71djrIuqBuXho89rGK7L84Z87/q8XtVrbq1LXZZRn3Wpz/Dn1UxztI5lXer7mRw5HMzrEkzf\nr6rGBcO6BOP3K/gOHx0LEUlX1RS3czQEW5emJ1TWA2xdmqrGWJdg3FM4FhPcDtCAbF2anlBZD7B1\naaoCvi7Nak/BGGNMzZrbnoIxxpgaWFEwxhhziBUFY4wxh1hR8BGR80RkgYiMF5Hz3M5zrEQkVkTS\nRWSE21mOloic6Ps8PhKRX7md51iIyKUi8i8R+VBELnQ7z7EQkeNF5A0R+cjtLEfD93/jbd/nca3b\neY5WoD6HkCgKIvKmiOwWkVVHjB8mIutFZKOIPFTLYhTIB6Jw+pN2RQOtC8CDwOTApKxdQ6yHqq5V\n1TuAq4EzA5m3Jg20Lp+q6m3AHcDoQOatSQOty2ZVvSWwSeunnut1OfCR7/MY2ehha1Cf9QjY53C0\nd8c1pQdwDjAQWOU3zgtsAo4HIoDvgL5Af+CLIx5tAY9vvnbAe0G+LhfgdFh0IzAiWNfDN89IYBpw\nTTB/Jn7zvQAMDJF1+cit9TjG9XoYSPZN877b2Y92PQL1OQS0k53GoqrzRaTbEaMHAxtVdTOAiEwC\nRqnq00BNh1T2A5GByFkXDbEuvsNfsTj/AYpEZKqqVgYy95Ea6jNRp4e+KSLyJfB+4BJXr4E+EwGe\nAaap6tLAJq5eA/9faTLqs144RwI6ActpYkdL6rkeawKRoUn9QRpYR2Cb33Cmb1yVRORyEXkNeBd4\nJcDZ6qte66Kqf1DV3+JsRP/V2AWhBvX9TM4TkZd8n8vU6qZzSb3WBbgLGApcKSJ3BDLYUajv59JK\nRMYDA0Tk4UCHOwbVrdd/gStE5J80XBMSgVTlegTqcwiJPYWGoKr/xfmyhAxVfcvtDMdCVecCc12O\n0SBU9SXgJbdzNARV3YtzbiQoqWoBcJPbOY5VoD6HUN5T2A509hvu5BsXjEJlXUJlPcDWJRiEyno1\n6nqEclFIA3qJSHcRicA58TrF5UxHK1TWJVTWA2xdgkGorFfjrofbZ9sb6Iz9B8BOoAzneNstvvEX\nA9/jnLn/g9s5m9O6hMp62LoExyNU1qsprIc1iGeMMeaQUD58ZIwxpp6sKBhjjDnEioIxxphDrCgY\nY4w5xIqCMcaYQ6woGGOMOcSKgglaIpLfyO/3uoj0baBlVYjIchFZJSKfi0hSLdMnicivG+K9jamJ\n3adggpaI5KtqXAMuL0xVyxtqebW816HsIvI28L2q/rmG6bsBX6hqv8bIZ5ov21MwIUVE2ojIxyKS\n5nuc6Rs/WEQWicgyEVkoIr19428UkSkiMhuY5WuZda44vb2tE5H3fM1e4xuf4nueLyJ/FpHvRCRV\nRNr5xvfwDa8UkSfruDezCF+rpCISJyKzRGSpbxmjfNM8A/Tw7V38xTftA751XCEij/nGxYrIl75c\nq0TEtQ59THCyomBCzYvA31T1VOAK4HXf+HXA2ao6AHgEeMpvnoHAlap6rm94APBbnP4ojqfqXt9i\ngVRVPQWYD9zm9/4vqmp/6tCDn4h4gfP5sS2bYuAyVR0IDAFe8BWlh4BNqpqsqg+I06VnL5y29pOB\nQSJyDjAM2KGqp/j2KqbXlsEYf9Z0tgk1Q4G+vh/3AAkiEgckAm+LSC+crlfD/eaZqar7/IYXq2om\ngIgsB7oBXx/xPqU4PZEBLMHp7Q7gdOBS3/P3geeryRntW3ZHYC0w0zdegKd8G/hK3+vtqpj/Qt9j\nmW84DqdILMApJM/iHG5aUM37G1MlKwom1HiAn6lqsf9IEXkFmKOql/mOz8/1e7ngiGWU+D2voOr/\nJ2X64wm56qapSZGqJotIDDAD+A1OfwvXAm2AQapaJiJbcPoNP5IAT6vqaz95QWQgTgNqT4rILFV9\nvJ7ZTDNmh49MqPkfTi9nAIhIsu9pIj+2QX9jAN8/FeewFThNHNdIVQuBu4H/E5EwnJy7fQVhCNDV\nN+kBIN5v1hnAzb69IESko4i0FZEOQKGqTgT+gnNozJg6sz0FE8xiRMT/uP1fcTawr4rICpzv93yc\n3qmewzl89EfgywBm+i0wUUT+gHM8P7e2GVR1mS/vWOA94HMRWQmk45wLQVX3isg3IrIKp5/nB0Tk\nRGCR71BZPnAd0BP4i4hU4jS//KsGX0MT0uySVGMakO9wUJGqqoiMAcaq6qja5jOmqbA9BWMa1iDg\nFd8VQznAzS7nMaZebE/BGGPMIXai2RhjzCFWFIwxxhxiRcEYY8whVhSMMcYcYkXBGGPMIVYUjDHG\nHPL/AYKoohDxaK/hAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting recalls for various learning rates for 100 epochs\n",
    "plt.plot([0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10], train_recall)\n",
    "plt.plot([0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10], test_recall)\n",
    "plt.title('Recall for various learning rates')\n",
    "plt.ylabel('Recall')\n",
    "plt.xlabel('Learning Rates')\n",
    "plt.xscale('log')\n",
    "plt.legend(['Train', 'Test'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
