{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP with Keras and Scikit-learn "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras\n",
    "\n",
    "    Keras is a high-level neural networks library, written in Python and capable of running on top of either TensorFlow, CNTK or Theano. Allows for easy and fast prototyping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fix random seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy.random.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the current working directory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "PATH = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set directory to the place where csv file exist"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "PATH = os.path.join(PATH, \"Day01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load pima indians dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  6.    148.     72.    ...   0.627  50.      1.   ]\n",
      " [  1.     85.     66.    ...   0.351  31.      0.   ]\n",
      " [  8.    183.     64.    ...   0.672  32.      1.   ]\n",
      " ...\n",
      " [  5.    121.     72.    ...   0.245  30.      0.   ]\n",
      " [  1.    126.     60.    ...   0.349  47.      1.   ]\n",
      " [  1.     93.     70.    ...   0.315  23.      0.   ]]\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the shape of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 9)\n"
     ]
    }
   ],
   "source": [
    "print (data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seperate independent variables(X) and target variable(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[:,0:8]\n",
    "y = data[:,8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using sklearn package, split X and y into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the shapes of the new X objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(576, 8)\n",
      "(192, 8)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the shapes of the new y objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(576,)\n",
      "(192,)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape)\n",
    "\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Models in Keras are defined as a sequence of layers.\n",
    "\n",
    "2. First sequential model is created and layers are added one at a time. \n",
    "\n",
    "__Note__:The best network structure is found through a process of trial and error experimentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will build a fully-connected network structure with two layers.\n",
    "\n",
    "    The first layer has 12 neurons and expects 8 input variables. \n",
    "    Finally the output layer has 1 neuron to predict the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Sequential model is a linear stack of layers. \n",
    "Sequential model is created by passing a list of layer instances to the constructor or by simply add layers via the .add() method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(12, input_dim=8, activation='relu', kernel_initializer='uniform'))\n",
    "model.add(Dense(1, activation='sigmoid', kernel_initializer='uniform'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dense implements the operation: output = activation(dot(input, kernel) + bias) where activation is the element-wise activation function passed as the activation argument, kernel is a weights matrix created by the layer, and bias is a bias vector created by the layer (only applicable if use_bias is True).\n",
    "\n",
    "\n",
    "Note: If the input to the layer has a rank greater than 2, then it is flattened prior to the initial dot product with kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fully connected layers are defined using the Dense class. \n",
    "\n",
    "We specify \n",
    "    1. The number of neurons in the layer as the first argument\n",
    "    2. The model needs to know what input shape it should expect. For this reason, the first layer in a Sequential model (and only the first, because following layers can do automatic shape inference) needs to receive information about its input shape. One way is using input_dim argument. \n",
    "    3. The activation function using the activation argument. ‘relu‘ activation function is used in first two layers and the sigmoid activiation function in the output layer. \n",
    "    4. Initializations define the way to set the initial random weights of Keras layers. We initialize the network weights to a small random number generated from a uniform distribution (‘uniform‘).\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training a model, you need to configure the learning process, which is done via the compile method. It receives three arguments:\n",
    "\n",
    "    an optimizer     : This could be the string identifier of an existing optimizer (such as rmsprop or adagrad), or an instance of the Optimizer class.\n",
    "    \n",
    "    a loss function  : This is the objective that the model will try to minimize. It can be the string identifier of an existing loss function (such as categorical_crossentropy or mse), or it can be an objective function.\n",
    "    \n",
    "    a list of metrics: For any classification problem you will want to set this to metrics=['accuracy']. A metric could be the string identifier of an existing metric or a custom metric function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras models are trained on Numpy arrays of input data and labels. For training a model, you will typically use the fit function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "576/576 [==============================] - 0s 434us/step - loss: 0.6814 - acc: 0.6493\n",
      "Epoch 2/150\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.6625 - acc: 0.6476\n",
      "Epoch 3/150\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.6518 - acc: 0.6441\n",
      "Epoch 4/150\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.6447 - acc: 0.6493\n",
      "Epoch 5/150\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.6363 - acc: 0.6528\n",
      "Epoch 6/150\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.6331 - acc: 0.6562\n",
      "Epoch 7/150\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.6361 - acc: 0.6649\n",
      "Epoch 8/150\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.6265 - acc: 0.6615\n",
      "Epoch 9/150\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.6227 - acc: 0.6510\n",
      "Epoch 10/150\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.6208 - acc: 0.6632\n",
      "Epoch 11/150\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.6137 - acc: 0.6927\n",
      "Epoch 12/150\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.6088 - acc: 0.6806\n",
      "Epoch 13/150\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.6053 - acc: 0.6684\n",
      "Epoch 14/150\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.6185 - acc: 0.6667\n",
      "Epoch 15/150\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.6254 - acc: 0.6580\n",
      "Epoch 16/150\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.6086 - acc: 0.6753\n",
      "Epoch 17/150\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.6007 - acc: 0.6858\n",
      "Epoch 18/150\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.5938 - acc: 0.6788\n",
      "Epoch 19/150\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.6117 - acc: 0.6701\n",
      "Epoch 20/150\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.6025 - acc: 0.6753\n",
      "Epoch 21/150\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5899 - acc: 0.6944\n",
      "Epoch 22/150\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.5888 - acc: 0.6927\n",
      "Epoch 23/150\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5818 - acc: 0.7031\n",
      "Epoch 24/150\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.5767 - acc: 0.7083\n",
      "Epoch 25/150\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.5777 - acc: 0.7083\n",
      "Epoch 26/150\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5842 - acc: 0.7066\n",
      "Epoch 27/150\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5881 - acc: 0.7031\n",
      "Epoch 28/150\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5729 - acc: 0.7222\n",
      "Epoch 29/150\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.5764 - acc: 0.7205\n",
      "Epoch 30/150\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.5737 - acc: 0.7170\n",
      "Epoch 31/150\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5688 - acc: 0.7153\n",
      "Epoch 32/150\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.5666 - acc: 0.7188\n",
      "Epoch 33/150\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.5698 - acc: 0.7361\n",
      "Epoch 34/150\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5672 - acc: 0.7188\n",
      "Epoch 35/150\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.5709 - acc: 0.7170\n",
      "Epoch 36/150\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5580 - acc: 0.7344\n",
      "Epoch 37/150\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.5609 - acc: 0.7257\n",
      "Epoch 38/150\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5647 - acc: 0.7309\n",
      "Epoch 39/150\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.5595 - acc: 0.7378\n",
      "Epoch 40/150\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.5546 - acc: 0.7240\n",
      "Epoch 41/150\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5660 - acc: 0.7326\n",
      "Epoch 42/150\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5546 - acc: 0.7361\n",
      "Epoch 43/150\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.5576 - acc: 0.7431\n",
      "Epoch 44/150\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5556 - acc: 0.7326\n",
      "Epoch 45/150\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5598 - acc: 0.7396\n",
      "Epoch 46/150\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5602 - acc: 0.7240\n",
      "Epoch 47/150\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.5575 - acc: 0.7274\n",
      "Epoch 48/150\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5552 - acc: 0.7309\n",
      "Epoch 49/150\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.5467 - acc: 0.7378\n",
      "Epoch 50/150\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5564 - acc: 0.7292\n",
      "Epoch 51/150\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.5463 - acc: 0.7500\n",
      "Epoch 52/150\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.5468 - acc: 0.7344\n",
      "Epoch 53/150\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5478 - acc: 0.7465\n",
      "Epoch 54/150\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.5428 - acc: 0.7309\n",
      "Epoch 55/150\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.5568 - acc: 0.7292\n",
      "Epoch 56/150\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5609 - acc: 0.7153\n",
      "Epoch 57/150\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.5510 - acc: 0.7396\n",
      "Epoch 58/150\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5406 - acc: 0.7309\n",
      "Epoch 59/150\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.5462 - acc: 0.7413\n",
      "Epoch 60/150\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5487 - acc: 0.7205\n",
      "Epoch 61/150\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5386 - acc: 0.7465\n",
      "Epoch 62/150\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.5399 - acc: 0.7587\n",
      "Epoch 63/150\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5379 - acc: 0.7483\n",
      "Epoch 64/150\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5590 - acc: 0.7274\n",
      "Epoch 65/150\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5397 - acc: 0.7396\n",
      "Epoch 66/150\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.5405 - acc: 0.7604\n",
      "Epoch 67/150\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5391 - acc: 0.7517\n",
      "Epoch 68/150\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5430 - acc: 0.7535\n",
      "Epoch 69/150\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.5496 - acc: 0.7396\n",
      "Epoch 70/150\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5496 - acc: 0.7396\n",
      "Epoch 71/150\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5371 - acc: 0.7431\n",
      "Epoch 72/150\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.5430 - acc: 0.7431\n",
      "Epoch 73/150\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.5344 - acc: 0.7517\n",
      "Epoch 74/150\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5417 - acc: 0.7483\n",
      "Epoch 75/150\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5289 - acc: 0.7604\n",
      "Epoch 76/150\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.5376 - acc: 0.7431\n",
      "Epoch 77/150\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5434 - acc: 0.7309\n",
      "Epoch 78/150\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.5492 - acc: 0.7274\n",
      "Epoch 79/150\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5550 - acc: 0.7396\n",
      "Epoch 80/150\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5310 - acc: 0.7587\n",
      "Epoch 81/150\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5387 - acc: 0.7396\n",
      "Epoch 82/150\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5541 - acc: 0.7326\n",
      "Epoch 83/150\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5445 - acc: 0.7188\n",
      "Epoch 84/150\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5395 - acc: 0.7465\n",
      "Epoch 85/150\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5288 - acc: 0.7431\n",
      "Epoch 86/150\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5421 - acc: 0.7587\n",
      "Epoch 87/150\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5312 - acc: 0.7483\n",
      "Epoch 88/150\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5256 - acc: 0.7500\n",
      "Epoch 89/150\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5314 - acc: 0.7587\n",
      "Epoch 90/150\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5283 - acc: 0.7517\n",
      "Epoch 91/150\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5392 - acc: 0.7517\n",
      "Epoch 92/150\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5338 - acc: 0.7483\n",
      "Epoch 93/150\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5304 - acc: 0.7500\n",
      "Epoch 94/150\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5239 - acc: 0.7656\n",
      "Epoch 95/150\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5258 - acc: 0.7587\n",
      "Epoch 96/150\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5224 - acc: 0.7587\n",
      "Epoch 97/150\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5257 - acc: 0.7517\n",
      "Epoch 98/150\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5293 - acc: 0.7674\n",
      "Epoch 99/150\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5207 - acc: 0.7552\n",
      "Epoch 100/150\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.5269 - acc: 0.7535\n",
      "Epoch 101/150\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5250 - acc: 0.7552\n",
      "Epoch 102/150\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5240 - acc: 0.7552\n",
      "Epoch 103/150\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5228 - acc: 0.7622\n",
      "Epoch 104/150\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5241 - acc: 0.7691\n",
      "Epoch 105/150\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5166 - acc: 0.7639\n",
      "Epoch 106/150\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5228 - acc: 0.7604\n",
      "Epoch 107/150\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.5193 - acc: 0.7726\n",
      "Epoch 108/150\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5305 - acc: 0.7483\n",
      "Epoch 109/150\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5170 - acc: 0.7569\n",
      "Epoch 110/150\n",
      "576/576 [==============================] - ETA: 0s - loss: 0.4411 - acc: 0.875 - 0s 54us/step - loss: 0.5325 - acc: 0.7344\n",
      "Epoch 111/150\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5238 - acc: 0.7465\n",
      "Epoch 112/150\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5227 - acc: 0.7535\n",
      "Epoch 113/150\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5184 - acc: 0.7587\n",
      "Epoch 114/150\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5136 - acc: 0.7743\n",
      "Epoch 115/150\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5123 - acc: 0.7726\n",
      "Epoch 116/150\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.5134 - acc: 0.7674\n",
      "Epoch 117/150\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5131 - acc: 0.7656\n",
      "Epoch 118/150\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.5164 - acc: 0.7483\n",
      "Epoch 119/150\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5168 - acc: 0.7535\n",
      "Epoch 120/150\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5143 - acc: 0.7535\n",
      "Epoch 121/150\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.5118 - acc: 0.7639\n",
      "Epoch 122/150\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5130 - acc: 0.7708\n",
      "Epoch 123/150\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5140 - acc: 0.7465\n",
      "Epoch 124/150\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5127 - acc: 0.7691\n",
      "Epoch 125/150\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.5096 - acc: 0.7743\n",
      "Epoch 126/150\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5216 - acc: 0.7448\n",
      "Epoch 127/150\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5174 - acc: 0.7656\n",
      "Epoch 128/150\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5096 - acc: 0.7656\n",
      "Epoch 129/150\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5139 - acc: 0.7552\n",
      "Epoch 130/150\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.5144 - acc: 0.7691\n",
      "Epoch 131/150\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5074 - acc: 0.7743\n",
      "Epoch 132/150\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5107 - acc: 0.7708\n",
      "Epoch 133/150\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5094 - acc: 0.7622\n",
      "Epoch 134/150\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5046 - acc: 0.7795\n",
      "Epoch 135/150\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5079 - acc: 0.7743\n",
      "Epoch 136/150\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.5072 - acc: 0.7812\n",
      "Epoch 137/150\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5054 - acc: 0.7743\n",
      "Epoch 138/150\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5054 - acc: 0.7569\n",
      "Epoch 139/150\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5071 - acc: 0.7587\n",
      "Epoch 140/150\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5027 - acc: 0.7795\n",
      "Epoch 141/150\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5030 - acc: 0.7674\n",
      "Epoch 142/150\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5005 - acc: 0.7778\n",
      "Epoch 143/150\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4991 - acc: 0.7865\n",
      "Epoch 144/150\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5024 - acc: 0.7691\n",
      "Epoch 145/150\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.5065 - acc: 0.7656\n",
      "Epoch 146/150\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.5001 - acc: 0.7726\n",
      "Epoch 147/150\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.4995 - acc: 0.7812\n",
      "Epoch 148/150\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5025 - acc: 0.7795\n",
      "Epoch 149/150\n",
      "576/576 [==============================] - 0s 54us/step - loss: 0.5013 - acc: 0.7778\n",
      "Epoch 150/150\n",
      "576/576 [==============================] - 0s 81us/step - loss: 0.5033 - acc: 0.7587\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x243e0273e80>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=150, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    x         : input data, as a Numpy array or list of Numpy arrays (if the model has multiple inputs).\n",
    "    y         : labels, as a Numpy array.\n",
    "    batch_size: integer. Number of samples per gradient update.\n",
    "    epochs  : integer, the number of epochs to train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case we trained neural network on the entire dataset and evaluating its on the same dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "576/576 [==============================] - 0s 81us/step\n",
      "acc: 76.91%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_train, y_train)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict using model.predict().  \n",
    "We are using a sigmoid activation function in the output layer, so the predictions will be in the range between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.32600406]\n",
      " [0.41105205]\n",
      " [0.5722285 ]\n",
      " [0.99999857]\n",
      " [0.21059655]\n",
      " [0.50984687]\n",
      " [0.60431117]\n",
      " [0.24324512]\n",
      " [0.3248869 ]\n",
      " [0.17131397]\n",
      " [0.07411409]\n",
      " [0.05467231]\n",
      " [0.07381954]\n",
      " [0.02660361]\n",
      " [0.9998654 ]\n",
      " [0.26792502]\n",
      " [0.72478783]\n",
      " [0.28065443]\n",
      " [0.30137545]\n",
      " [0.26373386]\n",
      " [0.8081806 ]\n",
      " [0.1349357 ]\n",
      " [0.05555315]\n",
      " [0.29530787]\n",
      " [0.10218793]\n",
      " [0.18830812]\n",
      " [0.07445649]\n",
      " [0.59713227]\n",
      " [0.20582283]\n",
      " [0.64894533]\n",
      " [0.73179775]\n",
      " [0.15807255]\n",
      " [0.03433481]\n",
      " [0.74837685]\n",
      " [0.7555488 ]\n",
      " [0.54519624]\n",
      " [0.19641888]\n",
      " [0.21479678]\n",
      " [0.6355916 ]\n",
      " [0.23059267]\n",
      " [0.8346812 ]\n",
      " [0.14133675]\n",
      " [0.73203915]\n",
      " [0.04557661]\n",
      " [0.7897977 ]\n",
      " [0.23024255]\n",
      " [0.2857993 ]\n",
      " [0.24955362]\n",
      " [0.28446773]\n",
      " [0.3448305 ]\n",
      " [0.46616793]\n",
      " [0.12715232]\n",
      " [0.47606263]\n",
      " [0.37688527]\n",
      " [0.62393755]\n",
      " [0.2604609 ]\n",
      " [0.31885806]\n",
      " [0.11419863]\n",
      " [0.06878575]\n",
      " [0.11271989]\n",
      " [0.44170853]\n",
      " [0.12586465]\n",
      " [0.6400375 ]\n",
      " [0.5590373 ]\n",
      " [0.20649362]\n",
      " [0.48842958]\n",
      " [0.12188855]\n",
      " [0.55853695]\n",
      " [0.2795969 ]\n",
      " [0.56894666]\n",
      " [0.7381737 ]\n",
      " [0.33213395]\n",
      " [0.88775814]\n",
      " [0.4641087 ]\n",
      " [0.14608924]\n",
      " [0.72858775]\n",
      " [0.05295714]\n",
      " [0.8254302 ]\n",
      " [0.33168346]\n",
      " [0.11163784]\n",
      " [0.63966316]\n",
      " [0.559749  ]\n",
      " [0.66972184]\n",
      " [0.43653053]\n",
      " [0.39220122]\n",
      " [0.5241636 ]\n",
      " [0.69766027]\n",
      " [0.86625797]\n",
      " [0.21186535]\n",
      " [0.12718537]\n",
      " [0.12182248]\n",
      " [0.5601351 ]\n",
      " [0.63198334]\n",
      " [0.65475297]\n",
      " [0.74263775]\n",
      " [0.05669809]\n",
      " [0.58441895]\n",
      " [0.05189267]\n",
      " [0.665011  ]\n",
      " [0.1559467 ]\n",
      " [0.03898879]\n",
      " [0.40289652]\n",
      " [0.39768532]\n",
      " [0.8218466 ]\n",
      " [0.3982712 ]\n",
      " [0.24163064]\n",
      " [0.20918413]\n",
      " [0.11189602]\n",
      " [0.1334528 ]\n",
      " [0.2697881 ]\n",
      " [0.47202757]\n",
      " [0.43590713]\n",
      " [0.22931162]\n",
      " [0.63779426]\n",
      " [0.14132357]\n",
      " [0.29531476]\n",
      " [0.29259083]\n",
      " [0.5062933 ]\n",
      " [0.4177646 ]\n",
      " [0.44725055]\n",
      " [0.10545445]\n",
      " [0.22081299]\n",
      " [0.04717655]\n",
      " [0.2282065 ]\n",
      " [0.2668585 ]\n",
      " [0.02682685]\n",
      " [0.8062068 ]\n",
      " [0.23921904]\n",
      " [0.30413604]\n",
      " [0.34324718]\n",
      " [0.32742703]\n",
      " [0.7573138 ]\n",
      " [0.17766349]\n",
      " [0.10912964]\n",
      " [0.2251065 ]\n",
      " [0.47964427]\n",
      " [0.4342032 ]\n",
      " [0.7560923 ]\n",
      " [0.29262185]\n",
      " [0.24686177]\n",
      " [0.74523467]\n",
      " [0.58730084]\n",
      " [0.16330685]\n",
      " [0.15095696]\n",
      " [0.07255477]\n",
      " [0.24997649]\n",
      " [0.23007633]\n",
      " [0.12801315]\n",
      " [0.59946734]\n",
      " [0.4894512 ]\n",
      " [0.00266979]\n",
      " [0.7281551 ]\n",
      " [0.08161507]\n",
      " [0.0122001 ]\n",
      " [0.5416663 ]\n",
      " [0.69028765]\n",
      " [0.35407758]\n",
      " [0.25963646]\n",
      " [0.74155444]\n",
      " [0.5298768 ]\n",
      " [0.18795727]\n",
      " [0.22487405]\n",
      " [0.17694451]\n",
      " [0.42599854]\n",
      " [0.72342616]\n",
      " [0.67108977]\n",
      " [0.9772314 ]\n",
      " [0.35655376]\n",
      " [0.1254138 ]\n",
      " [0.3056729 ]\n",
      " [0.7833755 ]\n",
      " [0.42367306]\n",
      " [0.24533711]\n",
      " [0.21079455]\n",
      " [0.5723302 ]\n",
      " [0.4395228 ]\n",
      " [0.58412355]\n",
      " [0.11515029]\n",
      " [0.73248863]\n",
      " [0.5830889 ]\n",
      " [0.1699665 ]\n",
      " [0.63748044]\n",
      " [0.12358335]\n",
      " [0.66673124]\n",
      " [0.85579574]\n",
      " [0.585399  ]\n",
      " [0.22661105]\n",
      " [0.11698113]\n",
      " [0.6388493 ]\n",
      " [0.30387843]\n",
      " [0.10003412]\n",
      " [0.27352163]\n",
      " [0.27497125]\n",
      " [0.42735872]\n",
      " [0.24250811]\n",
      " [0.6969617 ]\n",
      " [0.8851625 ]\n",
      " [0.44741964]\n",
      " [0.18781283]\n",
      " [0.88256925]\n",
      " [0.5926149 ]\n",
      " [0.29846182]\n",
      " [0.19352806]\n",
      " [0.53077894]\n",
      " [0.6715282 ]\n",
      " [0.24652857]\n",
      " [0.641342  ]\n",
      " [0.16828464]\n",
      " [0.00428396]\n",
      " [0.3621687 ]\n",
      " [0.40147617]\n",
      " [0.51105535]\n",
      " [0.35570893]\n",
      " [0.7926104 ]\n",
      " [0.4987421 ]\n",
      " [0.33274174]\n",
      " [0.03471856]\n",
      " [0.5327902 ]\n",
      " [0.2959134 ]\n",
      " [0.8539976 ]\n",
      " [0.76900226]\n",
      " [0.65397644]\n",
      " [0.25929812]\n",
      " [0.6423449 ]\n",
      " [0.2533774 ]\n",
      " [0.4024188 ]\n",
      " [0.21505217]\n",
      " [0.33756942]\n",
      " [0.608181  ]\n",
      " [0.54847705]\n",
      " [0.10009656]\n",
      " [0.18831259]\n",
      " [0.7733653 ]\n",
      " [0.31365472]\n",
      " [0.6864324 ]\n",
      " [0.3336173 ]\n",
      " [0.2840963 ]\n",
      " [0.61340165]\n",
      " [0.25836903]\n",
      " [0.2790018 ]\n",
      " [0.6920629 ]\n",
      " [0.10983466]\n",
      " [0.15935847]\n",
      " [0.38127056]\n",
      " [0.5829036 ]\n",
      " [0.38927752]\n",
      " [0.26209548]\n",
      " [0.17828025]\n",
      " [0.47709155]\n",
      " [0.6862587 ]\n",
      " [0.6816352 ]\n",
      " [0.5647275 ]\n",
      " [0.7567415 ]\n",
      " [0.1743551 ]\n",
      " [0.2784118 ]\n",
      " [0.7723158 ]\n",
      " [0.23633404]\n",
      " [0.3813125 ]\n",
      " [0.18988174]\n",
      " [0.08707273]\n",
      " [0.38865197]\n",
      " [0.46896338]\n",
      " [0.41559818]\n",
      " [0.2568194 ]\n",
      " [0.47138956]\n",
      " [0.20088498]\n",
      " [0.24600519]\n",
      " [0.23702402]\n",
      " [0.5989072 ]\n",
      " [0.45984527]\n",
      " [0.6916707 ]\n",
      " [0.07472153]\n",
      " [0.75402063]\n",
      " [0.21521652]\n",
      " [0.20593077]\n",
      " [0.21262501]\n",
      " [0.5238771 ]\n",
      " [0.17764577]\n",
      " [0.41388693]\n",
      " [0.02678587]\n",
      " [0.40820232]\n",
      " [0.0759603 ]\n",
      " [0.93460274]\n",
      " [0.06884296]\n",
      " [0.23316544]\n",
      " [0.15729092]\n",
      " [0.7957057 ]\n",
      " [0.5972314 ]\n",
      " [0.60846096]\n",
      " [0.40375656]\n",
      " [0.6195234 ]\n",
      " [0.37559712]\n",
      " [0.30554488]\n",
      " [0.43404666]\n",
      " [0.51112217]\n",
      " [0.07590782]\n",
      " [0.5129057 ]\n",
      " [0.19963641]\n",
      " [0.2361847 ]\n",
      " [0.15429759]\n",
      " [0.23321082]\n",
      " [0.44946742]\n",
      " [0.28024888]\n",
      " [0.55962306]\n",
      " [0.32627705]\n",
      " [0.5150024 ]\n",
      " [0.18678275]\n",
      " [0.08968554]\n",
      " [0.11057159]\n",
      " [0.56787187]\n",
      " [0.15800753]\n",
      " [0.6566359 ]\n",
      " [0.8096926 ]\n",
      " [0.28482994]\n",
      " [0.265581  ]\n",
      " [0.02723717]\n",
      " [0.31877324]\n",
      " [0.07293525]\n",
      " [0.45840955]\n",
      " [0.6294572 ]\n",
      " [0.08272651]\n",
      " [0.8733201 ]\n",
      " [0.64538157]\n",
      " [0.1461478 ]\n",
      " [0.5245944 ]\n",
      " [0.9996006 ]\n",
      " [0.68535095]\n",
      " [0.22429237]\n",
      " [0.06623014]\n",
      " [0.83480114]\n",
      " [0.19360757]\n",
      " [0.5134326 ]\n",
      " [0.34907842]\n",
      " [0.0133903 ]\n",
      " [0.6455561 ]\n",
      " [0.31078413]\n",
      " [0.05174378]\n",
      " [0.63751334]\n",
      " [0.3083116 ]\n",
      " [0.23413914]\n",
      " [0.8364981 ]\n",
      " [0.2000836 ]\n",
      " [0.10379028]\n",
      " [0.43744025]\n",
      " [0.35271147]\n",
      " [0.61674845]\n",
      " [0.28807592]\n",
      " [0.62672365]\n",
      " [0.07967877]\n",
      " [0.05841396]\n",
      " [0.3624436 ]\n",
      " [0.65953606]\n",
      " [0.4075387 ]\n",
      " [0.3286448 ]\n",
      " [0.14523113]\n",
      " [0.26303014]\n",
      " [0.01193036]\n",
      " [0.56048995]\n",
      " [0.8876288 ]\n",
      " [0.23383865]\n",
      " [0.77350795]\n",
      " [0.16468176]\n",
      " [0.0800823 ]\n",
      " [0.56862515]\n",
      " [0.36209154]\n",
      " [0.26777035]\n",
      " [0.17103127]\n",
      " [0.37821206]\n",
      " [0.62790686]\n",
      " [0.36831808]\n",
      " [0.37284696]\n",
      " [0.11864375]\n",
      " [0.53347343]\n",
      " [0.5482637 ]\n",
      " [0.49743256]\n",
      " [0.20934838]\n",
      " [0.64868927]\n",
      " [0.67148364]\n",
      " [0.52926123]\n",
      " [0.9994911 ]\n",
      " [0.6453073 ]\n",
      " [0.6028244 ]\n",
      " [0.4675453 ]\n",
      " [0.31478962]\n",
      " [0.53773224]\n",
      " [0.752302  ]\n",
      " [0.34172073]\n",
      " [0.52650124]\n",
      " [0.59460807]\n",
      " [0.6957387 ]\n",
      " [0.1884103 ]\n",
      " [0.11097508]\n",
      " [0.55439144]\n",
      " [0.5218373 ]\n",
      " [0.4952944 ]\n",
      " [0.6513522 ]\n",
      " [0.13258405]\n",
      " [0.49458933]\n",
      " [0.45009086]\n",
      " [0.4228835 ]\n",
      " [0.44784778]\n",
      " [0.1751129 ]\n",
      " [0.73890626]\n",
      " [0.02521679]\n",
      " [0.58950114]\n",
      " [0.21779068]\n",
      " [0.68443567]\n",
      " [0.18275002]\n",
      " [0.6210449 ]\n",
      " [0.46585655]\n",
      " [0.92555976]\n",
      " [0.3336192 ]\n",
      " [0.6144085 ]\n",
      " [0.5513146 ]\n",
      " [0.10127041]\n",
      " [0.34182325]\n",
      " [0.38022935]\n",
      " [0.7871789 ]\n",
      " [0.78502804]\n",
      " [0.7975821 ]\n",
      " [0.6962863 ]\n",
      " [0.29883593]\n",
      " [0.2784224 ]\n",
      " [0.80568385]\n",
      " [0.1781628 ]\n",
      " [0.32701358]\n",
      " [0.5160766 ]\n",
      " [0.54935753]\n",
      " [0.16408427]\n",
      " [0.12333585]\n",
      " [0.39068544]\n",
      " [0.3634878 ]\n",
      " [0.11250685]\n",
      " [0.58822817]\n",
      " [0.31838593]\n",
      " [0.11304966]\n",
      " [0.7522759 ]\n",
      " [0.78963333]\n",
      " [0.16148138]\n",
      " [0.25714502]\n",
      " [0.25556055]\n",
      " [0.10541766]\n",
      " [0.01519044]\n",
      " [0.14722592]\n",
      " [0.7599379 ]\n",
      " [0.15794311]\n",
      " [0.9553827 ]\n",
      " [0.64802915]\n",
      " [0.5201974 ]\n",
      " [0.22012518]\n",
      " [0.40245315]\n",
      " [0.2386848 ]\n",
      " [0.12861523]\n",
      " [0.19841261]\n",
      " [0.39464247]\n",
      " [0.22987233]\n",
      " [0.2065359 ]\n",
      " [0.21264227]\n",
      " [0.72228587]\n",
      " [0.10007542]\n",
      " [0.19829436]\n",
      " [0.06291411]\n",
      " [0.3490882 ]\n",
      " [0.58483684]\n",
      " [0.7569959 ]\n",
      " [0.5567534 ]\n",
      " [0.14518918]\n",
      " [0.05345624]\n",
      " [0.70318866]\n",
      " [0.5354972 ]\n",
      " [0.18349488]\n",
      " [0.5456399 ]\n",
      " [0.6589362 ]\n",
      " [0.11750307]\n",
      " [0.84507525]\n",
      " [0.03602687]\n",
      " [0.18386829]\n",
      " [0.21604317]\n",
      " [0.7801597 ]\n",
      " [0.13151874]\n",
      " [0.33376133]\n",
      " [0.7007875 ]\n",
      " [0.203906  ]\n",
      " [0.9415351 ]\n",
      " [0.0367632 ]\n",
      " [0.9255963 ]\n",
      " [0.42057192]\n",
      " [0.4179814 ]\n",
      " [0.8793974 ]\n",
      " [0.7179868 ]\n",
      " [0.7873222 ]\n",
      " [0.18368274]\n",
      " [0.7996023 ]\n",
      " [0.10641342]\n",
      " [0.58928716]\n",
      " [0.5928721 ]\n",
      " [0.5260135 ]\n",
      " [0.36331725]\n",
      " [0.6132302 ]\n",
      " [0.24647932]\n",
      " [0.6971538 ]\n",
      " [0.31097353]\n",
      " [0.40615228]\n",
      " [0.5704252 ]\n",
      " [0.986161  ]\n",
      " [0.7403928 ]\n",
      " [0.24114922]\n",
      " [0.35126528]\n",
      " [0.11410397]\n",
      " [0.42091084]\n",
      " [0.52510196]\n",
      " [0.23868592]\n",
      " [0.17141844]\n",
      " [0.6913809 ]\n",
      " [0.25142324]\n",
      " [0.23507805]\n",
      " [0.40693995]\n",
      " [0.835689  ]\n",
      " [0.48032427]\n",
      " [0.37247613]\n",
      " [0.31695879]\n",
      " [0.05225827]\n",
      " [0.21482137]\n",
      " [0.13501847]\n",
      " [0.02027701]\n",
      " [0.13017955]\n",
      " [0.2355465 ]\n",
      " [0.7036012 ]\n",
      " [0.5368916 ]\n",
      " [0.54930675]\n",
      " [0.182805  ]\n",
      " [0.6425276 ]\n",
      " [0.39613128]\n",
      " [0.2672114 ]\n",
      " [0.55425614]\n",
      " [0.360572  ]\n",
      " [0.46291894]\n",
      " [0.08874477]\n",
      " [0.7360425 ]\n",
      " [0.10674898]\n",
      " [0.17639077]\n",
      " [0.32065243]\n",
      " [0.28321397]\n",
      " [0.74774253]\n",
      " [0.5529415 ]\n",
      " [0.55188036]\n",
      " [0.32796907]\n",
      " [0.6458388 ]\n",
      " [0.5110717 ]\n",
      " [0.86621636]\n",
      " [0.50232416]\n",
      " [0.11908537]\n",
      " [0.1574477 ]\n",
      " [0.34294003]\n",
      " [0.10663652]\n",
      " [0.38956398]\n",
      " [0.34709343]\n",
      " [0.7630858 ]\n",
      " [0.31524947]\n",
      " [0.08449095]\n",
      " [0.4804586 ]\n",
      " [0.10945245]\n",
      " [0.7509224 ]\n",
      " [0.62472856]\n",
      " [0.4445544 ]\n",
      " [0.6374294 ]\n",
      " [0.02733201]\n",
      " [0.68066365]\n",
      " [0.30227497]\n",
      " [0.135121  ]\n",
      " [0.5813719 ]\n",
      " [0.79584163]\n",
      " [0.4460431 ]\n",
      " [0.31765366]\n",
      " [0.16855223]\n",
      " [0.15330729]]\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X_train)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "y_pred_class = model.predict_classes(X_test)\n",
    "print(y_pred_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate accuracy of class predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7291666666666666"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "metrics.accuracy_score(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[97, 29],\n",
       "       [23, 43]], dtype=int64)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference:\n",
    "\n",
    "    https://keras.io/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
